# InsightCosmos ä½¿ç”¨æ‰‹å†Š

> **ç‰ˆæœ¬**: 1.0
> **æœ€å¾Œæ›´æ–°**: 2025-11-25
> **é©ç”¨éšæ®µ**: Phase 1 - å€‹äººå®‡å®™ç‰ˆ

---

## ğŸ“š ç›®éŒ„

1. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
2. [å€‹äººåŒ–é…ç½®](#å€‹äººåŒ–é…ç½®)
3. [è³‡æ–™ç®¡ç†](#è³‡æ–™ç®¡ç†)
4. [é€²éšè¨­å®š](#é€²éšè¨­å®š)
5. [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)
6. [å‚™ä»½èˆ‡é‚„åŸ](#å‚™ä»½èˆ‡é‚„åŸ)
7. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
8. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åˆæ¬¡å®‰è£

```bash
# 1. Clone å°ˆæ¡ˆ
git clone https://github.com/your-repo/InsightCosmos.git
cd InsightCosmos

# 2. å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# 3. å®‰è£ä¾è³´
pip install -r requirements.txt

# 4. é…ç½®ç’°å¢ƒè®Šæ•¸
cp .env.example .env
# ç·¨è¼¯ .env å¡«å…¥ä½ çš„é…ç½®ï¼ˆè¦‹ä¸‹æ–¹è©³ç´°èªªæ˜ï¼‰

# 5. åˆå§‹åŒ–è³‡æ–™åº«
python -m src.memory.database

# 6. æ¸¬è©¦é‹è¡Œï¼ˆä¸ç™¼é€éƒµä»¶ï¼‰
python -m src.orchestrator.daily_runner --dry-run
```

### åŸºæœ¬ä½¿ç”¨

```bash
# åŸ·è¡Œæ¯æ—¥æƒ…å ±æ”¶é›†èˆ‡åˆ†æï¼ˆç”Ÿç”¢æ¨¡å¼ï¼‰
python -m src.orchestrator.daily_runner

# æ¸¬è©¦æ¨¡å¼ï¼ˆä¸ç™¼é€éƒµä»¶ï¼ŒæŸ¥çœ‹å ±å‘Šå…§å®¹ï¼‰
python -m src.orchestrator.daily_runner --dry-run

# è©³ç´°æ—¥èªŒæ¨¡å¼ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
python -m src.orchestrator.daily_runner --verbose

# çµ„åˆä½¿ç”¨
python -m src.orchestrator.daily_runner --dry-run --verbose
```

---

## ğŸ¯ å€‹äººåŒ–é…ç½®

### 1. ä¿®æ”¹é—œæ³¨é ˜åŸŸèˆ‡ä¸»é¡Œ

ç·¨è¼¯ `.env` æª”æ¡ˆä¸­çš„ `USER_INTERESTS` è®Šæ•¸ï¼š

```bash
# .env

# åŸºç¤è¨­å®š
USER_NAME=Ray
USER_INTERESTS=AI,Robotics,Multi-Agent Systems

# è‡ªè¨‚ä½ çš„é—œæ³¨é ˜åŸŸï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰
# ç¯„ä¾‹ 1: æ›´å»£æ³›çš„é ˜åŸŸ
USER_INTERESTS=AI,Machine Learning,Computer Vision,NLP,Robotics,Autonomous Systems

# ç¯„ä¾‹ 2: æ›´å°ˆæ³¨çš„é ˜åŸŸ
USER_INTERESTS=Large Language Models,Prompt Engineering,AI Agents,RAG

# ç¯„ä¾‹ 3: è·¨é ˜åŸŸ
USER_INTERESTS=AI,Healthcare,Drug Discovery,Medical Imaging

# ç¯„ä¾‹ 4: æŠ€è¡“ + å•†æ¥­
USER_INTERESTS=AI,Startups,Venture Capital,Product Management
```

### 2. èª¿æ•´ RSS è³‡æ–™ä¾†æº

ç·¨è¼¯ `src/agents/scout_agent.py`ï¼Œä¿®æ”¹ `DEFAULT_FEEDS` åˆ—è¡¨ï¼š

```python
# src/agents/scout_agent.py

DEFAULT_FEEDS = [
    # ç¾æœ‰ä¾†æº
    "https://feeds.feedburner.com/blogspot/gJZg",  # Google AI Blog
    "https://openai.com/blog/rss",                  # OpenAI Blog
    "https://www.deepmind.com/blog/rss.xml",        # DeepMind Blog

    # æ–°å¢ä½ çš„è‡ªè¨‚ä¾†æº
    "https://huggingface.co/blog/feed.xml",         # Hugging Face Blog
    "https://blog.research.google/feeds/posts/default", # Google Research
    "https://arxiv-sanity-lite.com/rss",            # ArXiv ML Papers
    "https://www.reddit.com/r/MachineLearning/.rss", # Reddit ML
    "https://news.ycombinator.com/rss",             # Hacker News

    # è¡Œæ¥­åª’é«”
    "https://venturebeat.com/category/ai/feed/",    # VentureBeat AI
    "https://techcrunch.com/category/artificial-intelligence/feed/", # TechCrunch AI
]
```

**æç¤º**:
- ä½¿ç”¨ [RSS.app](https://rss.app/) å°‡ä»»ä½•ç¶²ç«™è½‰æ›ç‚º RSS feed
- ä½¿ç”¨ [Feedly](https://feedly.com/) ç™¼ç¾æ›´å¤šé«˜å“è³ª RSS ä¾†æº
- ç¢ºä¿ RSS feed URL æœ‰æ•ˆï¼ˆå¯ç”¨ç€è¦½å™¨æ¸¬è©¦ï¼‰

### 3. èª¿æ•´æœç´¢é—œéµå­—

ç·¨è¼¯ `src/agents/scout_agent.py`ï¼Œä¿®æ”¹ `DEFAULT_SEARCH_QUERIES`ï¼š

```python
# src/agents/scout_agent.py

DEFAULT_SEARCH_QUERIES = [
    # ç¾æœ‰é—œéµå­—
    "AI breakthrough OR AGI OR artificial general intelligence",
    "OpenAI OR Anthropic OR Google AI latest",
    "multimodal AI OR vision language model",

    # æ–°å¢ä½ çš„è‡ªè¨‚é—œéµå­—
    # æŠ€è¡“æ–¹å‘
    "retrieval augmented generation RAG",
    "prompt engineering best practices",
    "AI agent framework OR autonomous agents",

    # æ‡‰ç”¨å ´æ™¯
    "AI in healthcare OR medical AI",
    "robotics manipulation OR robot learning",
    "self-driving OR autonomous vehicles",

    # å•†æ¥­èˆ‡è¶¨å‹¢
    "AI startup funding OR AI investment",
    "AI regulation OR AI policy",
    "AGI safety OR AI alignment",

    # ç ”ç©¶å‰æ²¿
    "transformer architecture OR attention mechanism",
    "reinforcement learning from human feedback RLHF",
    "mixture of experts MoE",
]
```

**æœç´¢æŠ€å·§**:
- ä½¿ç”¨ `OR` é€£æ¥åŒç¾©è©: `"AGI OR artificial general intelligence"`
- ä½¿ç”¨å¼•è™Ÿç²¾ç¢ºåŒ¹é…: `"large language model"`
- ä½¿ç”¨ `-` æ’é™¤é—œéµå­—: `"AI -cryptocurrency"`
- é™åˆ¶æ™‚é–“ç¯„åœ: åœ¨ search queries åŠ ä¸Š `after:2025-01-01`

### 4. èª¿æ•´å…§å®¹å„ªå…ˆåº¦æ¼”ç®—æ³•

ç·¨è¼¯ `prompts/analyst_prompt.txt`ï¼Œèª¿æ•´è©•åˆ†æ¨™æº–ï¼š

```python
# prompts/analyst_prompt.txt

ä½ çš„è©•åˆ†æ¨™æº–ï¼ˆ0.0-1.0ï¼‰ï¼š

**é«˜å„ªå…ˆåº¦ (0.8-1.0)**ï¼š
- æŠ€è¡“çªç ´ï¼ˆæ–°æ¨¡å‹ã€æ–°æ¶æ§‹ã€æ–°ç†è«–ï¼‰
- èˆ‡ {user_interests} é«˜åº¦ç›¸é—œ
- æœ‰å¯¦éš›æ‡‰ç”¨åƒ¹å€¼æˆ–ç¨‹å¼ç¢¼
- ä¾†è‡ªé ‚ç´šç ”ç©¶æ©Ÿæ§‹æˆ–å…¬å¸
- æœ‰è©³ç´°æŠ€è¡“ç´°ç¯€æˆ–è«–æ–‡é€£çµ

**ä¸­å„ªå…ˆåº¦ (0.5-0.7)**ï¼š
- è¡Œæ¥­è¶¨å‹¢åˆ†æ
- æ‡‰ç”¨æ¡ˆä¾‹åˆ†äº«
- å·¥å…·æˆ–æ¡†æ¶ç™¼å¸ƒ
- ç›¸é—œä½†éæ ¸å¿ƒé ˜åŸŸ

**ä½å„ªå…ˆåº¦ (0.0-0.4)**ï¼š
- æ–°èå ±å°æˆ–å…¬é—œç¨¿
- èˆ‡ {user_interests} ä¸ç›¸é—œ
- ç¼ºä¹æŠ€è¡“æ·±åº¦
- éæ™‚æˆ–é‡è¤‡å…§å®¹
```

ä½ å¯ä»¥æ ¹æ“šè‡ªå·±çš„éœ€æ±‚èª¿æ•´é€™äº›æ¨™æº–ã€‚ä¾‹å¦‚ï¼š

- **å­¸è¡“å°å‘**: æé«˜è«–æ–‡æ¬Šé‡ï¼Œé™ä½æ–°èå ±å°æ¬Šé‡
- **å•†æ¥­å°å‘**: æé«˜ç”¢å“ç™¼å¸ƒã€èè³‡æ–°èæ¬Šé‡
- **å·¥ç¨‹å°å‘**: æé«˜é–‹æºå·¥å…·ã€ç¨‹å¼ç¢¼ç¯„ä¾‹æ¬Šé‡

---

## ğŸ“§ å¤šäººéƒµä»¶é…ç½®

### æ–¹æ³• 1: ä¿®æ”¹ Curator Agentï¼ˆå–®æ¬¡ç™¼é€å¤šäººï¼‰

ç·¨è¼¯ `src/agents/curator_daily.py`ï¼Œä¿®æ”¹æ”¶ä»¶äººåˆ—è¡¨ï¼š

```python
# src/agents/curator_daily.py

def generate_daily_digest(
    config: Config,
    recipient_email: str = None,  # å–®ä¸€æ”¶ä»¶äººï¼ˆä¿ç•™å‘å¾Œå…¼å®¹ï¼‰
    recipient_emails: List[str] = None,  # æ–°å¢ï¼šå¤šå€‹æ”¶ä»¶äºº
    max_articles: int = 10
) -> Dict[str, Any]:
    """
    ç”Ÿæˆä¸¦ç™¼é€æ¯æ—¥æƒ…å ±æ‘˜è¦

    Args:
        config: é…ç½®å°è±¡
        recipient_email: å–®ä¸€æ”¶ä»¶äººï¼ˆå·²æ£„ç”¨ï¼‰
        recipient_emails: æ”¶ä»¶äººåˆ—è¡¨ï¼ˆæ¨è–¦ï¼‰
        max_articles: æœ€å¤šåŒ…å«æ–‡ç« æ•¸
    """

    # å‘å¾Œå…¼å®¹ï¼šå¦‚æœåªæä¾›å–®ä¸€æ”¶ä»¶äºº
    if recipient_emails is None:
        if recipient_email:
            recipient_emails = [recipient_email]
        else:
            recipient_emails = [config.email_account]

    # ç”Ÿæˆéƒµä»¶å…§å®¹ï¼ˆåŒå‰ï¼‰
    html_body = formatter.format_digest(...)
    text_body = formatter.format_digest_text(...)

    # ç™¼é€çµ¦å¤šå€‹æ”¶ä»¶äºº
    results = []
    for email in recipient_emails:
        result = sender.send(
            to_email=email,
            subject=f"InsightCosmos Daily Digest - {today_str}",
            html_body=html_body,
            text_body=text_body
        )
        results.append({
            "email": email,
            "status": result["status"]
        })

    return {
        "status": "success" if all(r["status"] == "success" for r in results) else "partial",
        "recipients": results,
        "total_sent": sum(1 for r in results if r["status"] == "success"),
        "total_failed": sum(1 for r in results if r["status"] != "success")
    }
```

### æ–¹æ³• 2: ä½¿ç”¨ç’°å¢ƒè®Šæ•¸é…ç½®å¤šå€‹æ”¶ä»¶äºº

ç·¨è¼¯ `.env` æª”æ¡ˆï¼š

```bash
# .env

# ä¸»è¦æ”¶ä»¶äººï¼ˆä½ è‡ªå·±ï¼‰
EMAIL_ACCOUNT=your_email@gmail.com

# é¡å¤–æ”¶ä»¶äººåˆ—è¡¨ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰
ADDITIONAL_RECIPIENTS=colleague1@example.com,colleague2@example.com,team@company.com

# æˆ–ä½¿ç”¨ CC/BCC
EMAIL_CC=colleague@example.com
EMAIL_BCC=archive@company.com
```

ç„¶å¾Œä¿®æ”¹ `src/tools/email_sender.py` æ”¯æ´ CC/BCCï¼š

```python
# src/tools/email_sender.py

def send(
    self,
    to_email: str,
    subject: str,
    html_body: Optional[str] = None,
    text_body: Optional[str] = None,
    cc: Optional[List[str]] = None,  # æ–°å¢
    bcc: Optional[List[str]] = None,  # æ–°å¢
    retry_count: int = 3
) -> Dict[str, Any]:
    # ...
    message = self._create_message(to_email, subject, html_body, text_body, cc, bcc)
    # ...

def _create_message(
    self,
    to_email: str,
    subject: str,
    html_body: Optional[str],
    text_body: Optional[str],
    cc: Optional[List[str]] = None,
    bcc: Optional[List[str]] = None
) -> MIMEMultipart:
    message = MIMEMultipart('alternative')
    message['From'] = self.config.sender_email
    message['To'] = to_email
    message['Subject'] = subject

    # æ–°å¢ CC/BCC æ”¯æ´
    if cc:
        message['Cc'] = ', '.join(cc)
    if bcc:
        message['Bcc'] = ', '.join(bcc)

    # ... rest of the code
```

### æ–¹æ³• 3: åœ˜éšŠå…±ç”¨éƒµä»¶åˆ—è¡¨

ä½¿ç”¨éƒµä»¶æœå‹™å•†çš„éƒµä»¶åˆ—è¡¨åŠŸèƒ½ï¼š

1. **Gmail**: å‰µå»º Google Group
   - è¨ªå• [groups.google.com](https://groups.google.com)
   - å‰µå»ºç¾¤çµ„ï¼ˆä¾‹å¦‚ï¼š`insightcosmos-team@googlegroups.com`ï¼‰
   - å°‡åœ˜éšŠæˆå“¡åŠ å…¥ç¾¤çµ„
   - å°‡ `.env` ä¸­çš„ `EMAIL_ACCOUNT` è¨­ç‚ºç¾¤çµ„éƒµä»¶åœ°å€

2. **è‡ªå»ºéƒµä»¶åˆ—è¡¨**:
   ```bash
   # .env
   EMAIL_ACCOUNT=team-digest@your-company.com
   ```
   è®“ IT éƒ¨é–€è¨­å®š `team-digest@your-company.com` è½‰ç™¼çµ¦æ‰€æœ‰åœ˜éšŠæˆå“¡

---

## ğŸ” æ“´å¤§è³‡æ–™æœé›†ç¯„åœ

### 1. å¢åŠ  RSS Feeds æ•¸é‡

```python
# src/agents/scout_agent.py

DEFAULT_FEEDS = [
    # å­¸è¡“ä¾†æº (10+)
    "https://arxiv.org/rss/cs.AI",
    "https://arxiv.org/rss/cs.LG",
    "https://arxiv.org/rss/cs.CL",
    "https://arxiv.org/rss/cs.CV",
    "https://proceedings.mlr.press/feed.xml",

    # å…¬å¸èˆ‡ç ”ç©¶æ©Ÿæ§‹ (20+)
    "https://openai.com/blog/rss",
    "https://www.anthropic.com/blog/rss",
    "https://www.deepmind.com/blog/rss.xml",
    "https://ai.meta.com/blog/rss/",
    "https://aws.amazon.com/blogs/machine-learning/feed/",
    "https://azure.microsoft.com/en-us/blog/topics/ai/feed/",

    # é–‹æºç¤¾ç¾¤ (10+)
    "https://huggingface.co/blog/feed.xml",
    "https://blog.langchain.dev/feed/",
    "https://www.llamaindex.ai/blog/rss.xml",

    # åª’é«”èˆ‡æ–°è (10+)
    "https://techcrunch.com/category/artificial-intelligence/feed/",
    "https://venturebeat.com/category/ai/feed/",
    "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
]
```

**æ³¨æ„äº‹é …**:
- RSS feed è¶Šå¤šï¼ŒæŠ“å–æ™‚é–“è¶Šé•·
- å»ºè­°åˆ†æ‰¹æ¸¬è©¦ï¼Œç¢ºä¿æ¯å€‹ feed éƒ½æœ‰æ•ˆ
- å¯èƒ½éœ€è¦èª¿æ•´ timeout è¨­å®šï¼ˆè¦‹ä¸‹æ–¹ï¼‰

### 2. å¢åŠ æœç´¢æŸ¥è©¢æ•¸é‡èˆ‡é »ç‡

```python
# src/agents/scout_agent.py

# å¢åŠ æœç´¢æŸ¥è©¢
DEFAULT_SEARCH_QUERIES = [
    # åŸæœ‰æŸ¥è©¢ (5-10å€‹)
    # ...

    # æ–°å¢æŸ¥è©¢ (10-20å€‹)
    # æŒ‰ä¸»é¡Œåˆ†é¡
    "large language model benchmarks",
    "AI agent reasoning capabilities",
    "vision transformer architecture",
    # ...
]

# èª¿æ•´æœç´¢åƒæ•¸
def search_articles(queries: List[str], max_results_per_query: int = 5):
    """
    Args:
        max_results_per_query: æ¯å€‹æŸ¥è©¢çš„æœ€å¤§çµæœæ•¸ï¼ˆé è¨­ 5ï¼Œå¯èª¿æ•´è‡³ 10-20ï¼‰
    """
```

### 3. æ–°å¢å…¶ä»–è³‡æ–™ä¾†æº

#### æ–¹æ³• A: æ–°å¢ Twitter/X APIï¼ˆéœ€ç”³è«‹ API Keyï¼‰

```python
# src/tools/twitter_fetcher.py

import tweepy
from typing import List, Dict

def fetch_tweets(
    keywords: List[str],
    max_tweets: int = 50
) -> List[Dict]:
    """
    å¾ Twitter æŠ“å–ç›¸é—œæ¨æ–‡

    éœ€è¦è¨­å®š .env:
        TWITTER_API_KEY=xxx
        TWITTER_API_SECRET=xxx
        TWITTER_ACCESS_TOKEN=xxx
        TWITTER_ACCESS_TOKEN_SECRET=xxx
    """
    # å¯¦ä½œç•¥
    pass
```

#### æ–¹æ³• B: æ–°å¢ Reddit API

```python
# src/tools/reddit_fetcher.py

import praw
from typing import List, Dict

def fetch_reddit_posts(
    subreddits: List[str] = ["MachineLearning", "artificial", "LocalLLaMA"],
    time_filter: str = "day",  # day, week, month
    limit: int = 50
) -> List[Dict]:
    """
    å¾ Reddit æŠ“å–ç†±é–€è²¼æ–‡

    éœ€è¦è¨­å®š .env:
        REDDIT_CLIENT_ID=xxx
        REDDIT_CLIENT_SECRET=xxx
        REDDIT_USER_AGENT=InsightCosmos/1.0
    """
    # å¯¦ä½œç•¥
    pass
```

#### æ–¹æ³• C: æ–°å¢ GitHub Trending

```python
# src/tools/github_fetcher.py

import requests
from typing import List, Dict

def fetch_trending_repos(
    language: str = "python",
    since: str = "daily"  # daily, weekly, monthly
) -> List[Dict]:
    """
    å¾ GitHub Trending æŠ“å–ç†±é–€å°ˆæ¡ˆ

    ä½¿ç”¨ GitHub Trending API (éå®˜æ–¹):
    https://github-trending-api.now.sh/repositories?language=python&since=daily
    """
    url = f"https://github-trending-api.now.sh/repositories"
    params = {"language": language, "since": since}
    response = requests.get(url, params=params)
    return response.json()
```

ç„¶å¾Œåœ¨ `src/agents/scout_agent.py` ä¸­æ•´åˆï¼š

```python
# src/agents/scout_agent.py

from src.tools.twitter_fetcher import fetch_tweets
from src.tools.reddit_fetcher import fetch_reddit_posts
from src.tools.github_fetcher import fetch_trending_repos

def collect_articles(
    user_prompt: str = None,
    enable_twitter: bool = False,
    enable_reddit: bool = True,
    enable_github: bool = True
) -> Dict[str, Any]:
    articles = []

    # åŸæœ‰ä¾†æº
    articles.extend(fetch_rss_articles(...))
    articles.extend(search_articles(...))

    # æ–°å¢ä¾†æºï¼ˆå¯é¸ï¼‰
    if enable_twitter:
        articles.extend(fetch_tweets(...))

    if enable_reddit:
        articles.extend(fetch_reddit_posts(...))

    if enable_github:
        articles.extend(fetch_trending_repos(...))

    # å»é‡èˆ‡è¿”å›
    return deduplicate_and_return(articles)
```

### 4. èª¿æ•´æ”¶é›†é »ç‡

**æ¯æ—¥å¤šæ¬¡æ”¶é›†**: ä¿®æ”¹ cron job æˆ–ç³»çµ±æ’ç¨‹

```bash
# crontab -e

# æ¯å¤©åŸ·è¡Œ 3 æ¬¡ï¼ˆæ—©ä¸Š 8 é»ã€ä¸­åˆ 12 é»ã€ä¸‹åˆ 6 é»ï¼‰
0 8 * * * cd /path/to/InsightCosmos && /path/to/venv/bin/python -m src.orchestrator.daily_runner
0 12 * * * cd /path/to/InsightCosmos && /path/to/venv/bin/python -m src.orchestrator.daily_runner
0 18 * * * cd /path/to/InsightCosmos && /path/to/venv/bin/python -m src.orchestrator.daily_runner
```

**å³æ™‚ç›£æ§**: å¯¦ä½œ webhook æˆ–æŒçºŒç›£æ§

```python
# src/orchestrator/realtime_monitor.py

import time
from datetime import datetime, timedelta

def realtime_monitor(check_interval_minutes: int = 30):
    """
    æ¯ N åˆ†é˜æª¢æŸ¥ä¸€æ¬¡æ–°å…§å®¹
    """
    while True:
        print(f"[{datetime.now()}] Checking for new content...")

        # åŸ·è¡Œæ”¶é›†
        result = run_daily_pipeline(dry_run=False)

        # å¦‚æœæœ‰æ–°å…§å®¹ï¼Œç™¼é€é€šçŸ¥
        if result["stats"]["phase1_stored"] > 0:
            print(f"Found {result['stats']['phase1_stored']} new articles!")

        # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
        time.sleep(check_interval_minutes * 60)
```

---

## ğŸ¤– æ›´æ› LLM æ¨¡å‹

InsightCosmos ä½¿ç”¨ **Google ADK (Agent Development Kit)**ï¼Œæ”¯æ´å¤šç¨® LLM æ¨¡å‹ã€‚

### 1. ä½¿ç”¨ä¸åŒçš„ Gemini æ¨¡å‹

ç·¨è¼¯ `src/agents/analyst_agent.py` å’Œ `src/agents/curator_daily.py`ï¼š

```python
# src/agents/analyst_agent.py

from google.adk.genai import Gemini

def create_analyst_agent(...):
    agent = LlmAgent(
        name="AnalystAgent",
        # é¸é … 1: Gemini 2.0 Flash (é è¨­ï¼Œå¿«é€Ÿï¼Œä¾¿å®œ)
        model=Gemini(model="gemini-2.0-flash-exp"),

        # é¸é … 2: Gemini 2.5 Flash (æ›´å¿«ï¼Œæœ€æ–°)
        # model=Gemini(model="gemini-2.5-flash-lite"),

        # é¸é … 3: Gemini 2.5 Pro (æ›´å¼·å¤§ï¼Œä½†æ›´è²´æ›´æ…¢)
        # model=Gemini(model="gemini-2.5-pro-preview"),

        # é¸é … 4: Gemini 1.5 Pro (ç©©å®šç‰ˆ)
        # model=Gemini(model="gemini-1.5-pro"),

        instruction=ANALYST_INSTRUCTION,
        tools=[],
        output_key="analysis"
    )
    return agent
```

### 2. ä½¿ç”¨ OpenAI GPT æ¨¡å‹ï¼ˆéœ€é¡å¤–é…ç½®ï¼‰

ADK ä¹Ÿæ”¯æ´ OpenAI æ¨¡å‹ã€‚éœ€å…ˆå®‰è£ä¸¦é…ç½®ï¼š

```bash
# .env
OPENAI_API_KEY=sk-xxx
```

```python
# src/agents/analyst_agent.py

from google.adk.openai import OpenAI  # æ³¨æ„ï¼šå¾ ADK å°å…¥

def create_analyst_agent(...):
    agent = LlmAgent(
        name="AnalystAgent",
        # ä½¿ç”¨ OpenAI GPT-4o
        model=OpenAI(model="gpt-4o"),

        # æˆ– GPT-4o-mini (æ›´ä¾¿å®œ)
        # model=OpenAI(model="gpt-4o-mini"),

        # æˆ– GPT-4 Turbo
        # model=OpenAI(model="gpt-4-turbo"),

        instruction=ANALYST_INSTRUCTION,
        tools=[],
        output_key="analysis"
    )
    return agent
```

### 3. ä½¿ç”¨ Anthropic Claude æ¨¡å‹

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-xxx
```

```python
# src/agents/analyst_agent.py

from google.adk.anthropic import Anthropic

def create_analyst_agent(...):
    agent = LlmAgent(
        name="AnalystAgent",
        # ä½¿ç”¨ Claude 3.7 Sonnet
        model=Anthropic(model="claude-3-7-sonnet-20250219"),

        # æˆ– Claude 3.5 Opus (æœ€å¼·å¤§)
        # model=Anthropic(model="claude-3-5-opus"),

        # æˆ– Claude 3.5 Haiku (æœ€ä¾¿å®œ)
        # model=Anthropic(model="claude-3-5-haiku"),

        instruction=ANALYST_INSTRUCTION,
        tools=[],
        output_key="analysis"
    )
    return agent
```

### 4. æ··åˆä½¿ç”¨å¤šå€‹æ¨¡å‹

ä¸åŒ Agent ä½¿ç”¨ä¸åŒæ¨¡å‹ï¼š

```python
# src/agents/analyst_agent.py
# ä½¿ç”¨ Claude Sonnet (æ¨ç†èƒ½åŠ›å¼·)
model=Anthropic(model="claude-3-7-sonnet-20250219")

# src/agents/curator_daily.py
# ä½¿ç”¨ Gemini Flash (ç”Ÿæˆå ±å‘Šå¿«)
model=Gemini(model="gemini-2.0-flash-exp")

# src/agents/scout_agent.py
# ä¸éœ€è¦ LLMï¼ˆç´”å·¥å…·èª¿ç”¨ï¼‰
```

### 5. æœ¬åœ° LLMï¼ˆOllamaï¼‰

å¦‚æœæƒ³ä½¿ç”¨æœ¬åœ° LLMï¼ˆç¯€çœæˆæœ¬ï¼‰ï¼Œå¯ä½¿ç”¨ Ollamaï¼š

```bash
# å®‰è£ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# ä¸‹è¼‰æ¨¡å‹
ollama pull llama3.3:70b
# æˆ– ollama pull qwen2.5:72b
```

ç„¶å¾Œå‰µå»º ADK ç›¸å®¹çš„åŒ…è£å™¨ï¼š

```python
# src/utils/ollama_wrapper.py

from google.adk.llm import LLM
import requests
from typing import Dict, Any

class OllamaLLM(LLM):
    def __init__(self, model: str = "llama3.3:70b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url

    def generate(self, prompt: str, **kwargs) -> str:
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={"model": self.model, "prompt": prompt, "stream": False}
        )
        return response.json()["response"]
```

ä½¿ç”¨ï¼š

```python
# src/agents/analyst_agent.py

from src.utils.ollama_wrapper import OllamaLLM

def create_analyst_agent(...):
    agent = LlmAgent(
        name="AnalystAgent",
        model=OllamaLLM(model="llama3.3:70b"),
        instruction=ANALYST_INSTRUCTION,
        tools=[],
        output_key="analysis"
    )
    return agent
```

### æ¨¡å‹é¸æ“‡å»ºè­°

| å ´æ™¯ | æ¨è–¦æ¨¡å‹ | åŸå›  |
|-----|---------|------|
| æˆæœ¬å„ªå…ˆ | Gemini 2.5 Flash Lite | æœ€ä¾¿å®œï¼Œé€Ÿåº¦å¿« |
| é€Ÿåº¦å„ªå…ˆ | Gemini 2.0 Flash | è¶…å¿«éŸ¿æ‡‰ï¼Œé©åˆå³æ™‚ |
| å“è³ªå„ªå…ˆ | Claude 3.7 Sonnet | æ¨ç†èƒ½åŠ›å¼·ï¼Œæ–‡å­—å“è³ªé«˜ |
| å¹³è¡¡é¸æ“‡ | GPT-4o | é€Ÿåº¦èˆ‡å“è³ªå…¼é¡§ |
| éš±ç§å„ªå…ˆ | Ollama (Local) | æœ¬åœ°é‹è¡Œï¼Œç„¡æ•¸æ“šå¤–æ´©é¢¨éšª |
| æ¸¬è©¦é–‹ç™¼ | Gemini Flash | å…è²»é¡åº¦é«˜ï¼Œé©åˆé–‹ç™¼ |

---

## ğŸ“Š è³‡æ–™ç®¡ç†

### 1. æŸ¥çœ‹è³‡æ–™åº«å…§å®¹

#### æ–¹æ³• A: ä½¿ç”¨ SQLite å‘½ä»¤åˆ—å·¥å…·

```bash
# é€²å…¥è³‡æ–™åº«
sqlite3 data/insights.db

# æŸ¥çœ‹æ‰€æœ‰æ–‡ç« 
SELECT id, title, source, status, priority_score, created_at FROM articles ORDER BY created_at DESC LIMIT 10;

# æŸ¥çœ‹é«˜å„ªå…ˆåº¦æ–‡ç« 
SELECT id, title, priority_score FROM articles WHERE priority_score >= 0.8 ORDER BY priority_score DESC;

# æŸ¥çœ‹å„ä¾†æºçš„æ–‡ç« æ•¸é‡
SELECT source_name, COUNT(*) as count FROM articles GROUP BY source_name ORDER BY count DESC;

# æŸ¥çœ‹å„ç‹€æ…‹çš„æ–‡ç« æ•¸é‡
SELECT status, COUNT(*) as count FROM articles GROUP BY status;

# åŒ¯å‡ºç‚º CSV
.headers on
.mode csv
.output articles_export.csv
SELECT * FROM articles WHERE created_at > date('now', '-7 days');
.output stdout
```

#### æ–¹æ³• B: ä½¿ç”¨ Python è…³æœ¬

å‰µå»ºæŸ¥è©¢å·¥å…·ï¼š

```python
# scripts/query_database.py

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.memory.database import Database
from src.utils.config import Config
import json

def main():
    config = Config.from_env()
    db = Database.from_config(config)

    # æŸ¥è©¢æœ€è¿‘ 7 å¤©çš„é«˜åˆ†æ–‡ç« 
    query = """
    SELECT id, title, source_name, priority_score, created_at
    FROM articles
    WHERE created_at > date('now', '-7 days')
      AND priority_score >= 0.7
    ORDER BY priority_score DESC, created_at DESC
    """

    results = db.conn.execute(query).fetchall()

    print(f"Found {len(results)} high-priority articles:")
    print("=" * 80)

    for row in results:
        print(f"[{row[4]}] ({row[3]:.2f}) {row[1]}")
        print(f"  Source: {row[2]}")
        print()

if __name__ == "__main__":
    main()
```

ä½¿ç”¨ï¼š

```bash
python scripts/query_database.py
```

#### æ–¹æ³• C: ä½¿ç”¨ SQLite GUI å·¥å…·

æ¨è–¦å·¥å…·ï¼š
- **DB Browser for SQLite** (å…è²»): https://sqlitebrowser.org/
- **TablePlus** (Mac/Windows): https://tableplus.com/
- **DataGrip** (JetBrains): https://www.jetbrains.com/datagrip/

æ­¥é©Ÿï¼š
1. ä¸‹è¼‰ä¸¦å®‰è£å·¥å…·
2. é–‹å•Ÿ `data/insights.db`
3. ä½¿ç”¨ GUI ç€è¦½ã€æŸ¥è©¢ã€åŒ¯å‡ºè³‡æ–™

### 2. æ¸…ç†èˆŠè³‡æ–™

```python
# scripts/cleanup_old_data.py

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.memory.database import Database
from src.utils.config import Config
from datetime import datetime, timedelta

def cleanup_old_articles(days: int = 90):
    """
    åˆªé™¤ N å¤©å‰çš„æ–‡ç« 

    Args:
        days: ä¿ç•™æœ€è¿‘ N å¤©çš„è³‡æ–™ï¼ˆé è¨­ 90 å¤©ï¼‰
    """
    config = Config.from_env()
    db = Database.from_config(config)

    cutoff_date = datetime.now() - timedelta(days=days)

    # è¨ˆç®—è¦åˆªé™¤çš„æ•¸é‡
    count_query = "SELECT COUNT(*) FROM articles WHERE created_at < ?"
    count = db.conn.execute(count_query, (cutoff_date,)).fetchone()[0]

    print(f"Found {count} articles older than {days} days")

    if count == 0:
        print("No articles to delete")
        return

    # ç¢ºèª
    confirm = input(f"Delete {count} articles? (yes/no): ")
    if confirm.lower() != 'yes':
        print("Cancelled")
        return

    # åˆªé™¤æ–‡ç« 
    delete_query = "DELETE FROM articles WHERE created_at < ?"
    db.conn.execute(delete_query, (cutoff_date,))
    db.conn.commit()

    print(f"Deleted {count} articles")

    # VACUUM é‡‹æ”¾ç©ºé–“
    print("Optimizing database...")
    db.conn.execute("VACUUM")
    print("Done!")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Clean up old articles")
    parser.add_argument("--days", type=int, default=90, help="Keep articles from last N days")
    args = parser.parse_args()

    cleanup_old_articles(args.days)
```

ä½¿ç”¨ï¼š

```bash
# åˆªé™¤ 90 å¤©å‰çš„è³‡æ–™
python scripts/cleanup_old_data.py

# åˆªé™¤ 30 å¤©å‰çš„è³‡æ–™
python scripts/cleanup_old_data.py --days 30
```

### 3. åŒ¯å‡ºè³‡æ–™

```python
# scripts/export_data.py

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.memory.database import Database
from src.utils.config import Config
import json
import csv
from datetime import datetime

def export_to_json(output_file: str = "export.json"):
    """åŒ¯å‡ºè³‡æ–™ç‚º JSON"""
    config = Config.from_env()
    db = Database.from_config(config)

    query = "SELECT * FROM articles ORDER BY created_at DESC"
    results = db.conn.execute(query).fetchall()

    # å–å¾—æ¬„ä½åç¨±
    columns = [description[0] for description in db.conn.execute(query).description]

    # è½‰æ›ç‚º dict list
    data = [dict(zip(columns, row)) for row in results]

    # å¯«å…¥ JSON
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=str)

    print(f"Exported {len(data)} articles to {output_file}")

def export_to_csv(output_file: str = "export.csv"):
    """åŒ¯å‡ºè³‡æ–™ç‚º CSV"""
    config = Config.from_env()
    db = Database.from_config(config)

    query = "SELECT * FROM articles ORDER BY created_at DESC"
    results = db.conn.execute(query).fetchall()

    # å–å¾—æ¬„ä½åç¨±
    columns = [description[0] for description in db.conn.execute(query).description]

    # å¯«å…¥ CSV
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(columns)
        writer.writerows(results)

    print(f"Exported {len(results)} articles to {output_file}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Export database")
    parser.add_argument("--format", choices=["json", "csv"], default="json", help="Export format")
    parser.add_argument("--output", help="Output file name")
    args = parser.parse_args()

    if args.format == "json":
        output = args.output or f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        export_to_json(output)
    else:
        output = args.output or f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        export_to_csv(output)
```

ä½¿ç”¨ï¼š

```bash
# åŒ¯å‡ºç‚º JSON
python scripts/export_data.py --format json

# åŒ¯å‡ºç‚º CSV
python scripts/export_data.py --format csv --output my_data.csv
```

---

## ğŸš€ éƒ¨ç½²æŒ‡å—

### 1. éƒ¨ç½²åˆ° Heroku

#### æ­¥é©Ÿ 1: æº–å‚™ Heroku å°ˆæ¡ˆ

```bash
# å®‰è£ Heroku CLI
# Mac: brew install heroku/brew/heroku
# Windows: choco install heroku-cli
# Linux: curl https://cli-assets.heroku.com/install.sh | sh

# ç™»å…¥
heroku login

# å‰µå»ºæ‡‰ç”¨
cd InsightCosmos
heroku create insightcosmos-yourname
```

#### æ­¥é©Ÿ 2: é…ç½®ç’°å¢ƒè®Šæ•¸

```bash
# è¨­å®šç’°å¢ƒè®Šæ•¸
heroku config:set GOOGLE_API_KEY=your_api_key
heroku config:set EMAIL_ACCOUNT=your_email@gmail.com
heroku config:set EMAIL_PASSWORD=your_app_password
heroku config:set USER_NAME=Ray
heroku config:set USER_INTERESTS="AI,Robotics,Multi-Agent Systems"
heroku config:set DATABASE_PATH=/app/data/insights.db
heroku config:set LOG_LEVEL=INFO

# æŸ¥çœ‹é…ç½®
heroku config
```

#### æ­¥é©Ÿ 3: å‰µå»º Heroku æ‰€éœ€æª”æ¡ˆ

**Procfile**ï¼ˆå‘Šè¨´ Heroku å¦‚ä½•é‹è¡Œï¼‰:

```bash
# Procfile
release: python -m src.memory.database
worker: python -m src.orchestrator.daily_runner
```

**runtime.txt**ï¼ˆæŒ‡å®š Python ç‰ˆæœ¬ï¼‰:

```
python-3.11.0
```

**æ·»åŠ  PostgreSQLï¼ˆå¯é¸ï¼Œç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼‰**:

```bash
# æ·»åŠ  PostgreSQL æ’ä»¶ï¼ˆHeroku å…è²»ç‰ˆé™åˆ¶ 10,000 è¡Œï¼‰
heroku addons:create heroku-postgresql:mini

# æŸ¥çœ‹è³‡æ–™åº« URL
heroku config:get DATABASE_URL
```

å¦‚æœä½¿ç”¨ PostgreSQLï¼Œéœ€è¦ä¿®æ”¹ `src/memory/database.py` æ”¯æ´ PostgreSQLï¼š

```python
# src/memory/database.py

import os

class Database:
    def __init__(self, database_url: str = None):
        if database_url is None:
            # æœ¬åœ°ä½¿ç”¨ SQLite
            database_url = os.getenv("DATABASE_PATH", "data/insights.db")
            self.conn = sqlite3.connect(database_url)
        else:
            # Heroku ä½¿ç”¨ PostgreSQL
            import psycopg2
            self.conn = psycopg2.connect(database_url, sslmode='require')

        # ... rest of the code
```

#### æ­¥é©Ÿ 4: éƒ¨ç½²

```bash
# æäº¤ç¨‹å¼ç¢¼
git add .
git commit -m "Prepare for Heroku deployment"

# éƒ¨ç½²åˆ° Heroku
git push heroku main

# æŸ¥çœ‹æ—¥èªŒ
heroku logs --tail

# æ‰‹å‹•åŸ·è¡Œä¸€æ¬¡ pipeline
heroku run python -m src.orchestrator.daily_runner --dry-run
```

#### æ­¥é©Ÿ 5: è¨­å®šå®šæ™‚ä»»å‹™ï¼ˆHeroku Schedulerï¼‰

```bash
# å®‰è£ Scheduler æ’ä»¶
heroku addons:create scheduler:standard

# é–‹å•Ÿ Scheduler Dashboard
heroku addons:open scheduler
```

åœ¨ Dashboard ä¸­æ·»åŠ ä»»å‹™ï¼š
- **Command**: `python -m src.orchestrator.daily_runner`
- **Frequency**: Daily at 8:00 AM (é¸æ“‡ä½ æƒ³è¦çš„æ™‚é–“)

#### æ­¥é©Ÿ 6: ç›£æ§èˆ‡ç¶­è­·

```bash
# æŸ¥çœ‹æ‡‰ç”¨ç‹€æ…‹
heroku ps

# æŸ¥çœ‹æ—¥èªŒ
heroku logs --tail

# é‡å•Ÿæ‡‰ç”¨
heroku restart

# æ“´å±• workerï¼ˆå¦‚éœ€è¦ï¼‰
heroku ps:scale worker=1
```

### 2. éƒ¨ç½²åˆ° AWS Lambdaï¼ˆServerlessï¼‰

**å„ªé»**:
- æŒ‰åŸ·è¡Œæ¬¡æ•¸ä»˜è²»ï¼ˆæ›´ä¾¿å®œï¼‰
- è‡ªå‹•æ“´å±•
- ç„¡éœ€ç®¡ç†ä¼ºæœå™¨

**æ­¥é©Ÿ**:

1. å®‰è£ Serverless Framework:
```bash
npm install -g serverless
```

2. å‰µå»º `serverless.yml`:
```yaml
service: insightcosmos

provider:
  name: aws
  runtime: python3.11
  region: us-east-1
  environment:
    GOOGLE_API_KEY: ${env:GOOGLE_API_KEY}
    EMAIL_ACCOUNT: ${env:EMAIL_ACCOUNT}
    EMAIL_PASSWORD: ${env:EMAIL_PASSWORD}
    USER_NAME: Ray
    USER_INTERESTS: "AI,Robotics,Multi-Agent Systems"

functions:
  dailyPipeline:
    handler: handler.run_daily
    timeout: 900  # 15 minutes
    events:
      # æ¯å¤©æ—©ä¸Š 8 é»ï¼ˆUTCï¼‰åŸ·è¡Œ
      - schedule: cron(0 8 * * ? *)

package:
  exclude:
    - node_modules/**
    - venv/**
    - tests/**
```

3. å‰µå»º `handler.py`:
```python
# handler.py

def run_daily(event, context):
    from src.orchestrator.daily_runner import run_daily_pipeline
    result = run_daily_pipeline(dry_run=False)
    return {
        'statusCode': 200,
        'body': result
    }
```

4. éƒ¨ç½²:
```bash
serverless deploy
```

### 3. éƒ¨ç½²åˆ° Google Cloud Run

**å„ªé»**:
- å®Œå…¨è¨—ç®¡
- è‡ªå‹•æ“´å±•ï¼ˆåŒ…æ‹¬ç¸®æ¸›è‡³ 0ï¼‰
- èˆ‡ Google AI æ•´åˆè‰¯å¥½

**æ­¥é©Ÿ**:

1. å‰µå»º `Dockerfile`:
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½ç¨‹å¼ç¢¼
COPY . .

# åˆå§‹åŒ–è³‡æ–™åº«
RUN python -m src.memory.database

# é‹è¡Œ
CMD ["python", "-m", "src.orchestrator.daily_runner"]
```

2. éƒ¨ç½²åˆ° Cloud Run:
```bash
# å®‰è£ gcloud CLI
# https://cloud.google.com/sdk/docs/install

# ç™»å…¥
gcloud auth login

# è¨­å®šå°ˆæ¡ˆ
gcloud config set project YOUR_PROJECT_ID

# æ§‹å»ºä¸¦éƒ¨ç½²
gcloud run deploy insightcosmos \
  --source . \
  --region us-central1 \
  --platform managed \
  --allow-unauthenticated \
  --set-env-vars GOOGLE_API_KEY=xxx,EMAIL_ACCOUNT=xxx,EMAIL_PASSWORD=xxx
```

3. è¨­å®š Cloud Schedulerï¼ˆå®šæ™‚åŸ·è¡Œï¼‰:
```bash
gcloud scheduler jobs create http daily-pipeline \
  --schedule="0 8 * * *" \
  --uri="https://insightcosmos-xxx.run.app" \
  --http-method=GET
```

### 4. æœ¬åœ°éƒ¨ç½²ï¼ˆè‡ªå‹•åŒ–ï¼‰

å¦‚æœæƒ³åœ¨æœ¬åœ°é›»è…¦æˆ–ä¼ºæœå™¨æŒçºŒé‹è¡Œï¼š

#### æ–¹æ³• A: Cron Jobï¼ˆLinux/Macï¼‰

```bash
# ç·¨è¼¯ crontab
crontab -e

# æ·»åŠ å®šæ™‚ä»»å‹™ï¼ˆæ¯å¤©æ—©ä¸Š 8 é»åŸ·è¡Œï¼‰
0 8 * * * cd /path/to/InsightCosmos && /path/to/venv/bin/python -m src.orchestrator.daily_runner >> /path/to/logs/daily.log 2>&1

# ä¿å­˜ä¸¦é€€å‡º
```

**å®Œæ•´ç¯„ä¾‹**ï¼ˆå¸¶æ—¥èªŒè¼ªæ›¿ï¼‰:

```bash
# æ¯å¤©æ—©ä¸Š 8 é»åŸ·è¡Œ
0 8 * * * cd /Users/ray/sides/InsightCosmos && /Users/ray/sides/InsightCosmos/venv/bin/python -m src.orchestrator.daily_runner >> /Users/ray/sides/InsightCosmos/logs/daily_$(date +\%Y\%m\%d).log 2>&1

# æ¯é€±æ—¥æ™šä¸Š 8 é»åŸ·è¡Œé€±å ±
0 20 * * 0 cd /Users/ray/sides/InsightCosmos && /Users/ray/sides/InsightCosmos/venv/bin/python -m src.orchestrator.weekly_runner >> /Users/ray/sides/InsightCosmos/logs/weekly_$(date +\%Y\%m\%d).log 2>&1

# æ¯æœˆ 1 è™Ÿæ¸…ç† 90 å¤©å‰çš„è³‡æ–™
0 0 1 * * cd /Users/ray/sides/InsightCosmos && /Users/ray/sides/InsightCosmos/venv/bin/python scripts/cleanup_old_data.py --days 90 >> /Users/ray/sides/InsightCosmos/logs/cleanup.log 2>&1
```

#### æ–¹æ³• B: systemd Serviceï¼ˆLinuxï¼‰

å‰µå»º systemd service æ–‡ä»¶ï¼š

```ini
# /etc/systemd/system/insightcosmos.service

[Unit]
Description=InsightCosmos Daily Pipeline
After=network.target

[Service]
Type=oneshot
User=youruser
WorkingDirectory=/path/to/InsightCosmos
ExecStart=/path/to/venv/bin/python -m src.orchestrator.daily_runner
StandardOutput=append:/path/to/logs/daily.log
StandardError=append:/path/to/logs/error.log

[Install]
WantedBy=multi-user.target
```

å‰µå»º timer æ–‡ä»¶ï¼š

```ini
# /etc/systemd/system/insightcosmos.timer

[Unit]
Description=Run InsightCosmos Daily at 8 AM

[Timer]
OnCalendar=daily
OnCalendar=08:00
Persistent=true

[Install]
WantedBy=timers.target
```

å•Ÿç”¨ service:

```bash
# é‡æ–°è¼‰å…¥ systemd
sudo systemctl daemon-reload

# å•Ÿç”¨ timer
sudo systemctl enable insightcosmos.timer

# å•Ÿå‹• timer
sudo systemctl start insightcosmos.timer

# æŸ¥çœ‹ç‹€æ…‹
sudo systemctl status insightcosmos.timer

# æŸ¥çœ‹æ—¥èªŒ
sudo journalctl -u insightcosmos.service -f
```

#### æ–¹æ³• C: Windows Task Scheduler

1. é–‹å•Ÿ Task Scheduler
2. å‰µå»ºåŸºæœ¬ä»»å‹™
3. è¨­å®šè§¸ç™¼å™¨ï¼ˆæ¯å¤©æ—©ä¸Š 8 é»ï¼‰
4. è¨­å®šå‹•ä½œï¼š
   - **Program**: `C:\path\to\InsightCosmos\venv\Scripts\python.exe`
   - **Arguments**: `-m src.orchestrator.daily_runner`
   - **Start in**: `C:\path\to\InsightCosmos`
5. å®Œæˆ

---

## ğŸ’¾ å‚™ä»½èˆ‡é‚„åŸ

### 1. è³‡æ–™åº«å‚™ä»½

#### è‡ªå‹•å‚™ä»½è…³æœ¬

```python
# scripts/backup_database.py

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import shutil
from datetime import datetime
from src.utils.config import Config

def backup_database(backup_dir: str = "backups"):
    """
    å‚™ä»½è³‡æ–™åº«

    Args:
        backup_dir: å‚™ä»½ç›®éŒ„
    """
    config = Config.from_env()
    db_path = Path(config.database_path)

    if not db_path.exists():
        print(f"Database not found: {db_path}")
        return

    # å‰µå»ºå‚™ä»½ç›®éŒ„
    backup_path = Path(backup_dir)
    backup_path.mkdir(exist_ok=True)

    # å‚™ä»½æª”æ¡ˆåç¨±ï¼ˆåŒ…å«æ™‚é–“æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = backup_path / f"insights_backup_{timestamp}.db"

    # è¤‡è£½è³‡æ–™åº«
    print(f"Backing up database to {backup_file}...")
    shutil.copy2(db_path, backup_file)

    # å£“ç¸®å‚™ä»½ï¼ˆå¯é¸ï¼‰
    import gzip
    with open(backup_file, 'rb') as f_in:
        with gzip.open(f"{backup_file}.gz", 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

    # åˆªé™¤æœªå£“ç¸®çš„å‚™ä»½
    backup_file.unlink()

    print(f"Backup completed: {backup_file}.gz")
    print(f"Backup size: {(backup_path / f'{backup_file.name}.gz').stat().st_size / 1024 / 1024:.2f} MB")

    # æ¸…ç†èˆŠå‚™ä»½ï¼ˆä¿ç•™æœ€è¿‘ 30 å€‹ï¼‰
    cleanup_old_backups(backup_path, keep=30)

def cleanup_old_backups(backup_dir: Path, keep: int = 30):
    """
    æ¸…ç†èˆŠå‚™ä»½ï¼Œä¿ç•™æœ€è¿‘ N å€‹

    Args:
        backup_dir: å‚™ä»½ç›®éŒ„
        keep: ä¿ç•™æ•¸é‡
    """
    backups = sorted(backup_dir.glob("insights_backup_*.db.gz"), reverse=True)

    if len(backups) <= keep:
        return

    print(f"\nCleaning up old backups (keeping {keep} most recent)...")
    for backup in backups[keep:]:
        print(f"  Deleting: {backup.name}")
        backup.unlink()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Backup InsightCosmos database")
    parser.add_argument("--dir", default="backups", help="Backup directory")
    args = parser.parse_args()

    backup_database(args.dir)
```

ä½¿ç”¨ï¼š

```bash
# æ‰‹å‹•å‚™ä»½
python scripts/backup_database.py

# æŒ‡å®šå‚™ä»½ç›®éŒ„
python scripts/backup_database.py --dir /path/to/backups
```

#### è‡ªå‹•å®šæ™‚å‚™ä»½ï¼ˆCronï¼‰

```bash
# crontab -e

# æ¯å¤©å‡Œæ™¨ 2 é»å‚™ä»½
0 2 * * * cd /path/to/InsightCosmos && /path/to/venv/bin/python scripts/backup_database.py >> /path/to/logs/backup.log 2>&1

# æ¯é€±æ—¥å‡Œæ™¨ 3 é»å‚™ä»½åˆ°é ç«¯ï¼ˆä½¿ç”¨ rsyncï¼‰
0 3 * * 0 rsync -av /path/to/InsightCosmos/backups/ user@remote-server:/backups/insightcosmos/
```

### 2. å‚™ä»½åˆ°é›²ç«¯

#### å‚™ä»½åˆ° Google Drive

```python
# scripts/backup_to_gdrive.py

from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
import os

def backup_to_gdrive(backup_file: str, folder_id: str = None):
    """
    ä¸Šå‚³å‚™ä»½åˆ° Google Drive

    Args:
        backup_file: å‚™ä»½æª”æ¡ˆè·¯å¾‘
        folder_id: Google Drive è³‡æ–™å¤¾ IDï¼ˆå¯é¸ï¼‰

    éœ€è¦è¨­å®š Google Drive API:
    https://developers.google.com/drive/api/quickstart/python
    """
    # è¼‰å…¥æ†‘è­‰
    creds = Credentials.from_authorized_user_file('token.json', ['https://www.googleapis.com/auth/drive.file'])

    # å»ºç«‹ Drive API client
    service = build('drive', 'v3', credentials=creds)

    # ä¸Šå‚³æª”æ¡ˆ
    file_metadata = {
        'name': os.path.basename(backup_file),
        'parents': [folder_id] if folder_id else []
    }
    media = MediaFileUpload(backup_file, resumable=True)
    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()

    print(f"Uploaded to Google Drive: {file.get('id')}")

# ä½¿ç”¨
# backup_to_gdrive("backups/insights_backup_20251125_080000.db.gz", "YOUR_FOLDER_ID")
```

#### å‚™ä»½åˆ° AWS S3

```python
# scripts/backup_to_s3.py

import boto3
from pathlib import Path

def backup_to_s3(backup_file: str, bucket_name: str, prefix: str = "insightcosmos"):
    """
    ä¸Šå‚³å‚™ä»½åˆ° AWS S3

    Args:
        backup_file: å‚™ä»½æª”æ¡ˆè·¯å¾‘
        bucket_name: S3 bucket åç¨±
        prefix: S3 ç‰©ä»¶å‰ç¶´ï¼ˆè³‡æ–™å¤¾ï¼‰

    éœ€è¦è¨­å®š AWS æ†‘è­‰:
    aws configure
    """
    s3 = boto3.client('s3')

    # ç”Ÿæˆ S3 ç‰©ä»¶ key
    file_name = Path(backup_file).name
    s3_key = f"{prefix}/{file_name}"

    # ä¸Šå‚³
    print(f"Uploading {backup_file} to s3://{bucket_name}/{s3_key}...")
    s3.upload_file(backup_file, bucket_name, s3_key)
    print("Upload completed")

# ä½¿ç”¨
# backup_to_s3("backups/insights_backup_20251125_080000.db.gz", "my-backups-bucket")
```

### 3. é‚„åŸè³‡æ–™åº«

```python
# scripts/restore_database.py

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import shutil
import gzip
from datetime import datetime
from src.utils.config import Config

def list_backups(backup_dir: str = "backups"):
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å‚™ä»½"""
    backup_path = Path(backup_dir)
    backups = sorted(backup_path.glob("insights_backup_*.db.gz"), reverse=True)

    if not backups:
        print("No backups found")
        return []

    print("Available backups:")
    print("=" * 80)
    for i, backup in enumerate(backups, 1):
        timestamp = backup.stem.split("_")[-2:]
        date_str = f"{timestamp[0][:4]}-{timestamp[0][4:6]}-{timestamp[0][6:8]} {timestamp[1][:2]}:{timestamp[1][2:4]}:{timestamp[1][4:6]}"
        size_mb = backup.stat().st_size / 1024 / 1024
        print(f"{i}. {backup.name} ({date_str}, {size_mb:.2f} MB)")
    print("=" * 80)

    return backups

def restore_database(backup_file: str):
    """
    é‚„åŸè³‡æ–™åº«

    Args:
        backup_file: å‚™ä»½æª”æ¡ˆè·¯å¾‘ï¼ˆ.db.gzï¼‰
    """
    config = Config.from_env()
    db_path = Path(config.database_path)
    backup_path = Path(backup_file)

    if not backup_path.exists():
        print(f"Backup file not found: {backup_path}")
        return

    # å‚™ä»½ç•¶å‰è³‡æ–™åº«ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if db_path.exists():
        current_backup = db_path.parent / f"insights_before_restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        print(f"Backing up current database to {current_backup}...")
        shutil.copy2(db_path, current_backup)

    # è§£å£“ç¸®å‚™ä»½
    print(f"Restoring database from {backup_path}...")
    with gzip.open(backup_path, 'rb') as f_in:
        with open(db_path, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)

    print("Database restored successfully!")
    print(f"Database location: {db_path}")

def restore_interactive(backup_dir: str = "backups"):
    """äº’å‹•å¼é‚„åŸ"""
    backups = list_backups(backup_dir)

    if not backups:
        return

    # é¸æ“‡å‚™ä»½
    while True:
        try:
            choice = input("\nSelect backup to restore (number): ")
            idx = int(choice) - 1
            if 0 <= idx < len(backups):
                break
            else:
                print("Invalid selection")
        except (ValueError, KeyboardInterrupt):
            print("\nCancelled")
            return

    selected_backup = backups[idx]

    # ç¢ºèª
    print(f"\nYou are about to restore: {selected_backup.name}")
    confirm = input("This will overwrite the current database. Continue? (yes/no): ")

    if confirm.lower() != 'yes':
        print("Cancelled")
        return

    # é‚„åŸ
    restore_database(str(selected_backup))

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Restore InsightCosmos database")
    parser.add_argument("--dir", default="backups", help="Backup directory")
    parser.add_argument("--file", help="Specific backup file to restore")
    args = parser.parse_args()

    if args.file:
        restore_database(args.file)
    else:
        restore_interactive(args.dir)
```

ä½¿ç”¨ï¼š

```bash
# äº’å‹•å¼é‚„åŸï¼ˆæœƒåˆ—å‡ºæ‰€æœ‰å‚™ä»½ä¾›é¸æ“‡ï¼‰
python scripts/restore_database.py

# é‚„åŸç‰¹å®šå‚™ä»½
python scripts/restore_database.py --file backups/insights_backup_20251125_080000.db.gz
```

### 4. è³‡æ–™é·ç§»ï¼ˆSQLite â†’ PostgreSQLï¼‰

å¦‚æœè¦å¾ SQLite é·ç§»åˆ° PostgreSQLï¼ˆç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒï¼‰ï¼š

```bash
# å®‰è£ pgloader
# Mac: brew install pgloader
# Ubuntu: apt-get install pgloader

# é·ç§»
pgloader data/insights.db postgresql://user:password@localhost/insightcosmos
```

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•æ¸¬è©¦éƒµä»¶é…ç½®æ˜¯å¦æ­£ç¢ºï¼Ÿ

```bash
# ä½¿ç”¨æ¸¬è©¦è…³æœ¬
python -c "
from src.tools.email_sender import EmailSender, EmailConfig
from src.utils.config import Config

config = Config.from_env()
email_config = EmailConfig(
    sender_email=config.email_account,
    sender_password=config.email_password
)

sender = EmailSender(email_config)
result = sender.test_connection()
print(result)
"
```

### Q2: å¦‚ä½•æŸ¥çœ‹ Pipeline åŸ·è¡Œæ—¥èªŒï¼Ÿ

```bash
# æŸ¥çœ‹æœ€è¿‘çš„æ—¥èªŒ
tail -f logs/insightcosmos.log

# æœå°‹ç‰¹å®šéŒ¯èª¤
grep "ERROR" logs/insightcosmos.log

# æŸ¥çœ‹ç‰¹å®šæ—¥æœŸçš„æ—¥èªŒ
cat logs/insightcosmos.log | grep "2025-11-25"
```

### Q3: å¦‚ä½•é™åˆ¶ API ä½¿ç”¨æˆæœ¬ï¼Ÿ

```python
# src/utils/config.py

class Config:
    # æ·»åŠ æˆæœ¬æ§åˆ¶åƒæ•¸
    max_articles_per_day: int = 50  # é™åˆ¶æ¯æ—¥è™•ç†æ–‡ç« æ•¸
    max_api_calls_per_day: int = 100  # é™åˆ¶ API å‘¼å«æ¬¡æ•¸
    use_cache: bool = True  # å•Ÿç”¨å¿«å–
```

ç„¶å¾Œåœ¨ `daily_runner.py` ä¸­æ·»åŠ æª¢æŸ¥ï¼š

```python
# src/orchestrator/daily_runner.py

def _run_phase2_analyst(self) -> int:
    # æª¢æŸ¥ä»Šæ—¥å·²è™•ç†æ•¸é‡
    today_count = self.article_store.get_analyzed_count_today()

    if today_count >= self.config.max_articles_per_day:
        self.logger.warning(f"Reached daily limit: {today_count}/{self.config.max_articles_per_day}")
        return 0

    # é™åˆ¶è™•ç†æ•¸é‡
    pending_articles = self.article_store.get_by_status("collected")
    pending_articles = pending_articles[:self.config.max_articles_per_day - today_count]

    # ... ç¹¼çºŒè™•ç†
```

### Q4: å¦‚ä½•åŠ å¿« Pipeline åŸ·è¡Œé€Ÿåº¦ï¼Ÿ

**æ–¹æ³• 1**: å¹³è¡Œè™•ç†æ–‡ç« åˆ†æ

```python
# src/orchestrator/daily_runner.py

import asyncio
from concurrent.futures import ThreadPoolExecutor

async def _run_phase2_analyst_parallel(self) -> int:
    """å¹³è¡Œåˆ†ææ–‡ç« """
    pending_articles = self.article_store.get_by_status("collected")

    async def analyze_one(article_dict):
        # ... åˆ†æå–®ç¯‡æ–‡ç« 
        pass

    # ä½¿ç”¨ asyncio.gather å¹³è¡ŒåŸ·è¡Œ
    results = await asyncio.gather(*[analyze_one(article) for article in pending_articles])

    return sum(1 for r in results if r["status"] == "success")
```

**æ–¹æ³• 2**: ä½¿ç”¨æ›´å¿«çš„ LLM æ¨¡å‹

```python
# ä½¿ç”¨ Gemini 2.5 Flash (æœ€å¿«)
model=Gemini(model="gemini-2.5-flash-lite")
```

**æ–¹æ³• 3**: æ¸›å°‘è³‡æ–™ä¾†æº

- æ¸›å°‘ RSS feeds æ•¸é‡
- æ¸›å°‘æœç´¢æŸ¥è©¢æ•¸é‡
- èª¿æ•´ `max_articles` åƒæ•¸

### Q5: å¦‚ä½•è™•ç† Rate Limit éŒ¯èª¤ï¼Ÿ

```python
# src/agents/analyst_agent.py

import time
from google.api_core import retry

def create_analyst_agent(...):
    agent = LlmAgent(
        name="AnalystAgent",
        model=Gemini(
            model="gemini-2.0-flash-exp",
            # æ·»åŠ  retry é…ç½®
            retry=retry.Retry(
                initial=1.0,
                maximum=60.0,
                multiplier=2.0,
                deadline=300.0
            )
        ),
        # ...
    )
```

æˆ–åœ¨ orchestrator ä¸­æ·»åŠ å»¶é²ï¼š

```python
# src/orchestrator/daily_runner.py

for idx, article_dict in enumerate(pending_articles, 1):
    # æ¯åˆ†æ 5 ç¯‡æ–‡ç« ï¼Œæš«åœ 10 ç§’
    if idx % 5 == 0:
        self.logger.info("  Pausing to avoid rate limit...")
        time.sleep(10)

    # åˆ†ææ–‡ç« 
    result = runner.analyze_article(...)
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: ç„¡æ³•ç™¼é€éƒµä»¶ï¼ˆAuthentication Failedï¼‰

**ç—‡ç‹€**:
```
SMTPAuthenticationError: (535, b'5.7.8 Username and Password not accepted')
```

**è§£æ±ºæ–¹æ¡ˆ**:

1. ç¢ºèªä½¿ç”¨çš„æ˜¯ **App Password**ï¼ˆä¸æ˜¯å¸³è™Ÿå¯†ç¢¼ï¼‰
   - Gmail: https://support.google.com/accounts/answer/185833
   - å‰å¾€ Google å¸³æˆ¶ â†’ å®‰å…¨æ€§ â†’ å…©æ­¥é©Ÿé©—è­‰ â†’ æ‡‰ç”¨ç¨‹å¼å¯†ç¢¼
   - ç”Ÿæˆæ–°çš„æ‡‰ç”¨ç¨‹å¼å¯†ç¢¼
   - å°‡å¯†ç¢¼æ›´æ–°åˆ° `.env` çš„ `EMAIL_PASSWORD`

2. ç¢ºèª `.env` æ ¼å¼æ­£ç¢ºï¼ˆç„¡å¤šé¤˜ç©ºæ ¼ï¼‰:
   ```bash
   EMAIL_ACCOUNT=your_email@gmail.com
   EMAIL_PASSWORD=abcd efgh ijkl mnop  # æ³¨æ„ï¼šApp Password åŒ…å«ç©ºæ ¼æ˜¯æ­£å¸¸çš„
   ```

3. æ¸¬è©¦é€£ç·š:
   ```bash
   python -c "from src.tools.email_sender import *; sender = EmailSender(EmailConfig(sender_email='xxx', sender_password='xxx')); print(sender.test_connection())"
   ```

### å•é¡Œ 2: Google Search Grounding å¤±æ•—

**ç—‡ç‹€**:
```
Error: Google Search Grounding failed
```

**è§£æ±ºæ–¹æ¡ˆ**:

1. ç¢ºèª API Key æœ‰æ•ˆä¸”æœ‰ Search Grounding æ¬Šé™
   - è¨ªå• [Google AI Studio](https://aistudio.google.com/apikey)
   - ç¢ºèª API Key ç‹€æ…‹ç‚º "Active"
   - ç¢ºèªæœ‰è¶³å¤ çš„å…è²»é…é¡

2. æª¢æŸ¥ç¶²è·¯é€£ç·šï¼ˆSearch Grounding éœ€è¦ç¶²è·¯ï¼‰

3. å¦‚æœæŒçºŒå¤±æ•—ï¼Œåˆ‡æ›ç‚ºç´” RSS æ¨¡å¼ï¼š
   ```python
   # src/agents/scout_agent.py

   def collect_articles(...):
       # æš«æ™‚é—œé–‰ Google Search
       # articles.extend(search_articles(...))  # è¨»è§£æ‰

       # åªä½¿ç”¨ RSS
       articles.extend(fetch_rss_articles(...))
   ```

### å•é¡Œ 3: è³‡æ–™åº«é–å®šï¼ˆDatabase is lockedï¼‰

**ç—‡ç‹€**:
```
sqlite3.OperationalError: database is locked
```

**åŸå› **: å¤šå€‹ç¨‹åºåŒæ™‚å­˜å– SQLite è³‡æ–™åº«

**è§£æ±ºæ–¹æ¡ˆ**:

1. ç¢ºèªæ²’æœ‰å¤šå€‹ Pipeline åŒæ™‚é‹è¡Œ:
   ```bash
   ps aux | grep daily_runner
   # å¦‚æœæœ‰å¤šå€‹ï¼Œåˆªé™¤å¤šé¤˜çš„
   kill <PID>
   ```

2. å¢åŠ  SQLite timeout:
   ```python
   # src/memory/database.py

   self.conn = sqlite3.connect(
       database_path,
       timeout=30.0  # å¢åŠ  timeoutï¼ˆé è¨­ 5.0ï¼‰
   )
   ```

3. è€ƒæ…®é·ç§»åˆ° PostgreSQLï¼ˆç”Ÿç”¢ç’°å¢ƒæ¨è–¦ï¼‰

### å•é¡Œ 4: è¨˜æ†¶é«”ä¸è¶³ï¼ˆMemory Errorï¼‰

**ç—‡ç‹€**:
```
MemoryError: Unable to allocate array
```

**åŸå› **: è™•ç†å¤ªå¤šæ–‡ç« æˆ– embedding ä½”ç”¨éå¤šè¨˜æ†¶é«”

**è§£æ±ºæ–¹æ¡ˆ**:

1. æ¸›å°‘æ¯æ¬¡è™•ç†çš„æ–‡ç« æ•¸é‡:
   ```python
   # src/orchestrator/daily_runner.py

   # åˆ†æ‰¹è™•ç†ï¼Œæ¯æ‰¹ 10 ç¯‡
   batch_size = 10
   for i in range(0, len(pending_articles), batch_size):
       batch = pending_articles[i:i+batch_size]
       # è™•ç† batch
   ```

2. å®šæœŸæ¸…ç†èˆŠè³‡æ–™:
   ```bash
   python scripts/cleanup_old_data.py --days 30
   ```

3. å¢åŠ ç³»çµ± swapï¼ˆLinuxï¼‰:
   ```bash
   sudo fallocate -l 4G /swapfile
   sudo chmod 600 /swapfile
   sudo mkswap /swapfile
   sudo swapon /swapfile
   ```

### å•é¡Œ 5: RSS Feed ç„¡æ³•è®€å–

**ç—‡ç‹€**:
```
Error fetching RSS feed: HTTP 403 Forbidden
```

**åŸå› **: æŸäº›ç¶²ç«™å°é–çˆ¬èŸ²æˆ– feedparser çš„é è¨­ User-Agent

**è§£æ±ºæ–¹æ¡ˆ**:

ä¿®æ”¹ `src/tools/fetcher.py` æ·»åŠ  User-Agent:

```python
# src/tools/fetcher.py

import feedparser
import requests

def fetch_rss_feed(feed_url: str) -> Dict[str, Any]:
    try:
        # ä½¿ç”¨ requests å…ˆå–å¾—å…§å®¹ï¼ˆå¸¶è‡ªè¨‚ User-Agentï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(feed_url, headers=headers, timeout=10)
        response.raise_for_status()

        # å†ç”¨ feedparser è§£æ
        feed = feedparser.parse(response.content)

        # ... ç¹¼çºŒè™•ç†
    except Exception as e:
        # ...
```

---

## ğŸ“ æ”¯æ´èˆ‡ç¤¾ç¾¤

### ç²å–å¹«åŠ©

- **GitHub Issues**: https://github.com/your-repo/InsightCosmos/issues
- **Discord ç¤¾ç¾¤**: [åŠ å…¥é€£çµ]
- **Email**: your-email@example.com

### è²¢ç»

æ­¡è¿æäº¤ Pull Requestï¼è«‹åƒè€ƒ `CONTRIBUTING.md`

### æˆæ¬Š

MIT License - è©³è¦‹ `LICENSE` æª”æ¡ˆ

---

**æœ€å¾Œæ›´æ–°**: 2025-11-25
**ç¶­è­·è€…**: Ray å¼µç‘æ¶µ
**ç‰ˆæœ¬**: 1.0
