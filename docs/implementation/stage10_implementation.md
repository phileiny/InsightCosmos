# Stage 10: Curator Weekly Agent - å¯¦ä½œç­†è¨˜

> **éšæ®µ**: Phase 1 - Stage 10/12
> **ç›®æ¨™**: å¯¦ç¾æ¯é€±æ·±åº¦æƒ…å ±å ±å‘Šç”Ÿæˆèˆ‡è¶¨å‹¢åˆ†æ
> **å¯¦ä½œæ—¥æœŸ**: 2025-11-25
> **è² è²¬äºº**: Ray å¼µç‘æ¶µ
> **ç‹€æ…‹**: âœ… æ ¸å¿ƒå¯¦ä½œå®Œæˆï¼ˆå¾…æ¸¬è©¦é©—è­‰ï¼‰

---

## ğŸ“‹ ç›®éŒ„

1. [å¯¦ä½œç¸½è¦½](#å¯¦ä½œç¸½è¦½)
2. [VectorClusteringTool å¯¦ä½œ](#vectorclusteringtool-å¯¦ä½œ)
3. [TrendAnalysisTool å¯¦ä½œ](#trendanalysistool-å¯¦ä½œ)
4. [Weekly Prompt è¨­è¨ˆ](#weekly-prompt-è¨­è¨ˆ)
5. [CuratorWeeklyRunner å¯¦ä½œ](#curatorweeklyrunner-å¯¦ä½œ)
6. [æ¨¡çµ„æ›´æ–°](#æ¨¡çµ„æ›´æ–°)
7. [æ¸¬è©¦æŒ‡å—](#æ¸¬è©¦æŒ‡å—)
8. [å·²çŸ¥å•é¡Œ](#å·²çŸ¥å•é¡Œ)
9. [ä¸‹ä¸€æ­¥](#ä¸‹ä¸€æ­¥)

---

## ğŸ¯ å¯¦ä½œç¸½è¦½

### å®Œæˆå…§å®¹

Stage 10 æ ¸å¿ƒå¯¦ä½œå·²å®Œæˆï¼ŒåŒ…å«ï¼š

1. âœ… **VectorClusteringTool** - å‘é‡èšé¡å·¥å…·ï¼ˆ~350 è¡Œï¼‰
2. âœ… **TrendAnalysisTool** - è¶¨å‹¢åˆ†æå·¥å…·ï¼ˆ~330 è¡Œï¼‰
3. âœ… **Weekly Prompt** - LLM æŒ‡ä»¤æ¨¡æ¿ï¼ˆ~300 è¡Œï¼‰
4. âœ… **CuratorWeeklyRunner** - é€±å ±é‹è¡Œå™¨ï¼ˆ~650 è¡Œï¼‰
5. âœ… **æ¨¡çµ„æ›´æ–°** - __init__.py èˆ‡ä¾è³´æ›´æ–°

**ç¸½ä»£ç¢¼é‡**: ~1,660 è¡Œï¼ˆä¸å«æ¸¬è©¦ï¼‰

### æŠ€è¡“æ£§

- **èšé¡ç®—æ³•**: scikit-learn (K-Means, DBSCAN)
- **é—œéµå­—æå–**: TF-IDF (scikit-learn)
- **LLM**: Gemini 2.0 Flash Exp
- **è¨˜æ†¶å±¤**: ArticleStore + EmbeddingStore (SQLite + NumPy)

---

## ğŸ§© VectorClusteringTool å¯¦ä½œ

### æ–‡ä»¶ä½ç½®

`src/tools/vector_clustering.py`

### æ ¸å¿ƒé¡è¨­è¨ˆ

```python
class VectorClusteringTool:
    """
    å‘é‡èšé¡å·¥å…·

    ä½¿ç”¨ K-Means æˆ– DBSCAN å°æ–‡ç«  Embeddings é€²è¡Œèšé¡
    """

    def __init__(
        self,
        method: str = "kmeans",
        n_clusters: int = 4,
        random_state: int = 42
    ):
        """åˆå§‹åŒ–èšé¡å·¥å…·"""

    def cluster_embeddings(
        self,
        embeddings: np.ndarray,
        article_metadata: List[Dict]
    ) -> Dict:
        """ä¸»è¦èšé¡æ–¹æ³•"""

    def extract_cluster_keywords(
        self,
        cluster: Dict,
        all_articles: List[Dict],
        top_k: int = 5
    ) -> List[str]:
        """TF-IDF é—œéµå­—æå–"""

    def find_representative_articles(
        self,
        cluster: Dict,
        top_n: int = 3
    ) -> List[Dict]:
        """æ‰¾å‡ºä»£è¡¨æ€§æ–‡ç« """
```

### é—œéµå¯¦ä½œç´°ç¯€

#### 1. K-Means èšé¡

```python
def _cluster_kmeans(self, embeddings, metadata):
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    # å‹•æ…‹èª¿æ•´èšé¡æ•¸é‡ï¼ˆä¸èƒ½è¶…éæ–‡ç« æ•¸ï¼‰
    n_clusters = min(self.n_clusters, len(embeddings) - 1)

    # K-Means èšé¡ï¼ˆå¤šæ¬¡åˆå§‹åŒ–å–æœ€ä½³ï¼‰
    kmeans = KMeans(
        n_clusters=n_clusters,
        random_state=self.random_state,
        n_init=10
    )
    labels = kmeans.fit_predict(embeddings)

    # è¨ˆç®—èšé¡è³ªé‡
    score = silhouette_score(embeddings, labels)

    # çµ„ç¹”çµæœ
    clusters = self._organize_clusters(
        labels, embeddings, metadata, kmeans.cluster_centers_
    )

    return {
        "status": "success",
        "clusters": clusters,
        "n_clusters": n_clusters,
        "silhouette_score": float(score)
    }
```

#### 2. TF-IDF é—œéµå­—æå–

```python
def extract_cluster_keywords(self, cluster, all_articles, top_k=5):
    from sklearn.feature_extraction.text import TfidfVectorizer

    # æº–å‚™é›†ç¾¤æ–‡æœ¬
    cluster_texts = [
        a["title"] + " " + a.get("summary", "")
        for a in all_articles
        if a["article_id"] in cluster["article_ids"]
    ]

    # æº–å‚™èƒŒæ™¯èªæ–™
    all_texts = [
        a["title"] + " " + a.get("summary", "")
        for a in all_articles
    ]

    # TF-IDF å‘é‡åŒ–
    vectorizer = TfidfVectorizer(
        max_features=100,
        stop_words="english",
        ngram_range=(1, 2)  # 1-2 å€‹è©çš„çŸ­èª
    )
    vectorizer.fit(all_texts)

    # è¨ˆç®—é›†ç¾¤çš„ TF-IDF
    cluster_tfidf = vectorizer.transform(cluster_texts)
    avg_tfidf = cluster_tfidf.mean(axis=0).A1

    # æå– Top K
    top_indices = avg_tfidf.argsort()[-top_k:][::-1]
    keywords = [vectorizer.get_feature_names_out()[i] for i in top_indices]

    return keywords
```

#### 3. å‹•æ…‹èšé¡æ•¸é‡èª¿æ•´

åœ¨ `CuratorWeeklyRunner._cluster_articles()` ä¸­ï¼š

```python
# å‹•æ…‹èª¿æ•´èšé¡æ•¸é‡
n_articles = len(articles)
if n_articles >= 40:
    n_clusters = 5
elif n_articles >= 25:
    n_clusters = 4
elif n_articles >= 15:
    n_clusters = 3
else:
    n_clusters = 2
```

### è¨­è¨ˆæ±ºç­–

1. **K-Means ç‚ºä¸»åŠ›**
   - å„ªå‹¢ï¼šç°¡å–®é«˜æ•ˆã€çµæœç©©å®šã€æ˜“æ–¼è§£é‡‹
   - é©ç”¨ï¼šæ–‡ç« æ•¸é‡é©ä¸­ï¼ˆ30-70 ç¯‡ï¼‰
   - åƒæ•¸ï¼š`n_init=10` ç¢ºä¿çµæœç©©å®š

2. **DBSCAN ç‚ºå‚™ç”¨**
   - å„ªå‹¢ï¼šè‡ªå‹•ç™¼ç¾é›†ç¾¤æ•¸é‡
   - åŠ£å‹¢ï¼šåƒæ•¸èª¿æ•´è¼ƒè¤‡é›œ
   - æ‡‰ç”¨ï¼šæ–‡ç« ä¸»é¡Œåˆ†æ•£æ™‚ä½¿ç”¨

3. **Silhouette Score è©•ä¼°**
   - ç¯„åœï¼š-1 åˆ° 1
   - ç›®æ¨™ï¼š>= 0.5 ç‚ºè‰¯å¥½èšé¡
   - ç”¨é€”ï¼šç›£æ§èšé¡è³ªé‡

---

## ğŸ“ˆ TrendAnalysisTool å¯¦ä½œ

### æ–‡ä»¶ä½ç½®

`src/tools/trend_analysis.py`

### æ ¸å¿ƒé¡è¨­è¨ˆ

```python
class TrendAnalysisTool:
    """è¶¨å‹¢åˆ†æå·¥å…·"""

    def identify_hot_trends(
        self,
        clusters: List[Dict],
        min_article_count: int = 5,
        min_avg_priority: float = 0.75
    ) -> List[Dict]:
        """è­˜åˆ¥ç†±é–€è¶¨å‹¢"""

    def detect_emerging_topics(
        self,
        current_articles: List[Dict],
        previous_articles: Optional[List[Dict]] = None,
        min_priority: float = 0.7
    ) -> List[Dict]:
        """åµæ¸¬æ–°èˆˆè©±é¡Œ"""

    def _extract_keywords_from_articles(
        self,
        articles: List[Dict]
    ) -> Dict[str, Dict]:
        """å¾æ–‡ç« ä¸­æå–é—œéµå­—çµ±è¨ˆ"""
```

### é—œéµå¯¦ä½œç´°ç¯€

#### 1. ç†±é–€è¶¨å‹¢è­˜åˆ¥

```python
def identify_hot_trends(self, clusters, min_article_count=5, min_avg_priority=0.75):
    hot_trends = []

    for cluster in clusters:
        article_count = cluster.get("article_count", 0)
        avg_priority = cluster.get("average_priority", 0.0)

        # æª¢æŸ¥æ¨™æº–ï¼šæ–‡ç« å¤š + å„ªå…ˆåº¦é«˜
        if (article_count >= min_article_count and
            avg_priority >= min_avg_priority):

            # è¨ˆç®—è¶¨å‹¢åˆ†æ•¸
            normalized_count = min(article_count / 10, 1.0)
            trend_score = normalized_count * avg_priority

            hot_trends.append({
                "cluster_id": cluster["cluster_id"],
                "article_count": article_count,
                "average_priority": avg_priority,
                "trend_score": trend_score,
                "evidence": f"{article_count} ç¯‡æ–‡ç« ï¼Œå¹³å‡å„ªå…ˆåº¦ {avg_priority:.2f}"
            })

    # æŒ‰è¶¨å‹¢åˆ†æ•¸æ’åº
    hot_trends.sort(key=lambda x: x["trend_score"], reverse=True)

    return hot_trends
```

#### 2. æ–°èˆˆè©±é¡Œåµæ¸¬

```python
def detect_emerging_topics(self, current_articles, previous_articles=None, min_priority=0.7):
    # æå–æœ¬é€±é—œéµå­—
    current_keywords = self._extract_keywords_from_articles(current_articles)

    # å¦‚æœæœ‰ä¸Šé€±æ•¸æ“šï¼Œæ‰¾å‡ºæ–°é—œéµå­—
    if previous_articles:
        previous_keywords = self._extract_keywords_from_articles(previous_articles)
        new_keywords = set(current_keywords.keys()) - set(previous_keywords.keys())
    else:
        # ç„¡ä¸Šé€±æ•¸æ“šï¼Œä½¿ç”¨ä½é »é«˜å„ªå…ˆåº¦çš„é—œéµå­—
        new_keywords = [
            k for k, v in current_keywords.items()
            if v["count"] <= 5 and v["avg_priority"] >= min_priority
        ]

    # èšåˆæˆæ–°èˆˆè©±é¡Œ
    emerging_topics = []
    for keyword in new_keywords:
        keyword_info = current_keywords.get(keyword)
        if keyword_info and keyword_info["avg_priority"] >= min_priority:
            emerging_topics.append({
                "topic_keywords": [keyword],
                "article_count": keyword_info["count"],
                "first_appearance": keyword_info["first_date"],
                "average_priority": keyword_info["avg_priority"],
                "articles": keyword_info["articles"][:3]
            })

    # æŒ‰å„ªå…ˆåº¦æ’åº
    emerging_topics.sort(key=lambda x: x["average_priority"], reverse=True)

    return emerging_topics
```

#### 3. é—œéµå­—æå–

```python
def _extract_keywords_from_articles(self, articles):
    from collections import defaultdict
    import re

    keyword_stats = defaultdict(lambda: {
        "count": 0,
        "priorities": [],
        "dates": [],
        "articles": []
    })

    # åœç”¨è©
    stopwords = {
        "with", "from", "that", "this", "have", "been", "more",
        # ... æ›´å¤šåœç”¨è©
    }

    for article in articles:
        # å¾æ¨™é¡Œã€æ¨™ç±¤ã€æ‘˜è¦æå–æ–‡æœ¬
        text = ""
        text += article.get("title", "") + " "
        text += article.get("tags", "") + " "
        text += article.get("summary", "")[:200]

        # æå–è‡³å°‘ 4 å­—å…ƒçš„å–®è©
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())

        # éæ¿¾åœç”¨è©
        words = [w for w in words if w not in stopwords]

        # çµ±è¨ˆï¼ˆæ¯ç¯‡æ–‡ç« æ¯å€‹è©åªè¨ˆæ•¸ä¸€æ¬¡ï¼‰
        for word in set(words):
            keyword_stats[word]["count"] += 1
            keyword_stats[word]["priorities"].append(article.get("priority_score", 0.0))
            # ...

    # è¨ˆç®—å¹³å‡å€¼
    result = {}
    for keyword, stats in keyword_stats.items():
        result[keyword] = {
            "count": stats["count"],
            "avg_priority": sum(stats["priorities"]) / len(stats["priorities"]),
            "first_date": min(stats["dates"]) if stats["dates"] else "",
            "articles": sorted(stats["articles"], key=lambda x: x["priority_score"], reverse=True)
        }

    return result
```

### è¨­è¨ˆæ±ºç­–

1. **è¶¨å‹¢åˆ†æ•¸å…¬å¼**
   - `trend_score = (article_count / 10) * avg_priority`
   - å¹³è¡¡ï¼šæ–‡ç« æ•¸é‡ vs å„ªå…ˆåº¦
   - æ¨™æº–åŒ–ï¼š10 ç¯‡æ–‡ç« è¦–ç‚ºæ»¿åˆ†

2. **æ–°èˆˆè©±é¡Œæ¨™æº–**
   - ä½é »ï¼ˆ<= 5 ç¯‡ï¼‰+ é«˜å„ªå…ˆåº¦ï¼ˆ>= 0.7ï¼‰
   - æˆ–ï¼šæœ¬é€±é¦–æ¬¡å‡ºç¾çš„é—œéµå­—
   - ç›®çš„ï¼šç™¼ç¾æ½›åŠ›è©±é¡Œï¼Œè€Œéå·²çŸ¥ç†±é–€

3. **é—œéµå­—æå–ç­–ç•¥**
   - æœ€å°é•·åº¦ï¼š4 å­—å…ƒï¼ˆéæ¿¾ "is", "the" ç­‰ï¼‰
   - å»é‡ï¼šæ¯ç¯‡æ–‡ç« æ¯å€‹è©åªè¨ˆæ•¸ä¸€æ¬¡
   - åœç”¨è©ï¼šæ“´å±•çš„è‹±æ–‡åœç”¨è©åˆ—è¡¨

---

## ğŸ“ Weekly Prompt è¨­è¨ˆ

### æ–‡ä»¶ä½ç½®

`prompts/weekly_prompt.txt`

### Prompt çµæ§‹

```
ä½ æ˜¯ InsightCosmos çš„ã€Œé€±å ±ç­–å±•äººã€(Weekly Curator)

## ä½ çš„ä»»å‹™
- æœ¬é€±ç¸½çµ
- ä¸»é¡Œé›†ç¾¤åˆ†æ
- ç†±é–€è¶¨å‹¢è­˜åˆ¥
- æ–°èˆˆè©±é¡Œåµæ¸¬
- Top æ–‡ç« æ¨è–¦
- æ´å¯Ÿç¸½çµ
- è¡Œå‹•å»ºè­°

## è¼¸å…¥è³‡æ–™
{JSON æ ¼å¼çš„è¼¸å…¥æ•¸æ“šå®šç¾©}

## è¼¸å‡ºæ ¼å¼
{åš´æ ¼çš„ JSON Schema}

## å¯«ä½œé¢¨æ ¼æŒ‡å—
- èªè¨€ï¼šç¹é«”ä¸­æ–‡
- ç°¡æ½”æœ‰åŠ›
- æŠ€è¡“æº–ç¢º
- æ´å¯Ÿæ·±åˆ»
- è¡Œå‹•å°å‘

## Example
{å®Œæ•´ç¤ºä¾‹è¼¸å‡º}
```

### é—œéµè¨­è¨ˆ

1. **æ˜ç¢ºè§’è‰²å®šä½**
   - ã€Œé€±å ±ç­–å±•äººã€è€Œéé€šç”¨ AI
   - é‡å° Ray çš„èˆˆè¶£é ˜åŸŸ
   - æŠ€è¡“æ°´å¹³ï¼šé«˜ç´šé–‹ç™¼è€…

2. **çµæ§‹åŒ–è¼¸å‡º**
   - åš´æ ¼ JSON æ ¼å¼ï¼ˆä¸ä½¿ç”¨ Markdown åŒ…è£ï¼‰
   - 7 å€‹ä¸»è¦æ¬„ä½
   - æ¯å€‹æ¬„ä½éƒ½æœ‰æ˜ç¢ºçš„æ ¼å¼è¦æ±‚

3. **è³ªé‡æ¨™æº–**
   - è¶¨å‹¢è­˜åˆ¥æº–ç¢ºæ€§
   - æ´å¯Ÿæ·±åº¦
   - è¡Œå‹•å»ºè­°å…·é«”æ€§
   - æ–‡å­—æµæš¢æ€§

4. **å®Œæ•´ç¤ºä¾‹**
   - æä¾›çœŸå¯¦çš„è¼¸å‡ºç¯„ä¾‹
   - å±•ç¤ºæœŸæœ›çš„å¯«ä½œé¢¨æ ¼
   - æ˜ç¢ºæ•¸æ“šèˆ‡æ´å¯Ÿçš„é—œä¿‚

---

## ğŸ¨ CuratorWeeklyRunner å¯¦ä½œ

### æ–‡ä»¶ä½ç½®

`src/agents/curator_weekly.py`

### æ ¸å¿ƒæµç¨‹

```
generate_weekly_report()
    â†“
1. _get_weekly_articles()       # æŸ¥è©¢æœ¬é€±æ–‡ç« ï¼ˆ7 å¤©ï¼‰
    â†“
2. _cluster_articles()           # å‘é‡èšé¡ï¼ˆK-Meansï¼‰
    â†“
3. _analyze_trends()             # è¶¨å‹¢åˆ†æ
    â†“
4. _generate_report_with_llm()   # LLM ç”Ÿæˆå ±å‘Š
    â†“
5. _format_and_send()            # æ ¼å¼åŒ–ä¸¦ç™¼é€
```

### é—œéµæ–¹æ³•å¯¦ä½œ

#### 1. æŸ¥è©¢æœ¬é€±æ–‡ç« 

```python
def _get_weekly_articles(self, week_start, week_end):
    # è¨ˆç®—æ—¥æœŸç¯„åœï¼ˆé»˜èªéå» 7 å¤©ï¼‰
    if week_end is None:
        end_date = datetime.now()
    else:
        end_date = datetime.strptime(week_end, "%Y-%m-%d")

    if week_start is None:
        start_date = end_date - timedelta(days=7)
    else:
        start_date = datetime.strptime(week_start, "%Y-%m-%d")

    # æŸ¥è©¢å·²åˆ†æçš„æ–‡ç« 
    articles = self.article_store.get_by_date_range(
        start_date=start_date.isoformat(),
        end_date=end_date.isoformat(),
        status="analyzed",
        min_priority=0.6  # éæ¿¾ä½å„ªå…ˆåº¦
    )

    return articles
```

#### 2. å‘é‡èšé¡

```python
def _cluster_articles(self, articles):
    # ç²å– Embeddings
    article_ids = [a["id"] for a in articles]
    embeddings_data = self.embedding_store.get_embeddings(article_ids)

    if not embeddings_data:
        return {
            "status": "error",
            "error_type": "no_embeddings",
            "error_message": "No embeddings found"
        }

    # çµ„ç¹”æˆ numpy çŸ©é™£
    embeddings_matrix = np.array([e["embedding"] for e in embeddings_data])

    # æº–å‚™å…ƒæ•¸æ“š
    metadata = [
        {
            "article_id": a["id"],
            "title": a["title"],
            "summary": a.get("summary", ""),
            "tags": a.get("tags", ""),
            "priority_score": a.get("priority_score", 0.0)
        }
        for a in articles
    ]

    # å‹•æ…‹èª¿æ•´èšé¡æ•¸é‡
    n_articles = len(articles)
    n_clusters = 5 if n_articles >= 40 else (4 if n_articles >= 25 else 3)

    # åŸ·è¡Œèšé¡
    clustering_tool = VectorClusteringTool(n_clusters=n_clusters)
    result = clustering_tool.cluster_embeddings(embeddings_matrix, metadata)

    # æå–é—œéµå­—
    if result["status"] == "success":
        for cluster in result["clusters"]:
            keywords = clustering_tool.extract_cluster_keywords(cluster, articles, top_k=5)
            cluster["keywords"] = keywords

    return result
```

#### 3. LLM å ±å‘Šç”Ÿæˆ

```python
def _generate_report_with_llm(self, articles, clusters, trend_result, week_start, week_end):
    # æº–å‚™è¼¸å…¥æ•¸æ“š
    input_data = self._prepare_llm_input(articles, clusters, trend_result, week_start, week_end)

    # å‰µå»º Agent
    agent = create_weekly_curator_agent()

    # èª¿ç”¨ LLM
    session_service = InMemorySessionService()
    session = session_service.create_session()

    input_json = json.dumps(input_data, ensure_ascii=False, indent=2)

    response = agent.send_message(
        message=f"è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šç”Ÿæˆé€±å ±ï¼š\n\n{input_json}",
        session=session
    )

    # è§£æè¼¸å‡ºï¼ˆæ”¯æ´ Markdown åŒ…è£ï¼‰
    report_json = self._parse_llm_output(response.final_response)

    if report_json is None:
        return {
            "status": "error",
            "error_type": "parse_error",
            "error_message": "Failed to parse LLM output"
        }

    return {
        "status": "success",
        "report": report_json
    }
```

#### 4. æº–å‚™ LLM è¼¸å…¥

```python
def _prepare_llm_input(self, articles, clusters, trend_result, week_start, week_end):
    # é›†ç¾¤æ•¸æ“šï¼ˆå«ä»£è¡¨æ€§æ–‡ç« ï¼‰
    clusters_with_articles = []
    for cluster in clusters:
        cluster_data = {
            "cluster_id": cluster["cluster_id"],
            "article_count": cluster["article_count"],
            "average_priority": cluster["average_priority"],
            "keywords": cluster.get("keywords", []),
            "representative_articles": []
        }

        # å–å‰ 3 ç¯‡ä»£è¡¨æ€§æ–‡ç« 
        for article_info in cluster["articles"][:3]:
            full_article = next((a for a in articles if a["id"] == article_info["article_id"]), None)
            if full_article:
                cluster_data["representative_articles"].append({
                    "title": full_article["title"],
                    "url": full_article["url"],
                    "summary": full_article.get("summary", ""),
                    "priority_score": full_article.get("priority_score", 0.0)
                })

        clusters_with_articles.append(cluster_data)

    # Top æ–‡ç« ï¼ˆå…¨å±€ Top 10ï¼‰
    top_articles = sorted(articles, key=lambda x: x.get("priority_score", 0.0), reverse=True)[:10]

    # çµ„åˆå®Œæ•´è¼¸å…¥
    return {
        "week_start": str(start_date),
        "week_end": str(end_date),
        "total_articles": len(articles),
        "analyzed_articles": len(articles),
        "topic_clusters": clusters_with_articles,
        "hot_trends": trend_result["hot_trends"],
        "emerging_topics": trend_result["emerging_topics"],
        "top_articles_overall": top_articles_data
    }
```

#### 5. ç°¡å–®æ ¼å¼åŒ–ï¼ˆè‡¨æ™‚æ–¹æ¡ˆï¼‰

```python
def _format_and_send(self, report_data, dry_run):
    # ç”Ÿæˆä¸»é¡Œ
    subject = f"InsightCosmos Weekly Report - {report_data.get('week_start')} to {report_data.get('week_end')}"

    # æ ¼å¼åŒ–ï¼ˆè‡¨æ™‚ä½¿ç”¨ç°¡å–®æ ¼å¼ï¼‰
    text_body = self._format_simple_text(report_data)
    html_body = self._format_simple_html(report_data)

    # ç™¼é€éƒµä»¶
    if not dry_run:
        sender = EmailSender(self.config)
        send_result = sender.send_html_email(
            to_email=self.config.email_account,
            subject=subject,
            html_body=html_body,
            text_body=text_body
        )
        return send_result
    else:
        return {
            "status": "success",
            "subject": subject,
            "recipients": [self.config.email_account],
            "html_body": html_body,
            "text_body": text_body
        }
```

### è¨­è¨ˆæ±ºç­–

1. **æ—¥æœŸç¯„åœ**
   - é»˜èªï¼šéå» 7 å¤©
   - å¯è‡ªå®šç¾©ï¼š`week_start` å’Œ `week_end`
   - æ ¼å¼ï¼š`YYYY-MM-DD`

2. **æœ€ä½å„ªå…ˆåº¦é–¾å€¼**
   - æŸ¥è©¢æ–‡ç« ï¼š>= 0.6
   - ç†±é–€è¶¨å‹¢ï¼š>= 0.75
   - æ–°èˆˆè©±é¡Œï¼š>= 0.7

3. **å‹•æ…‹èšé¡æ•¸é‡**
   - 40+ ç¯‡ï¼š5 å€‹é›†ç¾¤
   - 25-39 ç¯‡ï¼š4 å€‹é›†ç¾¤
   - 15-24 ç¯‡ï¼š3 å€‹é›†ç¾¤
   - <15 ç¯‡ï¼š2 å€‹é›†ç¾¤

4. **éŒ¯èª¤è™•ç†**
   - åˆ†å±¤éŒ¯èª¤è™•ç†ï¼ˆæ¯å€‹æ­¥é©Ÿç¨ç«‹ï¼‰
   - è©³ç´°éŒ¯èª¤è¨Šæ¯èˆ‡å»ºè­°
   - ç‹€æ…‹ç¢¼ï¼š"success" æˆ– "error"

---

## ğŸ”§ æ¨¡çµ„æ›´æ–°

### 1. src/tools/__init__.py

**ç‰ˆæœ¬**: 1.4.0

**æ›´æ–°å…§å®¹**:
```python
from src.tools.vector_clustering import VectorClusteringTool, cluster_articles
from src.tools.trend_analysis import TrendAnalysisTool, analyze_weekly_trends

__all__ = [
    # ... existing exports
    'VectorClusteringTool',
    'cluster_articles',
    'TrendAnalysisTool',
    'analyze_weekly_trends',
]
```

### 2. src/agents/__init__.py

**ç‰ˆæœ¬**: 1.3.0

**æ›´æ–°å…§å®¹**:
```python
from src.agents.curator_weekly import (
    CuratorWeeklyRunner,
    create_weekly_curator_agent,
    generate_weekly_report
)

__all__ = [
    # ... existing exports
    'CuratorWeeklyRunner',
    'create_weekly_curator_agent',
    'generate_weekly_report'
]
```

### 3. requirements.txt

**æ–°å¢ä¾è³´**:
```txt
scikit-learn>=1.3.0  # K-Means, DBSCAN, TF-IDF
```

---

## ğŸ§ª æ¸¬è©¦æŒ‡å—

### æ‰‹å‹•æ¸¬è©¦æ­¥é©Ÿ

#### 1. å®‰è£ä¾è³´

```bash
# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source .venv/bin/activate

# å®‰è£ scikit-learn
pip install scikit-learn>=1.3.0
```

#### 2. Import æ¸¬è©¦

```bash
# é‹è¡Œæ¸¬è©¦è…³æœ¬
python test_stage10_import.py
```

**é æœŸè¼¸å‡º**:
```
============================================================
Stage 10 Import Test
============================================================

[Test 1] VectorClusteringTool import...
âœ“ VectorClusteringTool import successful
âœ“ VectorClusteringTool initialization: method=kmeans, n_clusters=3

[Test 2] TrendAnalysisTool import...
âœ“ TrendAnalysisTool import successful
âœ“ TrendAnalysisTool initialization successful

[Test 3] CuratorWeeklyRunner import...
âœ“ CuratorWeeklyRunner import successful
âœ“ create_weekly_curator_agent import successful
âœ“ generate_weekly_report import successful

[Test 4] Tools module export...
âœ“ VectorClusteringTool exported from src.tools
âœ“ TrendAnalysisTool exported from src.tools

[Test 5] Agents module export...
âœ“ CuratorWeeklyRunner exported from src.agents
âœ“ create_weekly_curator_agent exported from src.agents
âœ“ generate_weekly_report exported from src.agents

[Test 6] scikit-learn availability...
âœ“ scikit-learn version: 1.5.x
âœ“ sklearn.cluster.KMeans import successful
âœ“ sklearn.feature_extraction.text.TfidfVectorizer import successful

============================================================
Import Test Complete
============================================================
```

#### 3. åŠŸèƒ½æ¸¬è©¦ï¼ˆéœ€è¦å·²åˆ†æçš„æ–‡ç« ï¼‰

```bash
# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
source .venv/bin/activate

# é‹è¡Œ Weekly Curatorï¼ˆDry Runï¼‰
python -c "from src.agents.curator_weekly import generate_weekly_report; result = generate_weekly_report(dry_run=True); print(result['status'])"
```

### å–®å…ƒæ¸¬è©¦ï¼ˆå¾…ç·¨å¯«ï¼‰

éœ€è¦å‰µå»ºä»¥ä¸‹æ¸¬è©¦æ–‡ä»¶ï¼š

1. **`tests/unit/test_vector_clustering.py`**
   - `test_kmeans_clustering_basic()`
   - `test_extract_cluster_keywords()`
   - `test_find_representative_articles()`
   - `test_invalid_input_handling()`

2. **`tests/unit/test_trend_analysis.py`**
   - `test_identify_hot_trends()`
   - `test_detect_emerging_topics()`
   - `test_extract_keywords_from_articles()`

3. **`tests/unit/test_curator_weekly.py`**
   - `test_runner_initialization()`
   - `test_get_weekly_articles()`
   - `test_cluster_articles()`
   - `test_analyze_trends()`
   - `test_generate_report_with_llm()`

### æ•´åˆæ¸¬è©¦ï¼ˆå¾…ç·¨å¯«ï¼‰

**`tests/integration/test_curator_weekly.py`**
- `test_weekly_pipeline_with_mock_data()`
- `test_weekly_clustering_integration()`
- `test_weekly_trend_analysis_integration()`

---

## âš ï¸ å·²çŸ¥å•é¡Œ

### 1. scikit-learn å®‰è£å•é¡Œ

**å•é¡Œ**: WSL ç’°å¢ƒä¸‹è™›æ“¬ç’°å¢ƒå‘½ä»¤åŸ·è¡Œç•°å¸¸ç·©æ…¢

**å½±éŸ¿**: ç„¡æ³•åœ¨ç•¶å‰æœƒè©±ä¸­å®Œæˆå®‰è£èˆ‡æ¸¬è©¦

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# æ‰‹å‹•å®‰è£ï¼ˆåœ¨è™›æ“¬ç’°å¢ƒå¤–ï¼‰
pip install --user scikit-learn

# æˆ–åœ¨è™›æ“¬ç’°å¢ƒä¸­ï¼ˆå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰
source .venv/bin/activate
pip install scikit-learn>=1.3.0
```

### 2. DigestFormatter æœªæ“´å±•

**å•é¡Œ**: `format_weekly_html()` å’Œ `format_weekly_text()` å°šæœªå¯¦ä½œ

**ç•¶å‰æ–¹æ¡ˆ**: ä½¿ç”¨è‡¨æ™‚çš„ç°¡å–®æ ¼å¼åŒ–æ–¹æ³•

**å¾…å®Œæˆ**:
- å‰µå»ºè±å¯Œçš„ Weekly HTML æ ¼å¼
- å‰µå»ºçµæ§‹åŒ–çš„ Weekly Text æ ¼å¼

### 3. æ¸¬è©¦è¦†è“‹ç‡

**å•é¡Œ**: å°šæœªç·¨å¯«å–®å…ƒæ¸¬è©¦èˆ‡æ•´åˆæ¸¬è©¦

**å½±éŸ¿**: ä»£ç¢¼æœªç¶“å……åˆ†æ¸¬è©¦é©—è­‰

**å„ªå…ˆç´š**: é«˜ï¼ˆä¸‹ä¸€æ­¥å·¥ä½œï¼‰

---

## ğŸ¯ ä¸‹ä¸€æ­¥

### ç«‹å³åŸ·è¡Œï¼ˆæœ¬éšæ®µï¼‰

1. âœ… **æ ¸å¿ƒå¯¦ä½œ** - å·²å®Œæˆ
2. â³ **å®‰è£ scikit-learn** - å¾…æ‰‹å‹•åŸ·è¡Œ
3. â³ **Import æ¸¬è©¦** - å¾…åŸ·è¡Œ `test_stage10_import.py`
4. â³ **åŸºæœ¬åŠŸèƒ½æ¸¬è©¦** - é‹è¡Œ dry_run æ¨¡å¼

### è¿‘æœŸè¨ˆåŠƒï¼ˆStage 10 å®Œå–„ï¼‰

1. **æ“´å±• DigestFormatter**
   - å¯¦ä½œ `format_weekly_html()`
   - å¯¦ä½œ `format_weekly_text()`
   - è¨­è¨ˆè±å¯Œçš„è¦–è¦ºåŒ–æ¨£å¼

2. **ç·¨å¯«æ¸¬è©¦**
   - VectorClustering å–®å…ƒæ¸¬è©¦
   - TrendAnalysis å–®å…ƒæ¸¬è©¦
   - CuratorWeekly æ•´åˆæ¸¬è©¦

3. **å„ªåŒ–èˆ‡èª¿æ•´**
   - èª¿æ•´èšé¡åƒæ•¸ï¼ˆåŸºæ–¼å¯¦éš›æ•¸æ“šï¼‰
   - èª¿æ•´è¶¨å‹¢è­˜åˆ¥é–¾å€¼
   - å„ªåŒ– Promptï¼ˆåŸºæ–¼ LLM è¼¸å‡ºï¼‰

### å¾ŒçºŒéšæ®µï¼ˆStage 11-12ï¼‰

1. **Stage 11**: Weekly Pipeline é›†æˆ
   - å‰µå»º Weekly Orchestrator
   - æ•´åˆå®Œæ•´é€±å ±æµç¨‹
   - æ’ç¨‹èˆ‡è‡ªå‹•åŸ·è¡Œ

2. **Stage 12**: è³ªé‡ä¿è­‰èˆ‡å„ªåŒ–
   - å®Œæ•´æ¸¬è©¦è¦†è“‹
   - æ€§èƒ½å„ªåŒ–
   - æ–‡æª”å®Œå–„
   - éƒ¨ç½²æº–å‚™

---

## ğŸ“Š çµ±è¨ˆæ•¸æ“š

### ä»£ç¢¼çµ±è¨ˆ

| æ¨¡çµ„ | æ–‡ä»¶ | è¡Œæ•¸ | èªªæ˜ |
|------|------|------|------|
| VectorClusteringTool | vector_clustering.py | ~350 | K-Means, TF-IDF |
| TrendAnalysisTool | trend_analysis.py | ~330 | è¶¨å‹¢åˆ†æ |
| CuratorWeeklyRunner | curator_weekly.py | ~650 | é€±å ±é‹è¡Œå™¨ |
| Weekly Prompt | weekly_prompt.txt | ~300 | LLM æŒ‡ä»¤ |
| æ¸¬è©¦è…³æœ¬ | test_stage10_import.py | ~120 | Import æ¸¬è©¦ |
| **ç¸½è¨ˆ** | **5 å€‹æ–‡ä»¶** | **~1,750 è¡Œ** | **æ ¸å¿ƒå¯¦ä½œ** |

### æ¨¡çµ„æ›´æ–°

| æ–‡ä»¶ | ç‰ˆæœ¬ | è®Šæ›´ |
|------|------|------|
| src/tools/__init__.py | 1.3.0 â†’ 1.4.0 | +2 exports |
| src/agents/__init__.py | 1.2.0 â†’ 1.3.0 | +3 exports |
| requirements.txt | - | +1 dependency |

---

## ğŸ“ æŠ€è¡“äº®é»

### 1. å‹•æ…‹åƒæ•¸èª¿æ•´

æ ¹æ“šæ–‡ç« æ•¸é‡è‡ªå‹•èª¿æ•´èšé¡æ•¸é‡ï¼Œç¢ºä¿èšé¡æ•ˆæœï¼š

```python
n_articles = len(articles)
n_clusters = 5 if n_articles >= 40 else (4 if n_articles >= 25 else 3)
```

### 2. TF-IDF é—œéµå­—æå–

ä½¿ç”¨ scikit-learn çš„ TfidfVectorizerï¼Œæ”¯æ´ 1-2 å€‹è©çš„çŸ­èªï¼š

```python
vectorizer = TfidfVectorizer(
    max_features=100,
    stop_words="english",
    ngram_range=(1, 2)
)
```

### 3. çµæ§‹åŒ– LLM è¼¸å‡º

è¨­è¨ˆè©³ç´°çš„ Promptï¼Œç¢ºä¿ LLM è¼¸å‡ºçµæ§‹åŒ– JSONï¼š

```python
# Prompt æ˜ç¢ºè¦æ±‚
"è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼ˆ**ä¸è¦ä½¿ç”¨ Markdown åŒ…è£**ï¼‰"
```

### 4. éŒ¯èª¤è™•ç†ç­–ç•¥

æ¯å€‹æ–¹æ³•éƒ½è¿”å›çµæ§‹åŒ–çš„çµæœï¼ŒåŒ…å«ç‹€æ…‹ç¢¼ã€éŒ¯èª¤è¨Šæ¯èˆ‡å»ºè­°ï¼š

```python
return {
    "status": "error",
    "error_type": "no_embeddings",
    "error_message": "No embeddings found for articles",
    "suggestion": "Ensure Analyst Agent has generated embeddings"
}
```

---

## ğŸ“š åƒè€ƒè³‡æ–™

### æŠ€è¡“æ–‡ä»¶

- [scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [scikit-learn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

### å°ˆæ¡ˆæ–‡ä»¶

- `docs/planning/stage10_curator_weekly.md` - è¦åŠƒæ–‡æª”
- `docs/planning/stage8_curator_daily.md` - Daily Curator åƒè€ƒ
- `docs/planning/stage7_analyst_agent.md` - Analyst Agent åƒè€ƒ
- `CLAUDE.md` - å°ˆæ¡ˆä¸€è‡´æ€§æŒ‡å—

---

**å‰µå»ºè€…**: Ray å¼µç‘æ¶µ
**å‰µå»ºæ—¥æœŸ**: 2025-11-25
**æœ€å¾Œæ›´æ–°**: 2025-11-25
**ç‹€æ…‹**: âœ… æ ¸å¿ƒå¯¦ä½œå®Œæˆï¼Œå¾…æ¸¬è©¦é©—è­‰
