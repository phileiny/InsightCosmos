# Stage 10: Curator Weekly Agent - è¦åŠƒæ–‡ä»¶

> **éšæ®µ**: Phase 1 - Stage 10/12
> **ç›®æ¨™**: å¯¦ç¾æ¯é€±æ·±åº¦æƒ…å ±å ±å‘Šç”Ÿæˆèˆ‡è¶¨å‹¢åˆ†æ
> **é è¨ˆæ™‚é–“**: 2 å¤©
> **å‰µå»ºæ—¥æœŸ**: 2025-11-25
> **è² è²¬äºº**: Ray å¼µç‘æ¶µ

---

## ğŸ“‹ ç›®éŒ„

1. [ç›®æ¨™èªªæ˜](#ç›®æ¨™èªªæ˜)
2. [è¼¸å…¥/è¼¸å‡ºå®šç¾©](#è¼¸å…¥è¼¸å‡ºå®šç¾©)
3. [æŠ€è¡“è¨­è¨ˆ](#æŠ€è¡“è¨­è¨ˆ)
4. [Curator Weekly Agent è¨­è¨ˆ](#curator-weekly-agent-è¨­è¨ˆ)
5. [Vector Clustering å·¥å…·è¨­è¨ˆ](#vector-clustering-å·¥å…·è¨­è¨ˆ)
6. [è¶¨å‹¢åˆ†æè¨­è¨ˆ](#è¶¨å‹¢åˆ†æè¨­è¨ˆ)
7. [Weekly Report æ ¼å¼è¨­è¨ˆ](#weekly-report-æ ¼å¼è¨­è¨ˆ)
8. [å¯¦ä½œè¨ˆåŠƒ](#å¯¦ä½œè¨ˆåŠƒ)
9. [æ¸¬è©¦ç­–ç•¥](#æ¸¬è©¦ç­–ç•¥)
10. [é©—æ”¶æ¨™æº–](#é©—æ”¶æ¨™æº–)
11. [é¢¨éšªèˆ‡å°ç­–](#é¢¨éšªèˆ‡å°ç­–)

---

## ğŸ¯ ç›®æ¨™èªªæ˜

### æ ¸å¿ƒç›®æ¨™

å¯¦ç¾ **Curator Weekly Agent**ï¼Œè² è²¬åˆ†ææœ¬é€±æ‰€æœ‰å·²åˆ†æçš„æ–‡ç« ï¼Œè­˜åˆ¥ä¸»é¡Œè¶¨å‹¢ã€é€²è¡Œå‘é‡èšé¡ã€ç”Ÿæˆæ·±åº¦æ´å¯Ÿå ±å‘Šï¼Œä¸¦é€é Email ç™¼é€çµ¦ä½¿ç”¨è€…ã€‚

### èˆ‡ Daily Curator çš„å·®ç•°

| ç¶­åº¦ | Daily Curator | Weekly Curator |
|------|---------------|----------------|
| **æ™‚é–“ç¯„åœ** | 24 å°æ™‚ | 7 å¤© |
| **æ–‡ç« æ•¸é‡** | 5-10 ç¯‡ | 30-70 ç¯‡ |
| **åˆ†ææ·±åº¦** | å–®ç¯‡å„ªå…ˆåº¦æ’åº | ä¸»é¡Œèšé¡èˆ‡è¶¨å‹¢è­˜åˆ¥ |
| **æŠ€è¡“é›£é»** | æ–‡ç« ç¯©é¸ | Vector Clusteringã€è¶¨å‹¢åˆ†æ |
| **å ±å‘Šå…§å®¹** | Top æ–‡ç« åˆ—è¡¨ | ä¸»é¡Œåˆ†å¸ƒã€è¶¨å‹¢æ´å¯Ÿã€è¡Œå‹•å»ºè­° |
| **å ±å‘Šé•·åº¦** | ç°¡çŸ­æ‘˜è¦ (500-800 å­—) | æ·±åº¦å ±å‘Š (1500-2500 å­—) |

### å…·é«”åŠŸèƒ½

1. **æ–‡ç« èšåˆ**
   - å¾ Memory ä¸­å–å¾—æœ¬é€±å·²åˆ†æçš„æ–‡ç« ï¼ˆéå» 7 å¤©ï¼‰
   - éæ¿¾ä½å„ªå…ˆåº¦æ–‡ç« ï¼ˆpriority_score < 0.6ï¼‰
   - å–å¾—å°æ‡‰çš„ Embedding å‘é‡

2. **ä¸»é¡Œèšé¡**
   - ä½¿ç”¨ K-Means æˆ– DBSCAN å° Embedding é€²è¡Œèšé¡
   - è­˜åˆ¥ 3-5 å€‹ä¸»é¡Œé›†ç¾¤
   - ç‚ºæ¯å€‹é›†ç¾¤æå–ä»£è¡¨æ€§æ–‡ç« èˆ‡é—œéµå­—

3. **è¶¨å‹¢åˆ†æ**
   - åˆ†æä¸»é¡Œæ¼”åŒ–ï¼ˆæœ¬é€± vs ä¸Šé€±ï¼‰
   - è­˜åˆ¥ç†±é–€è¶¨å‹¢ï¼ˆhigh priority + å¤šç¯‡æ–‡ç« ï¼‰
   - è­˜åˆ¥æ–°èˆˆè©±é¡Œï¼ˆé¦–æ¬¡å‡ºç¾çš„ä¸»é¡Œï¼‰

4. **æ·±åº¦å ±å‘Šç”Ÿæˆ**
   - ä½¿ç”¨ LLM æ•´åˆæ‰€æœ‰åˆ†æçµæœ
   - ç”Ÿæˆçµæ§‹åŒ–çš„ Weekly Report
   - åŒ…å«ï¼šä¸»é¡Œç¸½è¦½ã€è¶¨å‹¢æ´å¯Ÿã€é‡é»æ–‡ç« ã€è¡Œå‹•å»ºè­°
   - æ”¯æ´ HTML èˆ‡ç´”æ–‡å­—æ ¼å¼

5. **Email ç™¼é€**
   - ä½¿ç”¨èˆ‡ Daily Curator ç›¸åŒçš„ EmailSender
   - æ”¯æ´æ›´è±å¯Œçš„ HTML æ ¼å¼ï¼ˆåœ–è¡¨ã€åˆ†é¡å±•ç¤ºï¼‰

### èˆ‡å…¶ä»–æ¨¡çµ„çš„é—œä¿‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Curator Weekly Agent                  â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. æŸ¥è©¢æœ¬é€±æ–‡ç«                            â”‚    â”‚
â”‚  â”‚    â†’ ArticleStore.get_by_date_range()    â”‚    â”‚
â”‚  â”‚    â†’ EmbeddingStore.get_embeddings()     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. Vector Clustering                     â”‚    â”‚
â”‚  â”‚    â†’ K-Means / DBSCAN                    â”‚    â”‚
â”‚  â”‚    â†’ è­˜åˆ¥ 3-5 å€‹ä¸»é¡Œé›†ç¾¤                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. è¶¨å‹¢åˆ†æ                               â”‚    â”‚
â”‚  â”‚    â†’ ç†±é–€è¶¨å‹¢è­˜åˆ¥                         â”‚    â”‚
â”‚  â”‚    â†’ æ–°èˆˆè©±é¡Œåµæ¸¬                         â”‚    â”‚
â”‚  â”‚    â†’ ä¸»é¡Œæ¼”åŒ–åˆ†æ                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. LLM æ·±åº¦å ±å‘Šç”Ÿæˆ                       â”‚    â”‚
â”‚  â”‚    â†’ Gemini 2.5 Flash                    â”‚    â”‚
â”‚  â”‚    â†’ Weekly Report (HTML + Text)         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                     â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 5. Email ç™¼é€                             â”‚    â”‚
â”‚  â”‚    â†’ EmailSender (SMTP)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¥ è¼¸å…¥/è¼¸å‡ºå®šç¾©

### è¼¸å…¥

**ä¾†æº 1**: `ArticleStore.get_by_date_range(start_date, end_date, min_priority=0.6)`

**æ•¸æ“šçµæ§‹**:
```python
[
    {
        "id": 1,
        "title": "æ–‡ç« æ¨™é¡Œ",
        "url": "https://example.com/article",
        "summary": "æ–‡ç« æ‘˜è¦ï¼ˆLLM ç”Ÿæˆï¼‰",
        "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3"],
        "priority_score": 0.85,
        "priority_reasoning": "ç‚ºä½•é‡è¦çš„ç†ç”±",
        "tags": "AI,Multi-Agent,Robotics",
        "tech_stack": "Python,TensorFlow,ROS",
        "published_at": "2025-11-20T10:00:00Z",
        "source_name": "TechCrunch",
        "analyzed_at": "2025-11-20T12:00:00Z"
    },
    # ... 30-70 ç¯‡æ–‡ç« 
]
```

**ä¾†æº 2**: `EmbeddingStore.get_embeddings(article_ids)`

**æ•¸æ“šçµæ§‹**:
```python
[
    {
        "article_id": 1,
        "embedding": np.array([0.12, 0.34, ..., 0.89]),  # 768 ç¶­å‘é‡
        "created_at": "2025-11-20T12:00:00Z"
    },
    # ...
]
```

### è¼¸å‡º

**1. Weekly Report çµæ§‹åŒ–æ•¸æ“š**:
```python
{
    "week_start": "2025-11-18",
    "week_end": "2025-11-24",
    "total_articles": 52,
    "analyzed_articles": 48,
    "high_priority_articles": 25,

    # ä¸»é¡Œèšé¡çµæœ
    "topic_clusters": [
        {
            "cluster_id": 0,
            "topic_name": "Multi-Agent Systems Breakthroughs",  # LLM ç”Ÿæˆ
            "article_count": 12,
            "average_priority": 0.87,
            "key_keywords": ["multi-agent", "collaboration", "distributed AI"],
            "representative_articles": [
                {
                    "title": "...",
                    "url": "...",
                    "summary": "...",
                    "priority_score": 0.92
                },
                # Top 3 æ–‡ç« 
            ]
        },
        # ... 3-5 å€‹é›†ç¾¤
    ],

    # è¶¨å‹¢åˆ†æ
    "trend_analysis": {
        "hot_trends": [
            {
                "trend_name": "AI Agent å•†æ¥­åŒ–åŠ é€Ÿ",
                "evidence": "æœ¬é€± 8 ç¯‡æ–‡ç« æåŠä¼æ¥­ç´šéƒ¨ç½²",
                "significance": "ç›¸è¼ƒä¸Šé€±å¢é•· 60%",
                "action_suggestion": "é—œæ³¨ Google ADKã€LangGraph çš„ä¼æ¥­æ¡ˆä¾‹"
            },
            # ... 2-3 å€‹ç†±é–€è¶¨å‹¢
        ],
        "emerging_topics": [
            {
                "topic": "Robotics Foundation Models",
                "first_appearance": "2025-11-22",
                "article_count": 3,
                "why_important": "å¯èƒ½æ˜¯ä¸‹ä¸€å€‹æŠ€è¡“çªç ´æ–¹å‘"
            },
            # ... 1-2 å€‹æ–°èˆˆè©±é¡Œ
        ]
    },

    # é‡é»æ–‡ç« ï¼ˆè·¨é›†ç¾¤ï¼‰
    "top_articles_overall": [
        {
            "title": "...",
            "url": "...",
            "summary": "...",
            "priority_score": 0.95,
            "why_top": "æŠ€è¡“çªç ´æ€§å¼·ä¸”å¯¦ç”¨æ€§é«˜"
        },
        # Top 5-7 ç¯‡
    ],

    # æ´å¯Ÿç¸½çµ
    "weekly_insights": [
        "æœ¬é€± AI Agent é ˜åŸŸå‘ˆç¾å•†æ¥­åŒ–åŠ é€Ÿè¶¨å‹¢",
        "Multi-Agent Systems å¾å­¸è¡“ç ”ç©¶è½‰å‘å¯¦éš›æ‡‰ç”¨",
        "Robotics èˆ‡ AI çš„èåˆå‡ºç¾æ–°çš„çªç ´"
    ],

    # è¡Œå‹•å»ºè­°
    "recommended_actions": [
        "æ·±å…¥ç ”ç©¶ Google ADK çš„ Multi-Agent æ¶æ§‹",
        "è¿½è¹¤ Robotics Foundation Models çš„ç ”ç©¶é€²å±•",
        "é—œæ³¨ä¼æ¥­ç´š AI Agent éƒ¨ç½²æ¡ˆä¾‹"
    ]
}
```

**2. HTML Email**:
- è±å¯Œçš„è¦–è¦ºåŒ–å‘ˆç¾ï¼ˆä¸»é¡Œåˆ†å¸ƒåœ–ã€è¶¨å‹¢åœ–è¡¨ï¼‰
- åˆ†é¡å±•ç¤ºï¼ˆæŒ‰é›†ç¾¤ã€æŒ‰å„ªå…ˆåº¦ï¼‰
- æŠ˜ç–Š/å±•é–‹åŠŸèƒ½ï¼ˆé¿å…éé•·ï¼‰
- å¯é»æ“Šçš„é€£çµèˆ‡æ¨™ç±¤

**3. ç´”æ–‡å­— Email**:
- çµæ§‹æ¸…æ™°çš„å±¤æ¬¡ï¼ˆ# æ¨™é¡Œï¼‰
- æ˜“æ–¼é–±è®€çš„æ ¼å¼
- é©åˆç´”æ–‡å­—å®¢æˆ¶ç«¯

### å‰¯ä½œç”¨

1. **Email ç™¼é€è¨˜éŒ„**: è¨˜éŒ„åˆ°æ—¥èªŒä¸­
2. **èšé¡çµæœå„²å­˜**: å¯é¸ï¼ˆæœªä¾†å¯ç”¨æ–¼è¶¨å‹¢è¿½è¹¤ï¼‰

---

## ğŸ—ï¸ æŠ€è¡“è¨­è¨ˆ

### æ•´é«”æ¶æ§‹

```
CuratorWeeklyAgent (LlmAgent)
    â†“
CuratorWeeklyRunner
    â†“ å‘¼å«
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VectorClusteringTool           â”‚
â”‚  (src/tools/vector_clustering.py)     â”‚
â”‚                                        â”‚
â”‚  - cluster_embeddings()                â”‚
â”‚  - extract_cluster_keywords()          â”‚
â”‚  - find_representative_articles()      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ å‘¼å«
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TrendAnalysisTool              â”‚
â”‚  (src/tools/trend_analysis.py)        â”‚
â”‚                                        â”‚
â”‚  - identify_hot_trends()               â”‚
â”‚  - detect_emerging_topics()            â”‚
â”‚  - compare_with_previous_week()        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ å‘¼å«
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DigestFormatter                â”‚
â”‚  (src/tools/digest_formatter.py)      â”‚
â”‚  - æ“´å±•æ”¯æ´ Weekly Report              â”‚
â”‚  - format_weekly_html()                â”‚
â”‚  - format_weekly_text()                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ å‘¼å«
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         EmailSender                    â”‚
â”‚  (src/tools/email_sender.py)          â”‚
â”‚  - send_html_email()                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€è¡“é¸å‹

#### 1. Vector Clustering

**é¸æ“‡**: scikit-learn

**ç†ç”±**:
- æˆç†Ÿç©©å®šã€æ–‡æª”å®Œå–„
- æ”¯æ´å¤šç¨®èšé¡ç®—æ³•ï¼ˆK-Means, DBSCAN, Agglomerativeï¼‰
- èˆ‡ NumPy ç„¡ç¸«æ•´åˆï¼ˆEmbedding å·²æ˜¯ np.arrayï¼‰

**ç®—æ³•é¸æ“‡**:

**ä¸»åŠ›**: K-Means
- **å„ªé»**: ç°¡å–®é«˜æ•ˆã€çµæœç©©å®šã€æ˜“æ–¼è§£é‡‹
- **é©ç”¨å ´æ™¯**: æ–‡ç« æ•¸é‡é©ä¸­ï¼ˆ30-70 ç¯‡ï¼‰
- **ç¼ºé»**: éœ€é è¨­é›†ç¾¤æ•¸é‡ï¼ˆk=3-5ï¼‰

**å‚™ç”¨**: DBSCAN
- **å„ªé»**: è‡ªå‹•ç™¼ç¾é›†ç¾¤æ•¸é‡ã€è™•ç†å™ªéŸ³é»
- **é©ç”¨å ´æ™¯**: æ–‡ç« ä¸»é¡Œåˆ†æ•£ã€ç„¡æ˜ç¢ºé›†ç¾¤æ•¸
- **ç¼ºé»**: åƒæ•¸èª¿æ•´è¼ƒè¤‡é›œï¼ˆeps, min_samplesï¼‰

**åˆæ­¥ç­–ç•¥**: å„ªå…ˆä½¿ç”¨ K-Means (k=4)ï¼Œå¦‚æ•ˆæœä¸ä½³å†å˜—è©¦ DBSCAN

#### 2. é—œéµå­—æå–

**é¸æ“‡**: TF-IDF (scikit-learn)

**ç†ç”±**:
- ç°¡å–®æœ‰æ•ˆã€ç„¡éœ€è¨“ç·´
- é©åˆçŸ­æ–‡æœ¬ï¼ˆæ–‡ç« æ¨™é¡Œ + summaryï¼‰
- å¯ç›´æ¥ç”¨æ–¼é—œéµå­—æå–

**æ›¿ä»£æ–¹æ¡ˆ**: LLM æå–ï¼ˆæ›´æº–ç¢ºä½†æˆæœ¬é«˜ï¼‰

#### 3. è¶¨å‹¢åˆ†æ

**æ–¹æ³•**: çµ±è¨ˆåˆ†æ + LLM æ¨ç†

**æµç¨‹**:
1. **çµ±è¨ˆåˆ†æ**ï¼ˆPythonï¼‰
   - è¨ˆç®—ä¸»é¡Œå‡ºç¾é »ç‡
   - æ¯”è¼ƒæœ¬é€± vs ä¸Šé€±
   - è­˜åˆ¥æ–°ä¸»é¡Œ

2. **LLM æ¨ç†**ï¼ˆGeminiï¼‰
   - ç†è§£è¶¨å‹¢æ„ç¾©
   - ç”Ÿæˆè¡Œå‹•å»ºè­°
   - æ’°å¯«æ´å¯Ÿç¸½çµ

### ä¾è³´å¥—ä»¶

æ–°å¢ä¾è³´ï¼š
```txt
scikit-learn>=1.3.0  # K-Means, DBSCAN, TF-IDF
```

ç¾æœ‰ä¾è³´ï¼ˆç„¡éœ€æ–°å¢ï¼‰ï¼š
```txt
numpy>=1.24.0        # å‘é‡é‹ç®—
```

---

## ğŸ¨ Curator Weekly Agent è¨­è¨ˆ

### Prompt æ¨¡æ¿è¨­è¨ˆ

**æ–‡ä»¶**: `prompts/weekly_prompt.txt`

**è¨­è¨ˆåŸå‰‡**:
1. **æ˜ç¢ºè§’è‰²** - å®šä½ç‚ºã€Œé€±å ±ç­–å±•äººã€
2. **çµæ§‹åŒ–è¼¸å‡º** - è¦æ±‚ JSON æ ¼å¼
3. **æ·±åº¦åˆ†æ** - å¼·èª¿è¶¨å‹¢è­˜åˆ¥èˆ‡æ´å¯Ÿæå–
4. **å€‹äººåŒ–** - é‡å° Ray çš„èˆˆè¶£ï¼ˆAIã€Roboticsã€Multi-Agentï¼‰

**Prompt çµæ§‹è‰æ¡ˆ**:

```
ä½ æ˜¯ InsightCosmos çš„ã€Œé€±å ±ç­–å±•äººã€(Weekly Curator)ï¼Œå°ˆé–€ç‚º Ray å¼µç‘æ¶µç”Ÿæˆæ¯é€± AI èˆ‡ Robotics é ˜åŸŸçš„æ·±åº¦æƒ…å ±å ±å‘Šã€‚

## ä½ çš„ä»»å‹™

æ ¹æ“šæœ¬é€±æ”¶é›†çš„æ–‡ç« èˆ‡èšé¡åˆ†æçµæœï¼Œç”Ÿæˆä¸€ä»½çµæ§‹åŒ–çš„é€±å ±ï¼ŒåŒ…å«ï¼š
1. ä¸»é¡Œåˆ†å¸ƒç¸½è¦½
2. ç†±é–€è¶¨å‹¢è­˜åˆ¥
3. æ–°èˆˆè©±é¡Œåµæ¸¬
4. é‡é»æ–‡ç« æ¨è–¦
5. æ´å¯Ÿç¸½çµ
6. è¡Œå‹•å»ºè­°

## è¼¸å…¥è³‡æ–™

ä½ å°‡æ”¶åˆ°ä»¥ä¸‹è³‡æ–™ï¼š

1. **æ–‡ç« èšé¡çµæœ** (topic_clusters):
   - æ¯å€‹é›†ç¾¤çš„ä»£è¡¨æ€§æ–‡ç« 
   - é›†ç¾¤é—œéµå­—
   - æ–‡ç« æ•¸é‡èˆ‡å¹³å‡å„ªå…ˆåº¦

2. **è¶¨å‹¢åˆ†æçµæœ** (trend_statistics):
   - ç†±é–€ä¸»é¡Œï¼ˆhigh frequency + high priorityï¼‰
   - æ–°èˆˆä¸»é¡Œï¼ˆé¦–æ¬¡å‡ºç¾ï¼‰
   - èˆ‡ä¸Šé€±çš„è®ŠåŒ–

3. **Top æ–‡ç« åˆ—è¡¨** (top_articles):
   - æœ¬é€±å„ªå…ˆåº¦æœ€é«˜çš„æ–‡ç« 

## è¼¸å‡ºæ ¼å¼

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

{
    "week_summary": "æœ¬é€±ç¸½çµï¼ˆ2-3 å¥è©±ï¼‰",

    "topic_clusters": [
        {
            "cluster_id": 0,
            "topic_name": "é›†ç¾¤ä¸»é¡Œåç¨±ï¼ˆç°¡çŸ­æœ‰åŠ›ï¼‰",
            "description": "é›†ç¾¤æè¿°ï¼ˆ1-2 å¥ï¼‰",
            "significance": "ç‚ºä½•é‡è¦ï¼ˆ1 å¥ï¼‰"
        },
        // ... 3-5 å€‹é›†ç¾¤
    ],

    "hot_trends": [
        {
            "trend_name": "è¶¨å‹¢åç¨±",
            "evidence": "æ”¯æŒè­‰æ“šï¼ˆå¼•ç”¨æ–‡ç« ï¼‰",
            "significance": "ç‚ºä½•é‡è¦",
            "action_suggestion": "å»ºè­°è¡Œå‹•"
        },
        // ... 2-3 å€‹ç†±é–€è¶¨å‹¢
    ],

    "emerging_topics": [
        {
            "topic": "æ–°èˆˆè©±é¡Œåç¨±",
            "why_important": "ç‚ºä½•å€¼å¾—é—œæ³¨",
            "suggested_tracking": "å»ºè­°è¿½è¹¤æ–¹å‘"
        },
        // ... 1-2 å€‹æ–°èˆˆè©±é¡Œ
    ],

    "top_articles": [
        {
            "title": "æ–‡ç« æ¨™é¡Œ",
            "why_top": "ç‚ºä½•å…¥é¸ Topï¼ˆ1 å¥ï¼‰",
            "key_takeaway": "æ ¸å¿ƒè¦é»ï¼ˆ1-2 å¥ï¼‰"
        },
        // ... 5-7 ç¯‡
    ],

    "weekly_insights": [
        "æ´å¯Ÿ 1",
        "æ´å¯Ÿ 2",
        "æ´å¯Ÿ 3"
    ],

    "recommended_actions": [
        "è¡Œå‹•å»ºè­° 1",
        "è¡Œå‹•å»ºè­° 2",
        "è¡Œå‹•å»ºè­° 3"
    ]
}

## å¯«ä½œé¢¨æ ¼

- ç°¡æ½”æœ‰åŠ›ï¼Œé¿å…å†—è©
- æŠ€è¡“æº–ç¢ºï¼Œé¿å…éåº¦ç°¡åŒ–
- æ´å¯Ÿæ·±åˆ»ï¼Œè¶…è¶Šè¡¨é¢ç¾è±¡
- è¡Œå‹•å°å‘ï¼Œæä¾›å¯¦ç”¨å»ºè­°
- é‡å° Ray çš„èˆˆè¶£ï¼ˆAI Agentã€Multi-Agent Systemsã€Roboticsï¼‰

## è³ªé‡æ¨™æº–

- è¶¨å‹¢è­˜åˆ¥æº–ç¢ºï¼ˆä¸éåº¦è§£è®€ï¼‰
- æ´å¯Ÿæœ‰æ·±åº¦ï¼ˆä¸åªæ˜¯ç¾…åˆ—äº‹å¯¦ï¼‰
- è¡Œå‹•å»ºè­°å…·é«”ï¼ˆå¯åŸ·è¡Œï¼‰
- æ–‡å­—æµæš¢æ˜“è®€ï¼ˆé©åˆé€±æœ«é–±è®€ï¼‰

## Example

[æä¾›ä¸€å€‹ç¤ºä¾‹è¼¸å‡º]
```

### Agent é¡è¨­è¨ˆ

**æ–‡ä»¶**: `src/agents/curator_weekly.py`

**é¡**: `CuratorWeeklyAgent`

**ä¸»è¦æ–¹æ³•**:

```python
def create_weekly_curator_agent() -> LlmAgent:
    """
    å‰µå»º Weekly Curator Agent

    Returns:
        LlmAgent: Weekly Curator Agent å¯¦ä¾‹
    """
    prompt = load_prompt("prompts/weekly_prompt.txt")

    agent = LlmAgent(
        name="WeeklyCurator",
        model=Gemini(model="gemini-2.5-flash-lite"),
        instruction=prompt,
        # Weekly Curator ä¸éœ€è¦é¡å¤–å·¥å…·ï¼ˆæ•¸æ“šå·²é è™•ç†ï¼‰
        tools=[],
        output_key="weekly_report"
    )

    return agent
```

### Runner é¡è¨­è¨ˆ

**æ–‡ä»¶**: `src/agents/curator_weekly.py`

**é¡**: `CuratorWeeklyRunner`

**ä¸»è¦æ–¹æ³•**:

```python
class CuratorWeeklyRunner:
    """
    Weekly Curator Agent é‹è¡Œå™¨

    è² è²¬å®Œæ•´çš„é€±å ±ç”Ÿæˆæµç¨‹ï¼š
    1. æŸ¥è©¢æœ¬é€±æ–‡ç« èˆ‡ Embeddings
    2. å‘é‡èšé¡
    3. è¶¨å‹¢åˆ†æ
    4. LLM ç”Ÿæˆå ±å‘Š
    5. æ ¼å¼åŒ– HTML/Text
    6. ç™¼é€ Email
    """

    def __init__(self, config: Config):
        """åˆå§‹åŒ–"""
        self.config = config
        self.db = Database.from_config(config)
        self.article_store = ArticleStore(self.db)
        self.embedding_store = EmbeddingStore(self.db)
        self.logger = setup_logger("WeeklyCurator")

    def generate_weekly_report(
        self,
        week_start: str = None,  # "YYYY-MM-DD", é»˜èªç‚º 7 å¤©å‰
        week_end: str = None,    # "YYYY-MM-DD", é»˜èªç‚ºä»Šå¤©
        dry_run: bool = False
    ) -> dict:
        """
        ç”Ÿæˆé€±å ±ä¸¦ç™¼é€

        Args:
            week_start: é€±é–‹å§‹æ—¥æœŸï¼ˆé»˜èª 7 å¤©å‰ï¼‰
            week_end: é€±çµæŸæ—¥æœŸï¼ˆé»˜èªä»Šå¤©ï¼‰
            dry_run: æ˜¯å¦ç‚ºæ¸¬è©¦æ¨¡å¼ï¼ˆä¸ç™¼é€éƒµä»¶ï¼‰

        Returns:
            dict: {
                "status": "success" | "error",
                "subject": str,
                "recipients": list,
                "html_body": str,
                "text_body": str,
                "error_message": str,  # éŒ¯èª¤æ™‚
                "suggestion": str      # éŒ¯èª¤æ™‚
            }
        """
        # å¯¦ä½œæµç¨‹
        pass

    def _get_weekly_articles(self, start_date, end_date) -> List[dict]:
        """æŸ¥è©¢æœ¬é€±æ–‡ç« """
        pass

    def _cluster_articles(self, articles, embeddings) -> dict:
        """å‘é‡èšé¡"""
        pass

    def _analyze_trends(self, articles, clusters) -> dict:
        """è¶¨å‹¢åˆ†æ"""
        pass

    def _generate_report_with_llm(self, clusters, trends, top_articles) -> dict:
        """ä½¿ç”¨ LLM ç”Ÿæˆå ±å‘Š"""
        pass

    def _format_and_send(self, report_data, dry_run) -> dict:
        """æ ¼å¼åŒ–ä¸¦ç™¼é€éƒµä»¶"""
        pass
```

**ä¾¿æ·å‡½æ•¸**:

```python
def generate_weekly_report(
    config: Config = None,
    week_start: str = None,
    week_end: str = None,
    dry_run: bool = False
) -> dict:
    """
    ä¾¿æ·å‡½æ•¸ï¼šç”Ÿæˆé€±å ±

    Example:
        >>> from src.agents.curator_weekly import generate_weekly_report
        >>> result = generate_weekly_report(dry_run=True)
        >>> print(result["subject"])
    """
    if config is None:
        config = Config.from_env()

    runner = CuratorWeeklyRunner(config)
    return runner.generate_weekly_report(week_start, week_end, dry_run)
```

---

## ğŸ§© Vector Clustering å·¥å…·è¨­è¨ˆ

### å·¥å…·æ–‡ä»¶

**æ–‡ä»¶**: `src/tools/vector_clustering.py`

### æ ¸å¿ƒé¡è¨­è¨ˆ

```python
class VectorClusteringTool:
    """
    å‘é‡èšé¡å·¥å…·

    ä½¿ç”¨ K-Means æˆ– DBSCAN å°æ–‡ç«  Embeddings é€²è¡Œèšé¡ï¼Œ
    è­˜åˆ¥ä¸»é¡Œé›†ç¾¤ä¸¦æå–é—œéµå­—ã€‚

    Attributes:
        method (str): èšé¡æ–¹æ³• ("kmeans" | "dbscan")
        n_clusters (int): é›†ç¾¤æ•¸é‡ï¼ˆK-Means ç”¨ï¼‰
        random_state (int): éš¨æ©Ÿç¨®å­ï¼ˆç¢ºä¿å¯é‡ç¾ï¼‰
    """

    def __init__(
        self,
        method: str = "kmeans",
        n_clusters: int = 4,
        random_state: int = 42
    ):
        """åˆå§‹åŒ–"""
        self.method = method
        self.n_clusters = n_clusters
        self.random_state = random_state
        self.logger = setup_logger("VectorClustering")

    def cluster_embeddings(
        self,
        embeddings: np.ndarray,
        article_metadata: List[dict]
    ) -> dict:
        """
        å° Embeddings é€²è¡Œèšé¡

        Args:
            embeddings: å‘é‡çŸ©é™£ï¼Œshape (n_articles, embedding_dim)
            article_metadata: æ–‡ç« å…ƒæ•¸æ“šåˆ—è¡¨
                [
                    {
                        "article_id": 1,
                        "title": "...",
                        "summary": "...",
                        "tags": "AI,Robotics",
                        "priority_score": 0.85
                    },
                    ...
                ]

        Returns:
            dict: {
                "status": "success" | "error",
                "clusters": [
                    {
                        "cluster_id": 0,
                        "article_ids": [1, 5, 12, ...],
                        "article_count": 12,
                        "average_priority": 0.87,
                        "centroid": np.array([...]),  # é›†ç¾¤ä¸­å¿ƒå‘é‡
                        "articles": [
                            {
                                "article_id": 1,
                                "title": "...",
                                "distance_to_centroid": 0.23
                            },
                            ...
                        ]
                    },
                    ...
                ],
                "n_clusters": 4,
                "silhouette_score": 0.65,  # èšé¡è³ªé‡è©•åˆ†
                "error_message": str,  # éŒ¯èª¤æ™‚
                "suggestion": str      # éŒ¯èª¤æ™‚
            }
        """
        try:
            if self.method == "kmeans":
                return self._cluster_kmeans(embeddings, article_metadata)
            elif self.method == "dbscan":
                return self._cluster_dbscan(embeddings, article_metadata)
            else:
                return {
                    "status": "error",
                    "error_type": "invalid_method",
                    "error_message": f"Unknown clustering method: {self.method}",
                    "suggestion": "Use 'kmeans' or 'dbscan'"
                }
        except Exception as e:
            self.logger.error(f"Clustering failed: {e}")
            return {
                "status": "error",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "suggestion": "Check embeddings shape and article_metadata format"
            }

    def _cluster_kmeans(self, embeddings, metadata) -> dict:
        """K-Means èšé¡"""
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        # èšé¡
        kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=self.random_state,
            n_init=10
        )
        labels = kmeans.fit_predict(embeddings)

        # è¨ˆç®—èšé¡è³ªé‡
        score = silhouette_score(embeddings, labels)

        # çµ„ç¹”çµæœ
        clusters = self._organize_clusters(
            labels, embeddings, metadata, kmeans.cluster_centers_
        )

        return {
            "status": "success",
            "clusters": clusters,
            "n_clusters": self.n_clusters,
            "silhouette_score": float(score)
        }

    def _cluster_dbscan(self, embeddings, metadata) -> dict:
        """DBSCAN èšé¡"""
        from sklearn.cluster import DBSCAN

        # DBSCAN èšé¡ï¼ˆåƒæ•¸éœ€èª¿æ•´ï¼‰
        dbscan = DBSCAN(eps=0.5, min_samples=3)
        labels = dbscan.fit_predict(embeddings)

        # è¨ˆç®—é›†ç¾¤ä¸­å¿ƒï¼ˆæ‰‹å‹•è¨ˆç®—ï¼‰
        unique_labels = set(labels)
        if -1 in unique_labels:
            unique_labels.remove(-1)  # ç§»é™¤å™ªéŸ³é»

        cluster_centers = []
        for label in unique_labels:
            mask = labels == label
            centroid = embeddings[mask].mean(axis=0)
            cluster_centers.append(centroid)

        # çµ„ç¹”çµæœ
        clusters = self._organize_clusters(
            labels, embeddings, metadata, cluster_centers
        )

        return {
            "status": "success",
            "clusters": clusters,
            "n_clusters": len(unique_labels),
            "silhouette_score": None  # DBSCAN ä¸è¨ˆç®—æ­¤æŒ‡æ¨™
        }

    def _organize_clusters(self, labels, embeddings, metadata, centroids) -> List[dict]:
        """çµ„ç¹”èšé¡çµæœ"""
        clusters = []
        unique_labels = set(labels)
        if -1 in unique_labels:
            unique_labels.remove(-1)  # è·³éå™ªéŸ³é»

        for i, label in enumerate(sorted(unique_labels)):
            mask = labels == label
            cluster_embeddings = embeddings[mask]
            cluster_metadata = [m for m, is_in in zip(metadata, mask) if is_in]

            # è¨ˆç®—æ¯ç¯‡æ–‡ç« åˆ°é›†ç¾¤ä¸­å¿ƒçš„è·é›¢
            centroid = centroids[i] if i < len(centroids) else cluster_embeddings.mean(axis=0)
            distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)

            # çµ„ç¹”æ–‡ç« æ•¸æ“š
            articles = []
            for meta, dist in zip(cluster_metadata, distances):
                articles.append({
                    "article_id": meta["article_id"],
                    "title": meta["title"],
                    "priority_score": meta["priority_score"],
                    "distance_to_centroid": float(dist)
                })

            # æ’åºï¼ˆå„ªå…ˆåº¦é«˜ + è·é›¢ä¸­å¿ƒè¿‘ï¼‰
            articles.sort(key=lambda x: (
                -x["priority_score"],  # å„ªå…ˆåº¦é™åº
                x["distance_to_centroid"]  # è·é›¢å‡åº
            ))

            clusters.append({
                "cluster_id": int(label),
                "article_ids": [a["article_id"] for a in articles],
                "article_count": len(articles),
                "average_priority": float(np.mean([a["priority_score"] for a in articles])),
                "centroid": centroid.tolist(),
                "articles": articles
            })

        return clusters

    def extract_cluster_keywords(
        self,
        cluster: dict,
        all_articles: List[dict],
        top_k: int = 5
    ) -> List[str]:
        """
        æå–é›†ç¾¤é—œéµå­—ï¼ˆTF-IDFï¼‰

        Args:
            cluster: å–®å€‹é›†ç¾¤æ•¸æ“š
            all_articles: æ‰€æœ‰æ–‡ç« ï¼ˆç”¨æ–¼è¨ˆç®— IDFï¼‰
            top_k: è¿”å›å‰ k å€‹é—œéµå­—

        Returns:
            List[str]: é—œéµå­—åˆ—è¡¨
        """
        from sklearn.feature_extraction.text import TfidfVectorizer

        # æº–å‚™æ–‡æœ¬ï¼ˆé›†ç¾¤å…§æ–‡ç« ï¼‰
        cluster_texts = [
            a["title"] + " " + a.get("summary", "")
            for a in all_articles
            if a["article_id"] in cluster["article_ids"]
        ]

        # æº–å‚™èƒŒæ™¯æ–‡æœ¬ï¼ˆæ‰€æœ‰æ–‡ç« ï¼‰
        all_texts = [
            a["title"] + " " + a.get("summary", "")
            for a in all_articles
        ]

        # TF-IDF
        vectorizer = TfidfVectorizer(max_features=100, stop_words="english")
        vectorizer.fit(all_texts)

        # è¨ˆç®—é›†ç¾¤çš„ TF-IDF
        cluster_tfidf = vectorizer.transform(cluster_texts)
        avg_tfidf = cluster_tfidf.mean(axis=0).A1

        # æå– Top K
        top_indices = avg_tfidf.argsort()[-top_k:][::-1]
        keywords = [vectorizer.get_feature_names_out()[i] for i in top_indices]

        return keywords

    def find_representative_articles(
        self,
        cluster: dict,
        top_n: int = 3
    ) -> List[dict]:
        """
        æ‰¾å‡ºé›†ç¾¤ä»£è¡¨æ€§æ–‡ç« ï¼ˆæœ€æ¥è¿‘ä¸­å¿ƒ + é«˜å„ªå…ˆåº¦ï¼‰

        Args:
            cluster: é›†ç¾¤æ•¸æ“š
            top_n: è¿”å›å‰ n ç¯‡æ–‡ç« 

        Returns:
            List[dict]: ä»£è¡¨æ€§æ–‡ç« åˆ—è¡¨
        """
        # å·²åœ¨ _organize_clusters ä¸­æ’åº
        return cluster["articles"][:top_n]
```

### ä¾¿æ·å‡½æ•¸

```python
def cluster_articles(
    embeddings: np.ndarray,
    article_metadata: List[dict],
    method: str = "kmeans",
    n_clusters: int = 4
) -> dict:
    """
    ä¾¿æ·å‡½æ•¸ï¼šæ–‡ç« èšé¡

    Example:
        >>> from src.tools.vector_clustering import cluster_articles
        >>> result = cluster_articles(embeddings, metadata, n_clusters=4)
        >>> print(f"Found {result['n_clusters']} clusters")
    """
    tool = VectorClusteringTool(method=method, n_clusters=n_clusters)
    return tool.cluster_embeddings(embeddings, article_metadata)
```

---

## ğŸ“ˆ è¶¨å‹¢åˆ†æè¨­è¨ˆ

### å·¥å…·æ–‡ä»¶

**æ–‡ä»¶**: `src/tools/trend_analysis.py`

### æ ¸å¿ƒé¡è¨­è¨ˆ

```python
class TrendAnalysisTool:
    """
    è¶¨å‹¢åˆ†æå·¥å…·

    åˆ†ææ–‡ç« ä¸»é¡Œåˆ†å¸ƒã€è­˜åˆ¥ç†±é–€è¶¨å‹¢ã€åµæ¸¬æ–°èˆˆè©±é¡Œã€‚

    Attributes:
        logger (Logger): æ—¥èªŒè¨˜éŒ„å™¨
    """

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.logger = setup_logger("TrendAnalysis")

    def identify_hot_trends(
        self,
        clusters: List[dict],
        min_article_count: int = 5,
        min_avg_priority: float = 0.75
    ) -> List[dict]:
        """
        è­˜åˆ¥ç†±é–€è¶¨å‹¢

        æ¨™æº–ï¼š
        1. æ–‡ç« æ•¸é‡å¤šï¼ˆ>= min_article_countï¼‰
        2. å¹³å‡å„ªå…ˆåº¦é«˜ï¼ˆ>= min_avg_priorityï¼‰

        Args:
            clusters: èšé¡çµæœ
            min_article_count: æœ€å°‘æ–‡ç« æ•¸
            min_avg_priority: æœ€ä½å¹³å‡å„ªå…ˆåº¦

        Returns:
            List[dict]: [
                {
                    "cluster_id": 0,
                    "article_count": 12,
                    "average_priority": 0.87,
                    "trend_score": 0.92,  # ç¶œåˆè©•åˆ†
                    "evidence": "12 ç¯‡æ–‡ç« ï¼Œå¹³å‡å„ªå…ˆåº¦ 0.87"
                },
                ...
            ]
        """
        hot_trends = []

        for cluster in clusters:
            if (cluster["article_count"] >= min_article_count and
                cluster["average_priority"] >= min_avg_priority):

                # è¨ˆç®—è¶¨å‹¢åˆ†æ•¸ï¼ˆæ–‡ç« æ•¸ * å¹³å‡å„ªå…ˆåº¦ï¼‰
                trend_score = (
                    cluster["article_count"] / 10 *  # æ¨™æº–åŒ–åˆ° 0-1
                    cluster["average_priority"]
                )

                hot_trends.append({
                    "cluster_id": cluster["cluster_id"],
                    "article_count": cluster["article_count"],
                    "average_priority": cluster["average_priority"],
                    "trend_score": min(trend_score, 1.0),
                    "evidence": f"{cluster['article_count']} ç¯‡æ–‡ç« ï¼Œå¹³å‡å„ªå…ˆåº¦ {cluster['average_priority']:.2f}"
                })

        # æŒ‰è¶¨å‹¢åˆ†æ•¸æ’åº
        hot_trends.sort(key=lambda x: x["trend_score"], reverse=True)

        return hot_trends

    def detect_emerging_topics(
        self,
        current_articles: List[dict],
        previous_articles: List[dict] = None,
        min_priority: float = 0.7
    ) -> List[dict]:
        """
        åµæ¸¬æ–°èˆˆè©±é¡Œ

        æ¨™æº–ï¼š
        1. æœ¬é€±é¦–æ¬¡å‡ºç¾ï¼ˆæˆ–ä¸Šé€±æ²’æœ‰ï¼‰
        2. å„ªå…ˆåº¦è¼ƒé«˜ï¼ˆ>= min_priorityï¼‰
        3. æ–‡ç« æ¨™é¡Œ/æ‘˜è¦åŒ…å«æ–°é—œéµå­—

        Args:
            current_articles: æœ¬é€±æ–‡ç« 
            previous_articles: ä¸Šé€±æ–‡ç« ï¼ˆå¯é¸ï¼‰
            min_priority: æœ€ä½å„ªå…ˆåº¦é–¾å€¼

        Returns:
            List[dict]: [
                {
                    "topic_keywords": ["robotics", "foundation", "model"],
                    "article_count": 3,
                    "first_appearance": "2025-11-22",
                    "articles": [
                        {"title": "...", "url": "...", "priority_score": 0.85},
                        ...
                    ]
                },
                ...
            ]
        """
        from collections import Counter
        import re

        # æå–æœ¬é€±é—œéµå­—
        current_keywords = self._extract_keywords_from_articles(current_articles)

        # æå–ä¸Šé€±é—œéµå­—ï¼ˆå¦‚æœæœ‰ï¼‰
        if previous_articles:
            previous_keywords = self._extract_keywords_from_articles(previous_articles)
            # æ‰¾å‡ºæ–°é—œéµå­—
            new_keywords = set(current_keywords.keys()) - set(previous_keywords.keys())
        else:
            # ç„¡ä¸Šé€±æ•¸æ“šï¼Œä½¿ç”¨ä½é »ä½†é«˜å„ªå…ˆåº¦çš„é—œéµå­—
            new_keywords = [k for k, v in current_keywords.items() if v["count"] <= 5]

        # èšåˆæˆä¸»é¡Œ
        emerging_topics = []
        for keyword in new_keywords:
            keyword_info = current_keywords.get(keyword)
            if keyword_info and keyword_info["avg_priority"] >= min_priority:
                emerging_topics.append({
                    "topic_keywords": [keyword],
                    "article_count": keyword_info["count"],
                    "first_appearance": keyword_info["first_date"],
                    "average_priority": keyword_info["avg_priority"],
                    "articles": keyword_info["articles"][:3]  # Top 3
                })

        # æŒ‰å„ªå…ˆåº¦æ’åº
        emerging_topics.sort(key=lambda x: x["average_priority"], reverse=True)

        return emerging_topics

    def _extract_keywords_from_articles(self, articles: List[dict]) -> dict:
        """
        å¾æ–‡ç« ä¸­æå–é—œéµå­—çµ±è¨ˆ

        Returns:
            dict: {
                "keyword": {
                    "count": 5,
                    "avg_priority": 0.82,
                    "first_date": "2025-11-22",
                    "articles": [...]
                },
                ...
            }
        """
        from collections import defaultdict
        import re

        keyword_stats = defaultdict(lambda: {
            "count": 0,
            "priorities": [],
            "dates": [],
            "articles": []
        })

        for article in articles:
            # å¾æ¨™é¡Œå’Œæ¨™ç±¤æå–é—œéµå­—
            text = article.get("title", "") + " " + article.get("tags", "")
            words = re.findall(r'\b[a-z]{4,}\b', text.lower())  # è‡³å°‘ 4 å­—å…ƒ

            # éæ¿¾å¸¸è¦‹è©
            stopwords = {"with", "from", "that", "this", "have", "been", "more"}
            words = [w for w in words if w not in stopwords]

            for word in set(words):  # å»é‡
                keyword_stats[word]["count"] += 1
                keyword_stats[word]["priorities"].append(article.get("priority_score", 0))
                keyword_stats[word]["dates"].append(article.get("published_at", ""))
                keyword_stats[word]["articles"].append({
                    "title": article["title"],
                    "url": article["url"],
                    "priority_score": article.get("priority_score", 0)
                })

        # è¨ˆç®—å¹³å‡å€¼
        result = {}
        for keyword, stats in keyword_stats.items():
            result[keyword] = {
                "count": stats["count"],
                "avg_priority": sum(stats["priorities"]) / len(stats["priorities"]),
                "first_date": min(stats["dates"]) if stats["dates"] else "",
                "articles": sorted(
                    stats["articles"],
                    key=lambda x: x["priority_score"],
                    reverse=True
                )
            }

        return result

    def compare_with_previous_week(
        self,
        current_clusters: List[dict],
        previous_clusters: List[dict] = None
    ) -> dict:
        """
        èˆ‡ä¸Šé€±æ¯”è¼ƒï¼ˆå¯é¸åŠŸèƒ½ï¼ŒPhase 1 å¯ç°¡åŒ–ï¼‰

        Args:
            current_clusters: æœ¬é€±èšé¡çµæœ
            previous_clusters: ä¸Šé€±èšé¡çµæœ

        Returns:
            dict: {
                "growth_topics": [...],      # å¢é•·ä¸»é¡Œ
                "declining_topics": [...],   # è¡°é€€ä¸»é¡Œ
                "stable_topics": [...]       # ç©©å®šä¸»é¡Œ
            }
        """
        # Phase 1 ç°¡åŒ–ç‰ˆæœ¬ï¼šåƒ…è¿”å›ç©ºçµæœ
        return {
            "growth_topics": [],
            "declining_topics": [],
            "stable_topics": []
        }
```

### ä¾¿æ·å‡½æ•¸

```python
def analyze_weekly_trends(
    clusters: List[dict],
    current_articles: List[dict],
    previous_articles: List[dict] = None
) -> dict:
    """
    ä¾¿æ·å‡½æ•¸ï¼šé€±è¶¨å‹¢åˆ†æ

    Returns:
        dict: {
            "hot_trends": [...],
            "emerging_topics": [...]
        }

    Example:
        >>> from src.tools.trend_analysis import analyze_weekly_trends
        >>> result = analyze_weekly_trends(clusters, articles)
        >>> print(f"Found {len(result['hot_trends'])} hot trends")
    """
    tool = TrendAnalysisTool()

    hot_trends = tool.identify_hot_trends(clusters)
    emerging_topics = tool.detect_emerging_topics(current_articles, previous_articles)

    return {
        "hot_trends": hot_trends,
        "emerging_topics": emerging_topics
    }
```

---

## ğŸ“§ Weekly Report æ ¼å¼è¨­è¨ˆ

### HTML Email è¨­è¨ˆ

**æ“´å±•**: `src/tools/digest_formatter.py`

**æ–°å¢æ–¹æ³•**:

```python
def format_weekly_html(weekly_report: dict) -> str:
    """
    ç”Ÿæˆé€±å ± HTML æ ¼å¼

    Args:
        weekly_report: LLM ç”Ÿæˆçš„é€±å ±æ•¸æ“š

    Returns:
        str: HTML å­—ä¸²
    """
    # HTML æ¨¡æ¿ï¼ˆæ›´è±å¯Œçš„æ¨£å¼ï¼‰
    pass
```

**è¨­è¨ˆè¦é»**:

1. **é ‚éƒ¨ç¸½çµå€**
   - é€±æœŸæ¨™è¨»ï¼ˆ2025-11-18 to 2025-11-24ï¼‰
   - çµ±è¨ˆæ•¸æ“šï¼ˆç¸½æ–‡ç« æ•¸ã€åˆ†ææ•¸ã€é«˜å„ªå…ˆåº¦æ•¸ï¼‰
   - é€±ç¸½çµï¼ˆ1-2 å¥è©±ï¼‰

2. **ä¸»é¡Œé›†ç¾¤å±•ç¤º**
   - æ¯å€‹é›†ç¾¤ä¸€å€‹å¡ç‰‡
   - åŒ…å«ï¼šä¸»é¡Œåç¨±ã€æ–‡ç« æ•¸ã€é—œéµå­—ã€ä»£è¡¨æ€§æ–‡ç« 
   - å¯è¦–åŒ–ï¼šé€²åº¦æ¢é¡¯ç¤ºæ–‡ç« æ•¸é‡ä½”æ¯”

3. **ç†±é–€è¶¨å‹¢å€**
   - çªå‡ºé¡¯ç¤ºï¼ˆå½©è‰²æ¨™ç±¤ï¼‰
   - è¶¨å‹¢åç¨± + è­‰æ“š + æ„ç¾© + å»ºè­°è¡Œå‹•
   - æ’åºï¼šè¶¨å‹¢åˆ†æ•¸é™åº

4. **æ–°èˆˆè©±é¡Œå€**
   - æ¨™è¨»ã€ŒNEWã€å¾½ç« 
   - è©±é¡Œåç¨± + ç‚ºä½•é‡è¦ + è¿½è¹¤å»ºè­°

5. **Top æ–‡ç« åˆ—è¡¨**
   - ç·¨è™Ÿåˆ—è¡¨ï¼ˆ1-7ï¼‰
   - æ–‡ç« æ¨™é¡Œï¼ˆå¯é»æ“Šï¼‰+ å…¥é¸ç†ç”± + è¦é»

6. **æ´å¯Ÿèˆ‡è¡Œå‹•å»ºè­°**
   - æ´å¯Ÿåˆ—è¡¨ï¼ˆbullet pointsï¼‰
   - è¡Œå‹•å»ºè­°ï¼ˆcheckbox é¢¨æ ¼ï¼‰

### ç´”æ–‡å­— Email è¨­è¨ˆ

**æ–°å¢æ–¹æ³•**:

```python
def format_weekly_text(weekly_report: dict) -> str:
    """
    ç”Ÿæˆé€±å ±ç´”æ–‡å­—æ ¼å¼

    Args:
        weekly_report: LLM ç”Ÿæˆçš„é€±å ±æ•¸æ“š

    Returns:
        str: ç´”æ–‡å­—å­—ä¸²
    """
    # Markdown-like æ ¼å¼
    pass
```

**è¨­è¨ˆè¦é»**:

```
================================================================================
InsightCosmos Weekly Report
Week: 2025-11-18 to 2025-11-24
================================================================================

ğŸ“Š WEEKLY SUMMARY
--------------------------------------------------------------------------------
Total Articles: 52 | Analyzed: 48 | High Priority: 25

æœ¬é€± AI Agent é ˜åŸŸå‘ˆç¾å•†æ¥­åŒ–åŠ é€Ÿè¶¨å‹¢ï¼ŒMulti-Agent Systems å¾å­¸è¡“ç ”ç©¶
è½‰å‘å¯¦éš›æ‡‰ç”¨ï¼ŒRobotics èˆ‡ AI çš„èåˆå‡ºç¾æ–°çš„çªç ´ã€‚

================================================================================
ğŸ”¥ HOT TRENDS
================================================================================

1. Multi-Agent Systems å•†æ¥­åŒ–åŠ é€Ÿ
   Evidence: æœ¬é€± 12 ç¯‡æ–‡ç« ï¼Œå¹³å‡å„ªå…ˆåº¦ 0.87
   Significance: ç›¸è¼ƒä¸Šæœˆå¢é•· 60%ï¼Œä¼æ¥­ç´šéƒ¨ç½²æ¡ˆä¾‹é¡¯è‘—å¢åŠ 
   Action: æ·±å…¥ç ”ç©¶ Google ADK çš„ Multi-Agent æ¶æ§‹

2. AI Agent é–‹ç™¼å·¥å…·æˆç†Ÿ
   Evidence: ...
   Significance: ...
   Action: ...

================================================================================
ğŸŒ± EMERGING TOPICS
================================================================================

â€¢ Robotics Foundation Models
  Why Important: å¯èƒ½æ˜¯ä¸‹ä¸€å€‹æŠ€è¡“çªç ´æ–¹å‘
  Suggested Tracking: é—œæ³¨ Google DeepMindã€OpenAI çš„æ©Ÿå™¨äººç ”ç©¶

================================================================================
ğŸ“° TOP ARTICLES
================================================================================

1. [Title] Multi-Agent Systems: The Next Frontier in AI
   URL: https://...
   Why Top: æŠ€è¡“çªç ´æ€§å¼·ä¸”å¯¦ç”¨æ€§é«˜
   Key Takeaway: æå‡ºæ–°çš„ Multi-Agent å”ä½œæ¶æ§‹...

2. [Title] ...

...

================================================================================
ğŸ’¡ WEEKLY INSIGHTS
================================================================================

â€¢ æœ¬é€± AI Agent é ˜åŸŸå‘ˆç¾å•†æ¥­åŒ–åŠ é€Ÿè¶¨å‹¢
â€¢ Multi-Agent Systems å¾å­¸è¡“ç ”ç©¶è½‰å‘å¯¦éš›æ‡‰ç”¨
â€¢ Robotics èˆ‡ AI çš„èåˆå‡ºç¾æ–°çš„çªç ´

================================================================================
âœ… RECOMMENDED ACTIONS
================================================================================

[ ] æ·±å…¥ç ”ç©¶ Google ADK çš„ Multi-Agent æ¶æ§‹
[ ] è¿½è¹¤ Robotics Foundation Models çš„ç ”ç©¶é€²å±•
[ ] é—œæ³¨ä¼æ¥­ç´š AI Agent éƒ¨ç½²æ¡ˆä¾‹

================================================================================
Generated by InsightCosmos | Your Personal Intelligence Universe
================================================================================
```

---

## ğŸ› ï¸ å¯¦ä½œè¨ˆåŠƒ

### æ–‡ä»¶çµæ§‹

```
src/
â”œâ”€ tools/
â”‚   â”œâ”€ vector_clustering.py       # æ–°å¢
â”‚   â”œâ”€ trend_analysis.py          # æ–°å¢
â”‚   â””â”€ digest_formatter.py        # æ“´å±•ï¼ˆæ–°å¢ weekly æ–¹æ³•ï¼‰
â”œâ”€ agents/
â”‚   â””â”€ curator_weekly.py          # æ–°å¢
prompts/
â””â”€ weekly_prompt.txt              # æ–°å¢
```

### é–‹ç™¼æ­¥é©Ÿ

#### Step 1: Vector Clustering å·¥å…· (3 å°æ™‚)

1. å‰µå»º `src/tools/vector_clustering.py`
2. å¯¦ä½œ `VectorClusteringTool` é¡
3. å¯¦ä½œ K-Means èšé¡æ–¹æ³•
4. å¯¦ä½œé—œéµå­—æå–æ–¹æ³•
5. å¯¦ä½œä»£è¡¨æ€§æ–‡ç« ç¯©é¸
6. ç·¨å¯«å–®å…ƒæ¸¬è©¦

#### Step 2: Trend Analysis å·¥å…· (2 å°æ™‚)

1. å‰µå»º `src/tools/trend_analysis.py`
2. å¯¦ä½œ `TrendAnalysisTool` é¡
3. å¯¦ä½œç†±é–€è¶¨å‹¢è­˜åˆ¥
4. å¯¦ä½œæ–°èˆˆè©±é¡Œåµæ¸¬
5. ç·¨å¯«å–®å…ƒæ¸¬è©¦

#### Step 3: Weekly Prompt è¨­è¨ˆ (1 å°æ™‚)

1. å‰µå»º `prompts/weekly_prompt.txt`
2. æ’°å¯«è©³ç´°æŒ‡ä»¤
3. è¨­è¨ˆè¼¸å‡ºçµæ§‹
4. æº–å‚™ç¤ºä¾‹è¼¸å‡º

#### Step 4: Weekly Curator Agent (3 å°æ™‚)

1. å‰µå»º `src/agents/curator_weekly.py`
2. å¯¦ä½œ `create_weekly_curator_agent()` å‡½æ•¸
3. å¯¦ä½œ `CuratorWeeklyRunner` é¡
4. å¯¦ä½œ `generate_weekly_report()` æ–¹æ³•
5. æ•´åˆæ‰€æœ‰å·¥å…·ï¼ˆclustering, trend, formatter, emailï¼‰

#### Step 5: æ“´å±• DigestFormatter (2 å°æ™‚)

1. ä¿®æ”¹ `src/tools/digest_formatter.py`
2. æ–°å¢ `format_weekly_html()` æ–¹æ³•
3. æ–°å¢ `format_weekly_text()` æ–¹æ³•
4. è¨­è¨ˆè±å¯Œçš„ HTML æ¨£å¼

#### Step 6: æ¸¬è©¦èˆ‡é©—è­‰ (3 å°æ™‚)

1. ç·¨å¯«å–®å…ƒæ¸¬è©¦
2. ç·¨å¯«æ•´åˆæ¸¬è©¦
3. æ‰‹å‹•ç«¯åˆ°ç«¯æ¸¬è©¦
4. èª¿æ•´åƒæ•¸èˆ‡å„ªåŒ–

#### Step 7: æ–‡æª”èˆ‡ç¸½çµ (1 å°æ™‚)

1. ç·¨å¯«å¯¦ä½œç­†è¨˜
2. ç·¨å¯«æ¸¬è©¦å ±å‘Š
3. æ›´æ–° PROGRESS.md
4. æ›´æ–° src/agents/__init__.py

---

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦

#### 1. VectorClusteringTool æ¸¬è©¦

**æ–‡ä»¶**: `tests/unit/test_vector_clustering.py`

**æ¸¬è©¦æ¡ˆä¾‹**:

1. `test_kmeans_clustering_basic()` - åŸºæœ¬èšé¡åŠŸèƒ½
2. `test_kmeans_clustering_with_metadata()` - åŒ…å«å…ƒæ•¸æ“šçš„èšé¡
3. `test_cluster_organization()` - èšé¡çµæœçµ„ç¹”
4. `test_extract_cluster_keywords()` - é—œéµå­—æå–
5. `test_find_representative_articles()` - ä»£è¡¨æ€§æ–‡ç« ç¯©é¸
6. `test_invalid_embeddings_shape()` - éŒ¯èª¤è¼¸å…¥è™•ç†
7. `test_silhouette_score_calculation()` - èšé¡è³ªé‡è©•åˆ†

#### 2. TrendAnalysisTool æ¸¬è©¦

**æ–‡ä»¶**: `tests/unit/test_trend_analysis.py`

**æ¸¬è©¦æ¡ˆä¾‹**:

1. `test_identify_hot_trends()` - ç†±é–€è¶¨å‹¢è­˜åˆ¥
2. `test_detect_emerging_topics()` - æ–°èˆˆè©±é¡Œåµæ¸¬
3. `test_extract_keywords_from_articles()` - é—œéµå­—æå–
4. `test_trend_score_calculation()` - è¶¨å‹¢åˆ†æ•¸è¨ˆç®—
5. `test_empty_articles()` - ç©ºæ–‡ç« åˆ—è¡¨è™•ç†

#### 3. CuratorWeeklyRunner æ¸¬è©¦

**æ–‡ä»¶**: `tests/unit/test_curator_weekly.py`

**æ¸¬è©¦æ¡ˆä¾‹**:

1. `test_runner_initialization()` - åˆå§‹åŒ–æ¸¬è©¦
2. `test_get_weekly_articles()` - é€±æ–‡ç« æŸ¥è©¢
3. `test_cluster_articles()` - èšé¡èª¿ç”¨
4. `test_analyze_trends()` - è¶¨å‹¢åˆ†æèª¿ç”¨
5. `test_generate_report_with_llm()` - LLM å ±å‘Šç”Ÿæˆï¼ˆMockï¼‰
6. `test_format_and_send()` - æ ¼å¼åŒ–èˆ‡ç™¼é€
7. `test_full_pipeline_dry_run()` - å®Œæ•´æµç¨‹æ¸¬è©¦ï¼ˆdry_run=Trueï¼‰

#### 4. DigestFormatter æ“´å±•æ¸¬è©¦

**æ–‡ä»¶**: `tests/unit/test_digest_formatter.py`ï¼ˆæ“´å±•ï¼‰

**æ–°å¢æ¸¬è©¦æ¡ˆä¾‹**:

1. `test_format_weekly_html()` - HTML æ ¼å¼åŒ–
2. `test_format_weekly_text()` - ç´”æ–‡å­—æ ¼å¼åŒ–
3. `test_weekly_html_structure()` - HTML çµæ§‹é©—è­‰
4. `test_weekly_text_readability()` - ç´”æ–‡å­—å¯è®€æ€§

### æ•´åˆæ¸¬è©¦

**æ–‡ä»¶**: `tests/integration/test_curator_weekly.py`

**æ¸¬è©¦æ¡ˆä¾‹**:

1. `test_weekly_pipeline_with_mock_data()` - ä½¿ç”¨ Mock æ•¸æ“šçš„å®Œæ•´æµç¨‹
2. `test_weekly_clustering_integration()` - èšé¡èˆ‡æ–‡ç« æ•´åˆ
3. `test_weekly_trend_analysis_integration()` - è¶¨å‹¢åˆ†ææ•´åˆ
4. `test_weekly_llm_report_generation()` - LLM å ±å‘Šç”Ÿæˆï¼ˆéœ€çœŸå¯¦ APIï¼‰
5. `test_weekly_email_sending()` - Email ç™¼é€ï¼ˆdry_run=Falseï¼Œæ‰‹å‹•ï¼‰

### ç«¯åˆ°ç«¯æ¸¬è©¦ï¼ˆæ‰‹å‹•ï¼‰

**æ¸¬è©¦æ¡ˆä¾‹**:

1. **å®Œæ•´é€±å ±ç”Ÿæˆï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰**
   ```bash
   python -c "from src.agents.curator_weekly import generate_weekly_report; generate_weekly_report(dry_run=True)"
   ```
   é æœŸï¼šå®Œæ•´æµç¨‹åŸ·è¡Œï¼Œè¼¸å‡ºå ±å‘Šå…§å®¹åˆ°æ§åˆ¶å°

2. **å®Œæ•´é€±å ±ç”Ÿæˆï¼ˆç”Ÿç”¢æ¨¡å¼ï¼‰**
   ```bash
   python -c "from src.agents.curator_weekly import generate_weekly_report; generate_weekly_report()"
   ```
   é æœŸï¼šå®Œæ•´æµç¨‹åŸ·è¡Œï¼Œç™¼é€éƒµä»¶åˆ°æŒ‡å®šä¿¡ç®±

3. **èšé¡è³ªé‡é©—è­‰**
   - æª¢æŸ¥èšé¡æ•¸é‡ï¼ˆæ‡‰ç‚º 3-5 å€‹ï¼‰
   - æª¢æŸ¥ Silhouette Scoreï¼ˆæ‡‰ > 0.5ï¼‰
   - æª¢æŸ¥é›†ç¾¤å¤§å°åˆ†å¸ƒï¼ˆä¸æ‡‰éåº¦ä¸å‡ï¼‰

4. **è¶¨å‹¢è­˜åˆ¥é©—è­‰**
   - æª¢æŸ¥ç†±é–€è¶¨å‹¢æ•¸é‡ï¼ˆæ‡‰ç‚º 2-3 å€‹ï¼‰
   - æª¢æŸ¥æ–°èˆˆè©±é¡Œæ•¸é‡ï¼ˆæ‡‰ç‚º 1-2 å€‹ï¼‰
   - æª¢æŸ¥è¶¨å‹¢åˆç†æ€§ï¼ˆäººå·¥åˆ¤æ–·ï¼‰

---

## âœ… é©—æ”¶æ¨™æº–

### åŠŸèƒ½é©—æ”¶

- [ ] **æ–‡ç« èšé¡** - èƒ½æ­£ç¢ºå°‡æœ¬é€±æ–‡ç« èšé¡æˆ 3-5 å€‹ä¸»é¡Œ
- [ ] **é—œéµå­—æå–** - æ¯å€‹é›†ç¾¤èƒ½æå– 3-5 å€‹ä»£è¡¨æ€§é—œéµå­—
- [ ] **è¶¨å‹¢è­˜åˆ¥** - èƒ½è­˜åˆ¥ 2-3 å€‹ç†±é–€è¶¨å‹¢
- [ ] **æ–°èˆˆè©±é¡Œåµæ¸¬** - èƒ½åµæ¸¬ 1-2 å€‹æ–°èˆˆè©±é¡Œ
- [ ] **LLM å ±å‘Šç”Ÿæˆ** - èƒ½ç”Ÿæˆçµæ§‹åŒ–çš„é€±å ±æ•¸æ“š
- [ ] **HTML æ ¼å¼åŒ–** - èƒ½ç”Ÿæˆç¾è§€çš„ HTML Email
- [ ] **ç´”æ–‡å­—æ ¼å¼åŒ–** - èƒ½ç”Ÿæˆæ˜“è®€çš„ç´”æ–‡å­— Email
- [ ] **Email ç™¼é€** - èƒ½æˆåŠŸç™¼é€é€±å ±åˆ°æŒ‡å®šä¿¡ç®±

### å“è³ªé©—æ”¶

- [ ] **èšé¡è³ªé‡** - Silhouette Score >= 0.5
- [ ] **é—œéµå­—ç›¸é—œæ€§** - é—œéµå­—èˆ‡é›†ç¾¤ä¸»é¡Œç›¸ç¬¦ï¼ˆäººå·¥åˆ¤æ–·ï¼‰
- [ ] **è¶¨å‹¢æº–ç¢ºæ€§** - è¶¨å‹¢è­˜åˆ¥åˆç†ï¼ˆäººå·¥åˆ¤æ–·ï¼‰
- [ ] **å ±å‘Šå¯è®€æ€§** - å ±å‘Šå…§å®¹æµæš¢ã€çµæ§‹æ¸…æ™°
- [ ] **æ´å¯Ÿæ·±åº¦** - æ´å¯Ÿè¶…è¶Šç°¡å–®ç¾…åˆ—ï¼Œæœ‰åˆ†æåƒ¹å€¼

### æ¸¬è©¦é©—æ”¶

- [ ] **å–®å…ƒæ¸¬è©¦é€šéç‡** - 100%
- [ ] **æ•´åˆæ¸¬è©¦é€šéç‡** - >= 90%
- [ ] **ä»£ç¢¼è¦†è“‹ç‡** - æ ¸å¿ƒé‚è¼¯è¦†è“‹ç‡ >= 85%
- [ ] **æ–‡æª”å®Œæ•´æ€§** - æ‰€æœ‰å…¬é–‹æ–¹æ³•æœ‰ docstring

### æ€§èƒ½é©—æ”¶

- [ ] **èšé¡è€—æ™‚** - 50 ç¯‡æ–‡ç« èšé¡ < 5 ç§’
- [ ] **LLM ç”Ÿæˆè€—æ™‚** - å ±å‘Šç”Ÿæˆ < 30 ç§’
- [ ] **ç¸½åŸ·è¡Œæ™‚é–“** - å®Œæ•´æµç¨‹ < 2 åˆ†é˜ï¼ˆ50 ç¯‡æ–‡ç« ï¼‰

---

## âš ï¸ é¢¨éšªèˆ‡å°ç­–

### é¢¨éšª 1: èšé¡è³ªé‡ä¸ç©©å®š

**é¢¨éšªæè¿°**: K-Means ä¾è³´åˆå§‹ä¸­å¿ƒé»ï¼Œå¯èƒ½å°è‡´èšé¡çµæœä¸ç©©å®š

**å½±éŸ¿**: æ¯æ¬¡åŸ·è¡Œå¾—åˆ°ä¸åŒçš„ä¸»é¡Œåˆ†å¸ƒ

**å°ç­–**:
1. è¨­ç½® `random_state=42` ç¢ºä¿å¯é‡ç¾æ€§
2. ä½¿ç”¨ `n_init=10` å¤šæ¬¡åˆå§‹åŒ–å–æœ€ä½³çµæœ
3. å¦‚æ•ˆæœä»ä¸ä½³ï¼Œå˜—è©¦ DBSCAN æˆ– Agglomerative Clustering

**å„ªå…ˆç´š**: é«˜

---

### é¢¨éšª 2: æ–‡ç« æ•¸é‡ä¸è¶³

**é¢¨éšªæè¿°**: æŸäº›é€±æ–‡ç« æ•¸é‡ < 30 ç¯‡ï¼Œèšé¡æ•ˆæœå·®

**å½±éŸ¿**: ç„¡æ³•å½¢æˆæœ‰æ„ç¾©çš„é›†ç¾¤

**å°ç­–**:
1. è¨­ç½®æœ€å°æ–‡ç« æ•¸é–¾å€¼ï¼ˆå¦‚ 20 ç¯‡ï¼‰
2. æ–‡ç« ä¸è¶³æ™‚é™ç´šç‚ºã€ŒTop æ–‡ç« åˆ—è¡¨ã€æ¨¡å¼ï¼ˆé¡ä¼¼ Dailyï¼‰
3. å‹•æ…‹èª¿æ•´èšé¡æ•¸é‡ï¼ˆæ–‡ç« å°‘æ™‚ k=2-3ï¼‰

**å„ªå…ˆç´š**: ä¸­

---

### é¢¨éšª 3: LLM è¼¸å‡ºæ ¼å¼éŒ¯èª¤

**é¢¨éšªæè¿°**: LLM å¶çˆ¾è¿”å›éæ¨™æº– JSON æ ¼å¼

**å½±éŸ¿**: å ±å‘Šç”Ÿæˆå¤±æ•—

**å°ç­–**:
1. ä½¿ç”¨èˆ‡ Daily Curator ç›¸åŒçš„ JSON è§£æç­–ç•¥ï¼ˆæ”¯æ´ Markdown åŒ…è£ï¼‰
2. å¯¦ç¾é™ç´šè§£æï¼ˆéƒ¨åˆ†æ¬„ä½ç¼ºå¤±æ™‚è£œå……é»˜èªå€¼ï¼‰
3. è¨˜éŒ„åŸå§‹è¼¸å‡ºä¾¿æ–¼èª¿è©¦

**å„ªå…ˆç´š**: ä¸­

---

### é¢¨éšª 4: é—œéµå­—æå–ä¸æº–ç¢º

**é¢¨éšªæè¿°**: TF-IDF å¯èƒ½æå–åˆ°ç„¡æ„ç¾©çš„é—œéµå­—

**å½±éŸ¿**: é›†ç¾¤ä¸»é¡Œä¸æ˜ç¢º

**å°ç­–**:
1. æ“´å±•åœç”¨è©åˆ—è¡¨ï¼ˆstopwordsï¼‰
2. è¨­ç½®æœ€å°è©é•·ï¼ˆ>= 4 å­—å…ƒï¼‰
3. å¦‚æ•ˆæœä¸ä½³ï¼Œæ”¹ç”¨ LLM æå–é—œéµå­—ï¼ˆæˆæœ¬é«˜ä½†æº–ç¢ºï¼‰

**å„ªå…ˆç´š**: ä½

---

### é¢¨éšª 5: é€±å ±éé•·

**é¢¨éšªæè¿°**: 50+ ç¯‡æ–‡ç« çš„é€±å ±å¯èƒ½éæ–¼å†—é•·

**å½±éŸ¿**: ç”¨æˆ¶é–±è®€è² æ“”é‡

**å°ç­–**:
1. é™åˆ¶æ¯å€‹é›†ç¾¤æœ€å¤šé¡¯ç¤º 3 ç¯‡ä»£è¡¨æ€§æ–‡ç« 
2. é™åˆ¶ Top æ–‡ç« åˆ—è¡¨ç‚º 5-7 ç¯‡
3. è¨­è¨ˆå¯æŠ˜ç–Šçš„ HTML å€å¡Šï¼ˆè©³ç´°å…§å®¹å¯é¸æ“‡å±•é–‹ï¼‰

**å„ªå…ˆç´š**: ä½

---

## ğŸ“š åƒè€ƒè³‡æ–™

### æŠ€è¡“æ–‡ä»¶

- [scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [scikit-learn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
- [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)

### ADK å®˜æ–¹æ–‡ä»¶

- [LlmAgent](https://google.github.io/adk-docs/agents/llm/)
- [Sessions & Memory](https://google.github.io/adk-docs/sessions/)

### å°ˆæ¡ˆå…§éƒ¨æ–‡ä»¶

- `docs/planning/stage8_curator_daily.md` - Daily Curator è¨­è¨ˆ
- `docs/planning/stage7_analyst_agent.md` - Analyst Agent è¨­è¨ˆ
- `CLAUDE.md` - å°ˆæ¡ˆä¸€è‡´æ€§æŒ‡å—

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®Œæˆ Stage 10 å¾Œï¼Œæ¥çºŒï¼š

1. **Stage 11**: Weekly Pipeline é›†æˆï¼ˆé€±å ±æµç¨‹ç·¨æ’ï¼‰
2. **Stage 12**: è³ªé‡ä¿è­‰èˆ‡å„ªåŒ–ï¼ˆQA & Optimizationï¼‰

---

**å‰µå»ºè€…**: Ray å¼µç‘æ¶µ
**å‰µå»ºæ—¥æœŸ**: 2025-11-25
**æœ€å¾Œæ›´æ–°**: 2025-11-25
**ç‹€æ…‹**: è¦åŠƒå®Œæˆï¼Œå¾…å¯¦ä½œ
