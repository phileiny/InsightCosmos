# Stage 6: Content Extraction Tool

> **éšæ®µç·¨è™Ÿ**: Stage 6
> **éšæ®µç›®æ¨™**: å¯¦ç¾æ–‡ç« å…§å®¹æå–å·¥å…·ï¼ŒæŠ“å– URL å®Œæ•´æ­£æ–‡
> **å‰ç½®ä¾è³´**: Stage 5 å®Œæˆï¼ˆScout Agentï¼‰
> **é è¨ˆæ™‚é–“**: 1 å¤©
> **ç‹€æ…‹**: Planning

---

## ğŸ¯ éšæ®µç›®æ¨™

### æ ¸å¿ƒç›®æ¨™

å¯¦ç¾ä¸€å€‹å¼·å¤§ä¸”å¯é çš„ Content Extraction Toolï¼Œèƒ½å¤ å¾ URL ä¸­æå–é«˜å“è³ªçš„æ–‡ç« å…§å®¹ï¼Œç‚º Analyst Agent æä¾›å®Œæ•´çš„åˆ†æç´ æã€‚

é€™å€‹å·¥å…·å°‡ï¼š
1. å¾ä»»æ„ URL æŠ“å– HTML å…§å®¹
2. æ™ºèƒ½è­˜åˆ¥ä¸¦æå–æ–‡ç« ä¸»é«”å…§å®¹
3. æ¸…ç†ç„¡é—œå…ƒç´ ï¼ˆå°èˆªã€å»£å‘Šã€å´é‚Šæ¬„ç­‰ï¼‰
4. æå–é—œéµå…ƒæ•¸æ“šï¼ˆæ¨™é¡Œã€ä½œè€…ã€ç™¼å¸ƒæ—¥æœŸã€åœ–ç‰‡ï¼‰
5. æä¾›çµæ§‹åŒ–ä¸”ä¸€è‡´çš„è¼¸å‡ºæ ¼å¼

### ç‚ºä»€éº¼éœ€è¦é€™å€‹éšæ®µï¼Ÿ

Scout Agent æ”¶é›†çš„æ–‡ç« é€šå¸¸åªåŒ…å«æ¨™é¡Œå’Œæ‘˜è¦ï¼Œç¼ºä¹å®Œæ•´å…§å®¹ã€‚Analyst Agent éœ€è¦å®Œæ•´çš„æ–‡ç« æ­£æ–‡æ‰èƒ½ï¼š
- é€²è¡Œæ·±åº¦æŠ€è¡“åˆ†æ
- è­˜åˆ¥é—œéµæ´å¯Ÿèˆ‡è¶¨å‹¢
- æº–ç¢ºè©•ä¼°æ–‡ç« åƒ¹å€¼èˆ‡å„ªå…ˆåº¦
- æå–å…·é«”çš„æŠ€è¡“ç´°ç¯€èˆ‡æ•¸æ“š

æ²’æœ‰é«˜å“è³ªçš„å…§å®¹æå–ï¼Œå¾ŒçºŒçš„åˆ†æèˆ‡å ±å‘Šç”Ÿæˆå°‡å—åˆ°åš´é‡é™åˆ¶ã€‚

---

## ğŸ“¥ è¼¸å…¥ (Input)

### ä¾†è‡ªä¸Šä¸€éšæ®µçš„ç”¢å‡º

- **Stage 5 (Scout Agent)**:
  - `raw_articles[]` - æ–‡ç« åˆ—è¡¨ï¼Œæ¯ç¯‡åŒ…å« `url` æ¬„ä½
  - ç¤ºä¾‹: `[{"title": "...", "url": "https://...", "source": "..."}, ...]`

### å¤–éƒ¨ä¾è³´

- **æŠ€è¡“ä¾è³´**:
  - `trafilatura` - ä¸»åŠ›å…§å®¹æå–å¥—ä»¶ï¼ˆæ ¹æ“š Context7 æŸ¥è©¢çµæœï¼‰
  - `beautifulsoup4` - HTML è§£æï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
  - `requests` - HTTP è«‹æ±‚
  - `lxml` - XML/HTML è§£æå™¨ï¼ˆtrafilatura ä¾è³´ï¼‰

- **é…ç½®ä¾è³´**:
  - ï¼ˆå¯é¸ï¼‰`USER_AGENT` - è‡ªå®šç¾© User-Agent å­—ä¸²
  - ï¼ˆå¯é¸ï¼‰`REQUEST_TIMEOUT` - HTTP è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆé è¨­ 30 ç§’ï¼‰
  - ï¼ˆå¯é¸ï¼‰`MAX_RETRIES` - æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 3 æ¬¡ï¼‰

- **æ•¸æ“šä¾è³´**:
  - æ¸¬è©¦ URL åˆ—è¡¨ï¼ˆæ¶µè“‹ä¸åŒç¶²ç«™é¡å‹ï¼‰

---

## ğŸ“¤ è¼¸å‡º (Output)

### ä»£ç¢¼ç”¢å‡º

```
src/
â””â”€ tools/
    â”œâ”€ content_extractor.py  # ä¸»è¦å¯¦ç¾ï¼ˆNEWï¼‰
    â””â”€ __init__.py           # æ›´æ–°å°å‡º
tests/
â””â”€ unit/
    â””â”€ test_content_extractor.py  # å–®å…ƒæ¸¬è©¦ï¼ˆNEWï¼‰
```

### æ–‡æª”ç”¢å‡º

- `docs/implementation/stage6_notes.md` - å¯¦ä½œç­†è¨˜
- `docs/validation/stage6_test_report.md` - æ¸¬è©¦å ±å‘Šï¼ˆå¯é¸ï¼‰

### åŠŸèƒ½ç”¢å‡º

- [x] URL å…§å®¹æŠ“å–ï¼ˆHTTP GETï¼‰
- [x] HTML è§£æèˆ‡æ¸…ç†
- [x] ä¸»é«”å…§å®¹æå–
- [x] å…ƒæ•¸æ“šæå–ï¼ˆæ¨™é¡Œã€ä½œè€…ã€æ—¥æœŸã€åœ–ç‰‡ï¼‰
- [x] éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
- [x] çµæ§‹åŒ–è¼¸å‡ºæ ¼å¼

---

## ğŸ—ï¸ æŠ€è¡“è¨­è¨ˆ

### æ¶æ§‹åœ–

```
Input: URL
    â†“
HTTP Request (requests)
    â†“
HTML Content
    â†“
Content Extraction (trafilatura)
    â†“
Metadata Extraction
    â†“
Output: Structured Article
```

### æ ¸å¿ƒçµ„ä»¶

#### çµ„ä»¶ 1: ContentExtractor é¡

**è·è²¬**: ç®¡ç†å…§å®¹æå–æµç¨‹èˆ‡é…ç½®

**é¡è¨­è¨ˆ**:

```python
class ContentExtractor:
    """
    æ–‡ç« å…§å®¹æå–å™¨

    ä½¿ç”¨ trafilatura ä½œç‚ºä¸»åŠ›æå–å¼•æ“ï¼Œæä¾›çµ±ä¸€çš„æ¥å£ã€‚
    """

    def __init__(
        self,
        timeout: int = 30,
        max_retries: int = 3,
        user_agent: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            timeout: HTTP è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            user_agent: è‡ªå®šç¾© User-Agentï¼ˆé è¨­ä½¿ç”¨æ¨™æº–ç€è¦½å™¨ UAï¼‰
        """
        pass

    def extract(self, url: str) -> dict:
        """
        å¾ URL æå–æ–‡ç« å…§å®¹

        Args:
            url: æ–‡ç«  URL

        Returns:
            dict: çµæ§‹åŒ–æ–‡ç« æ•¸æ“šï¼ˆè¦‹è¼¸å‡ºæ ¼å¼ï¼‰

        Raises:
            ValueError: URL æ ¼å¼ç„¡æ•ˆ
            requests.RequestException: ç¶²è·¯è«‹æ±‚å¤±æ•—

        Example:
            >>> extractor = ContentExtractor()
            >>> article = extractor.extract("https://example.com/article")
            >>> print(article["title"])
            "Example Article Title"
        """
        pass

    def extract_batch(self, urls: List[str]) -> List[dict]:
        """
        æ‰¹é‡æå–å¤šå€‹ URLï¼ˆé †åºåŸ·è¡Œï¼‰

        Args:
            urls: URL åˆ—è¡¨

        Returns:
            List[dict]: çµæ§‹åŒ–æ–‡ç« åˆ—è¡¨ï¼ˆå¤±æ•—çš„è¿”å› error ç‹€æ…‹ï¼‰
        """
        pass
```

**è¼¸å‡ºæ ¼å¼**:

```python
{
    "status": "success" | "error",
    "url": "https://example.com/article",
    "title": "æ–‡ç« æ¨™é¡Œ",
    "author": "ä½œè€…åç¨±",           # å¯èƒ½ç‚º None
    "published_date": "2025-11-23", # ISO æ ¼å¼ï¼Œå¯èƒ½ç‚º None
    "content": "å®Œæ•´æ­£æ–‡å…§å®¹...",   # ç´”æ–‡æœ¬ï¼Œå·²æ¸…ç† HTML
    "content_html": "<p>...</p>",   # ä¿ç•™åŸºæœ¬æ ¼å¼çš„ HTMLï¼ˆå¯é¸ï¼‰
    "images": [                      # ä¸»è¦åœ–ç‰‡åˆ—è¡¨
        "https://example.com/image1.jpg",
        "https://example.com/image2.jpg"
    ],
    "word_count": 1234,              # å­—æ•¸çµ±è¨ˆ
    "language": "en",                # èªè¨€ä»£ç¢¼ï¼ˆå¯èƒ½ç‚º Noneï¼‰
    "error_message": None,           # éŒ¯èª¤æ™‚åŒ…å«éŒ¯èª¤è¨Šæ¯
    "extraction_time": 1.23          # æå–è€—æ™‚ï¼ˆç§’ï¼‰
}
```

**éŒ¯èª¤è™•ç†**:

| éŒ¯èª¤é¡å‹ | è™•ç†æ–¹å¼ | è¿”å›ä¿¡æ¯ |
|---------|---------|---------|
| URL ç„¡æ•ˆ | ç«‹å³è¿”å›éŒ¯èª¤ | "Invalid URL format: {url}" |
| HTTP 404 | ç«‹å³è¿”å›éŒ¯èª¤ | "Page not found (404): {url}" |
| HTTP 403/401 | ç«‹å³è¿”å›éŒ¯èª¤ | "Access denied (403/401): {url}" |
| é€£æ¥è¶…æ™‚ | é‡è©¦ 3 æ¬¡å¾Œè¿”å›éŒ¯èª¤ | "Connection timeout after {n} retries: {url}" |
| å…§å®¹æå–å¤±æ•— | å˜—è©¦å‚™ç”¨æ–¹æ¡ˆï¼ˆBeautifulSoupï¼‰ | "Content extraction failed: {reason}" |
| ç„¡å¯ç”¨å…§å®¹ | è¿”å›éŒ¯èª¤ | "No extractable content found: {url}" |

---

## ğŸ”§ å¯¦ä½œç´°ç¯€

### æ­¥é©Ÿ 1: HTTP å…§å®¹æŠ“å–

**ç›®æ¨™**: ç©©å®šå¯é åœ°æŠ“å– HTML å…§å®¹

**å¯¦ä½œè¦é»**:
- ä½¿ç”¨ `requests` å¥—ä»¶
- è¨­å®šåˆç†çš„ User-Agent é¿å…è¢«å°é–
- å¯¦ç¾é‡è©¦æ©Ÿåˆ¶ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
- è™•ç†å„ç¨® HTTP éŒ¯èª¤ç‹€æ…‹ç¢¼
- æª¢æ¸¬ä¸¦è™•ç†é‡å®šå‘

**ä»£ç¢¼ç¤ºä¾‹**:

```python
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def _fetch_html(url: str, timeout: int = 30) -> str:
    """æŠ“å– URL çš„ HTML å…§å®¹"""

    # é…ç½®é‡è©¦ç­–ç•¥
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,  # 1, 2, 4 ç§’
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )

    adapter = HTTPAdapter(max_retries=retry_strategy)
    session = requests.Session()
    session.mount("http://", adapter)
    session.mount("https://", adapter)

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    response = session.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()
    return response.text
```

### æ­¥é©Ÿ 2: ä½¿ç”¨ Trafilatura æå–å…§å®¹

**ç›®æ¨™**: æ™ºèƒ½æå–æ–‡ç« ä¸»é«”èˆ‡å…ƒæ•¸æ“š

**å¯¦ä½œè¦é»**:
- ä½¿ç”¨ `trafilatura.extract()` æå–ä¸»æ–‡æœ¬
- ä½¿ç”¨ `trafilatura.extract_metadata()` æå–å…ƒæ•¸æ“š
- é…ç½®æå–é¸é …ï¼ˆåŒ…å«æ ¼å¼ã€åœ–ç‰‡ç­‰ï¼‰
- è™•ç†æå–å¤±æ•—æƒ…æ³

**ä»£ç¢¼ç¤ºä¾‹**:

```python
import trafilatura

def _extract_with_trafilatura(html: str, url: str) -> dict:
    """ä½¿ç”¨ trafilatura æå–å…§å®¹"""

    # æå–ä¸»æ–‡æœ¬
    content = trafilatura.extract(
        html,
        include_images=True,
        include_links=False,
        output_format='txt',  # æˆ– 'xml', 'json'
        url=url
    )

    # æå–å…ƒæ•¸æ“š
    metadata = trafilatura.extract_metadata(html)

    if content is None:
        raise ValueError("No content extracted")

    return {
        "content": content,
        "title": metadata.title if metadata else None,
        "author": metadata.author if metadata else None,
        "published_date": metadata.date if metadata else None,
        "language": metadata.language if metadata else None
    }
```

### æ­¥é©Ÿ 3: å‚™ç”¨æ–¹æ¡ˆï¼ˆBeautifulSoupï¼‰

**ç›®æ¨™**: ç•¶ trafilatura å¤±æ•—æ™‚æä¾›å‚™ç”¨æå–æ–¹æ¡ˆ

**å¯¦ä½œè¦é»**:
- è­˜åˆ¥å¸¸è¦‹çš„å…§å®¹æ¨™ç±¤ï¼ˆarticle, main, .content, .postï¼‰
- ç§»é™¤ç„¡é—œå…ƒç´ ï¼ˆnav, header, footer, aside, script, styleï¼‰
- æå–ç´”æ–‡æœ¬

**ä»£ç¢¼ç¤ºä¾‹**:

```python
from bs4 import BeautifulSoup

def _extract_with_beautifulsoup(html: str) -> dict:
    """ä½¿ç”¨ BeautifulSoup ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ"""

    soup = BeautifulSoup(html, 'lxml')

    # ç§»é™¤ç„¡é—œå…ƒç´ 
    for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
        tag.decompose()

    # å˜—è©¦æ‰¾åˆ°ä¸»å…§å®¹å€å¡Š
    content_tag = (
        soup.find('article') or
        soup.find('main') or
        soup.find(class_=['content', 'post', 'article', 'entry-content'])
    )

    if content_tag:
        content = content_tag.get_text(separator='\n', strip=True)
    else:
        # é™ç´šæ–¹æ¡ˆï¼šæå– body
        content = soup.body.get_text(separator='\n', strip=True) if soup.body else ""

    # æå–æ¨™é¡Œ
    title = soup.find('title')
    title_text = title.get_text(strip=True) if title else None

    return {
        "content": content,
        "title": title_text,
        "author": None,
        "published_date": None
    }
```

### æ­¥é©Ÿ 4: æ•´åˆèˆ‡éŒ¯èª¤è™•ç†

**ç›®æ¨™**: çµ„åˆå„å€‹çµ„ä»¶ï¼Œæä¾›çµ±ä¸€æ¥å£

**å¯¦ä½œè¦é»**:
- å¯¦ç¾ä¸»è¦çš„ `extract()` æ–¹æ³•
- çµ„åˆ HTTP æŠ“å–èˆ‡å…§å®¹æå–
- çµ±ä¸€éŒ¯èª¤è™•ç†
- è¨˜éŒ„æå–æ™‚é–“

---

## ğŸ§ª æ¸¬è©¦ç­–ç•¥

### å–®å…ƒæ¸¬è©¦

**æ¸¬è©¦æ–‡ä»¶**: `tests/unit/test_content_extractor.py`

**æ¸¬è©¦æ¡ˆä¾‹æ¸…å–®**:

| æ¸¬è©¦æ¡ˆä¾‹ ID | æ¸¬è©¦å…§å®¹ | è¼¸å…¥ | æœŸæœ›è¼¸å‡º | å„ªå…ˆç´š |
|-----------|---------|------|---------|--------|
| TC-6-01 | æå–æ¨™æº–æ–°èæ–‡ç«  | TechCrunch URL | æˆåŠŸæå–æ¨™é¡Œã€å…§å®¹ã€ä½œè€… | High |
| TC-6-02 | æå– Medium æ–‡ç«  | Medium URL | æˆåŠŸæå–å…§å®¹ | High |
| TC-6-03 | è™•ç† 404 éŒ¯èª¤ | ä¸å­˜åœ¨çš„ URL | status="error", éŒ¯èª¤è¨Šæ¯ | High |
| TC-6-04 | è™•ç†è¶…æ™‚ | æ¨¡æ“¬è¶…æ™‚å ´æ™¯ | é‡è©¦ 3 æ¬¡å¾Œè¿”å›éŒ¯èª¤ | High |
| TC-6-05 | æå–ç´”æ–‡æœ¬å…§å®¹ | ä»»æ„ URL | content æ¬„ä½éç©º | High |
| TC-6-06 | æå–å…ƒæ•¸æ“š | åŒ…å« metadata çš„é é¢ | æ­£ç¢ºæå– title, author, date | Medium |
| TC-6-07 | è™•ç†ç„¡å…§å®¹é é¢ | ç©ºç™½é é¢ | status="error" | Medium |
| TC-6-08 | æ‰¹é‡æå– | 3 å€‹ URL | è¿”å› 3 å€‹çµæœ | Medium |
| TC-6-09 | è™•ç†ç„¡æ•ˆ URL | "not-a-url" | ValueError | Low |
| TC-6-10 | çµ±è¨ˆå­—æ•¸ | ä»»æ„æ–‡ç«  | word_count > 0 | Low |

**é—œéµæ¸¬è©¦å ´æ™¯**:

1. **æ­£å¸¸å ´æ™¯**: æå–æ¨™æº–æ–°èç¶²ç«™æ–‡ç« 
   ```python
   def test_extract_standard_article():
       """æ¸¬è©¦æå–æ¨™æº–æ–°èæ–‡ç« """
       extractor = ContentExtractor()

       # ä½¿ç”¨å·²çŸ¥çš„ç©©å®šæ¸¬è©¦ URL
       url = "https://techcrunch.com/..."  # å¯¦éš›æ¸¬è©¦æ™‚éœ€è¦çœŸå¯¦ URL
       result = extractor.extract(url)

       assert result["status"] == "success"
       assert result["title"] is not None
       assert len(result["content"]) > 100  # è‡³å°‘ 100 å­—å…ƒ
       assert result["url"] == url
   ```

2. **é‚Šç•Œå ´æ™¯**: è™•ç†ç©ºå…§å®¹é é¢
   ```python
   @patch('requests.Session.get')
   def test_extract_empty_page(mock_get):
       """æ¸¬è©¦è™•ç†ç„¡å…§å®¹é é¢"""
       mock_response = Mock()
       mock_response.text = "<html><body></body></html>"
       mock_response.status_code = 200
       mock_get.return_value = mock_response

       extractor = ContentExtractor()
       result = extractor.extract("https://example.com")

       assert result["status"] == "error"
       assert "No extractable content" in result["error_message"]
   ```

3. **ç•°å¸¸å ´æ™¯**: è™•ç† HTTP éŒ¯èª¤
   ```python
   @patch('requests.Session.get')
   def test_extract_http_404(mock_get):
       """æ¸¬è©¦è™•ç† 404 éŒ¯èª¤"""
       mock_get.side_effect = requests.HTTPError("404 Not Found")

       extractor = ContentExtractor()
       result = extractor.extract("https://example.com/not-found")

       assert result["status"] == "error"
       assert "404" in result["error_message"]
   ```

### æ•´åˆæ¸¬è©¦

**æ¸¬è©¦å ´æ™¯**: èˆ‡ Scout Agent æ•´åˆ

æ¸¬è©¦å¾ Scout Agent ç²å–æ–‡ç« åˆ—è¡¨ï¼Œç„¶å¾Œæ‰¹é‡æå–å…§å®¹ï¼š

```python
def test_integration_with_scout():
    """æ¸¬è©¦èˆ‡ Scout Agent æ•´åˆ"""
    # 1. æ¨¡æ“¬ Scout Agent è¼¸å‡º
    articles = [
        {"title": "Article 1", "url": "https://example.com/1"},
        {"title": "Article 2", "url": "https://example.com/2"}
    ]

    # 2. æ‰¹é‡æå–å…§å®¹
    extractor = ContentExtractor()
    urls = [a["url"] for a in articles]
    results = extractor.extract_batch(urls)

    # 3. é©—è­‰çµæœ
    assert len(results) == 2
    for result in results:
        assert "content" in result
```

**æ¸¬è©¦æ•¸æ“š**:

å‰µå»º `tests/fixtures/test_urls.json`ï¼š
```json
{
  "valid_urls": [
    "https://techcrunch.com/...",
    "https://medium.com/...",
    "https://github.com/..."
  ],
  "invalid_urls": [
    "https://example.com/404",
    "not-a-url",
    ""
  ]
}
```

---

## âœ… é©—æ”¶æ¨™æº– (Acceptance Criteria)

### åŠŸèƒ½é©—æ”¶

- [x] èƒ½æˆåŠŸæå–æ¨™æº–æ–°èç¶²ç«™å…§å®¹ï¼ˆTechCrunch, Medium ç­‰ï¼‰
- [x] èƒ½æå–åŸºæœ¬å…ƒæ•¸æ“šï¼ˆæ¨™é¡Œã€ä½œè€…ã€æ—¥æœŸï¼‰
- [x] èƒ½è™•ç† HTTP éŒ¯èª¤ï¼ˆ404, 403, è¶…æ™‚ç­‰ï¼‰
- [x] èƒ½è™•ç†ç„¡å…§å®¹æˆ–æå–å¤±æ•—çš„æƒ…æ³
- [x] æ‰¹é‡æå–åŠŸèƒ½æ­£å¸¸å·¥ä½œ

### è³ªé‡é©—æ”¶

- [x] å–®å…ƒæ¸¬è©¦é€šéç‡ = 100%
- [x] ä»£ç¢¼è¦†è“‹ç‡ >= 80%
- [x] æ‰€æœ‰å‡½æ•¸æœ‰å®Œæ•´ docstring
- [x] æ‰€æœ‰å‡½æ•¸æœ‰é¡å‹æ¨™è¨»
- [x] éŒ¯èª¤è™•ç†è¦†è“‹ä¸»è¦å ´æ™¯

### æ€§èƒ½é©—æ”¶

- [x] å–®å€‹ URL æå–æ™‚é–“ < 10 ç§’ï¼ˆ95% æƒ…æ³ï¼‰
- [x] é‡è©¦æ©Ÿåˆ¶ä¸è¶…é 30 ç§’ç¸½è¶…æ™‚
- [x] è¨˜æ†¶é«”ä½¿ç”¨åˆç†ï¼ˆ< 100MB per extractionï¼‰

### æ–‡æª”é©—æ”¶

- [x] ä»£ç¢¼è¨»é‡‹å®Œæ•´æ¸…æ™°
- [x] å¯¦ä½œç­†è¨˜è¨˜éŒ„é—œéµæ±ºç­–
- [x] README åŒ…å«ä½¿ç”¨ç¤ºä¾‹

---

## ğŸš§ é¢¨éšªèˆ‡æŒ‘æˆ°

### å·²çŸ¥é¢¨éšª

| é¢¨éšª | å½±éŸ¿ | ç·©è§£æ–¹æ¡ˆ |
|------|------|---------|
| ç¶²ç«™åçˆ¬èŸ²æ©Ÿåˆ¶ | éƒ¨åˆ† URL ç„¡æ³•æå– | ä½¿ç”¨åˆç†çš„ User-Agentã€è«‹æ±‚å»¶é² |
| JavaScript æ¸²æŸ“é é¢ | trafilatura ç„¡æ³•è™•ç† | è¨˜éŒ„å¤±æ•— URLï¼ŒPhase 2 è€ƒæ…® Playwright |
| å…§å®¹æ ¼å¼å¤šæ¨£æ€§ | æå–è³ªé‡ä¸ä¸€è‡´ | æä¾›å‚™ç”¨æå–æ–¹æ¡ˆï¼ˆBeautifulSoupï¼‰ |
| ç¶²è·¯ä¸ç©©å®š | æå–å¤±æ•—ç‡é«˜ | å¯¦ç¾é‡è©¦æ©Ÿåˆ¶èˆ‡è¶…æ™‚æ§åˆ¶ |

### æŠ€è¡“æŒ‘æˆ°

1. **æŒ‘æˆ° 1**: å¦‚ä½•è­˜åˆ¥ä¸»é«”å…§å®¹ vs å»£å‘Šï¼å°èˆª
   - **è§£æ±ºæ–¹æ¡ˆ**: ä¾è³´ trafilatura çš„æ¼”ç®—æ³•ï¼Œå·²é‡å°æ–°èæ–‡ç« å„ªåŒ–

2. **æŒ‘æˆ° 2**: å¦‚ä½•è™•ç†ä¸åŒç¶²ç«™çš„çµæ§‹å·®ç•°
   - **è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨é€šç”¨æå–æ¼”ç®—æ³•ï¼ˆtrafilaturaï¼‰ï¼Œé¿å…é‡å°ç‰¹å®šç¶²ç«™çš„è¦å‰‡

3. **æŒ‘æˆ° 3**: å¦‚ä½•å¹³è¡¡æå–é€Ÿåº¦èˆ‡æº–ç¢ºæ€§
   - **è§£æ±ºæ–¹æ¡ˆ**: æä¾›å¯é…ç½®çš„è¶…æ™‚èˆ‡é‡è©¦åƒæ•¸ï¼Œå…è¨±æ ¹æ“šå ´æ™¯èª¿æ•´

---

## ğŸ“š åƒè€ƒè³‡æ–™

### æŠ€è¡“æ–‡æª”

- [Trafilatura å®˜æ–¹æ–‡æª”](https://trafilatura.readthedocs.io/) - ä¸»åŠ›æå–å¥—ä»¶
- [BeautifulSoup æ–‡æª”](https://www.crummy.com/software/BeautifulSoup/) - å‚™ç”¨è§£æå¥—ä»¶
- [Requests æ–‡æª”](https://docs.python-requests.org/) - HTTP è«‹æ±‚

### Context7 æŸ¥è©¢çµæœ

æ ¹æ“š Context7 MCP æŸ¥è©¢ï¼Œç²å–ä»¥ä¸‹é—œéµè³‡è¨Šï¼š

**Trafilatura** (`/adbar/trafilatura`):
- å°ˆç‚ºæ–°èæ–‡ç« èˆ‡ç¶²é å…§å®¹æå–è¨­è¨ˆ
- æä¾› `extract()` å’Œ `extract_metadata()` å…©å€‹æ ¸å¿ƒå‡½æ•¸
- æ”¯æ´å¤šç¨®è¼¸å‡ºæ ¼å¼ï¼ˆtxt, xml, jsonï¼‰
- Code Snippets: 25,379 å€‹ï¼ˆæ–‡æª”éå¸¸è±å¯Œï¼‰
- Benchmark Score: 72.8ï¼ˆé«˜å“è³ªï¼‰

**BeautifulSoup4** (`/wention/beautifulsoup4`):
- é€šç”¨ HTML/XML è§£æå™¨
- æä¾› `.get_text()` æ–¹æ³•æå–æ–‡æœ¬
- æ”¯æ´å¤šç¨®è§£æå™¨ï¼ˆlxml, html.parserï¼‰
- Code Snippets: 176 å€‹
- Benchmark Score: 97.9ï¼ˆéå¸¸æˆç†Ÿï¼‰

### å…§éƒ¨åƒè€ƒ

- `docs/planning/stage3_rss_tool.md` - é¡ä¼¼çš„ç¶²è·¯è«‹æ±‚è™•ç†æ¨¡å¼
- `CLAUDE.md` - å·¥å…·è¨­è¨ˆè¦ç¯„ï¼ˆSection: ç¨‹å¼ç¢¼ç·¨å¯«æ¨™æº–ï¼‰

---

## ğŸ“ é–‹ç™¼æ¸…å–® (Checklist)

### è¦åŠƒéšæ®µ âœ“

- [x] å®Œæˆæœ¬è¦åŠƒæ–‡æª”
- [x] ä½¿ç”¨ Context7 æŸ¥è©¢æŠ€è¡“æ–¹æ¡ˆ
- [x] è©•å¯©é€šé

### å¯¦ä½œéšæ®µ

- [ ] å®‰è£ä¾è³´å¥—ä»¶ï¼ˆtrafilatura, beautifulsoup4, lxmlï¼‰
- [ ] å»ºç«‹æ–‡ä»¶çµæ§‹ï¼ˆcontent_extractor.pyï¼‰
- [ ] å¯¦ç¾ ContentExtractor é¡
- [ ] å¯¦ç¾ HTTP æŠ“å–é‚è¼¯
- [ ] å¯¦ç¾ trafilatura æå–é‚è¼¯
- [ ] å¯¦ç¾ BeautifulSoup å‚™ç”¨æ–¹æ¡ˆ
- [ ] å¯¦ç¾éŒ¯èª¤è™•ç†èˆ‡é‡è©¦
- [ ] å¯¦ç¾æ‰¹é‡æå–åŠŸèƒ½
- [ ] ç·¨å¯«å–®å…ƒæ¸¬è©¦
- [ ] ä»£ç¢¼è‡ªæ¸¬é€šé
- [ ] æ›´æ–° `dev_log.md`

### é©—è­‰éšæ®µ

- [ ] å–®å…ƒæ¸¬è©¦å…¨éƒ¨é€šé
- [ ] æ‰‹å‹•æ¸¬è©¦çœŸå¯¦ URLï¼ˆTechCrunch, Medium, GitHubï¼‰
- [ ] æ•´åˆæ¸¬è©¦èˆ‡ Scout Agent
- [ ] äººå·¥é©—æ”¶æå–å…§å®¹å“è³ª
- [ ] æ–‡æª”æ›´æ–°å®Œæˆ

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³é–‹å§‹

1. å®‰è£å¿…è¦çš„ Python å¥—ä»¶
2. å‰µå»º `src/tools/content_extractor.py` æ–‡ä»¶
3. å¯¦ç¾ `ContentExtractor` é¡çš„åŸºæœ¬æ¡†æ¶
4. å¯¦ç¾ HTTP æŠ“å–èˆ‡ trafilatura æå–é‚è¼¯

### æº–å‚™å·¥ä½œ

- æº–å‚™æ¸¬è©¦ URL åˆ—è¡¨ï¼ˆè‡³å°‘ 5 å€‹ä¸åŒä¾†æºï¼‰
- ç¢ºèªè™›æ“¬ç’°å¢ƒå·²å•Ÿå‹•
- ç¢ºèªä¾è³´å¥—ä»¶å¯æ­£å¸¸å®‰è£

---

## ğŸ“Š æ™‚é–“åˆ†é…

| éšæ®µ | é è¨ˆæ™‚é–“ | å æ¯” |
|------|---------|------|
| è¦åŠƒ | 1.5 å°æ™‚ | 20% |
| å¯¦ä½œ | 4.5 å°æ™‚ | 60% |
| é©—è­‰ | 1.5 å°æ™‚ | 20% |
| **ç¸½è¨ˆ** | **7.5 å°æ™‚** | **100%** |

---

**å‰µå»ºæ—¥æœŸ**: 2025-11-23
**æœ€å¾Œæ›´æ–°**: 2025-11-23
**è² è²¬äºº**: Ray å¼µç‘æ¶µ
**ç‹€æ…‹**: Planning â†’ Implementation â†’ Validation â†’ Done
